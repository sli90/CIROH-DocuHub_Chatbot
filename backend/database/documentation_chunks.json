[
  {
    "idurl": 1,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH DocuHub Homepage\nsource: https://docs.ciroh.org/\nscraped_date: 2025-07-31\n---"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 2,
    "content": "# CIROH DocuHub\nDocumenting Water Research in the Digital Age\nWelcome to CIROH's DocuHub ‚Äì Your centralized gateway to expert insights on our Products, Services, and comprehensive documentation. This powerful resource empowers our community with the technical knowledge needed to enhance collaboration and drive impactful contributions."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 3,
    "content": "## Featured Services and Resources"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 4,
    "content": "### CIROH-2i2c JupyterHub\nAccess cloud-based JupyterHub environment on Google Cloud tailored for hydrological research. Leverage computing power with both CPU and GPU capabilities for advanced computational needs.\n[Learn More](https://docs.ciroh.org/docs/services/cloudservices/2i2c)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 5,
    "content": "### Pantarhei\nAccess Pantarhei, a high-performance computing (HPC) cluster, to run computationally intensive hydrological models. Utilize Pantarhei to perform large-scale simulations and data processing tasks.\n[Learn More](https://docs.ciroh.org/docs/services/on-prem/Pantarhei)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 6,
    "content": "### CIROH Portal\nThis portal enhances collaboration and innovation by providing access to interactive web apps, datasets, and learning modules, supporting CIROH and NOAA NWM researchers in advancing hydrological science.\n[Learn More](https://portal.ciroh.org/)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 7,
    "content": "### Documentation\nDive into our comprehensive documentation to access in-depth information about various CIROH products, including but not limited to NextGen, Snow model, Tethys, and more.\n[Learn More](https://docs.ciroh.org/docs/products/intro)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 8,
    "content": "### Cloud Services\nExplore our array of cloud services and offerings, where you can delve into the specifics of CIROH-AWS cloud. Learn how to gain access to this cloud infrastructure and uncover insights into working seamlessly with the 2i2c cloud services.\n[Learn More](https://docs.ciroh.org/docs/services/intro)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 9,
    "content": "### Policies\nDiscover recommendations and best practices for CIROH's research, projects, and infrastructure.\n[Learn More](https://docs.ciroh.org/docs/policies/intro)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 10,
    "content": "### NextGen In A Box\nUtilize NextGen In A Box (NGIAB) to locally run NextGen framework. Choose specific regions or basins for analysis, control input data, and modify configurations, all within a containerized environment.\n[Learn More](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 11,
    "content": "### AWS\nLeverage the power of CIROH AWS Account to elevate your hydrological research. Get access to enterprise-level AWS cloud platform, and utilize AWS computing resources and scalable storage for your research.\n[Learn More](https://docs.ciroh.org/docs/services/cloudservices/aws)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 12,
    "content": "### Google Cloud\nExplore different services and tools offered by CIROH Google Cloud. Learn how to access Google Cloud Platform (GCP) and leverage its resources for your research and projects.\n[Learn More](https://docs.ciroh.org/docs/services/cloudservices/google-cloud)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 13,
    "content": "## CONTRIBUTE\n\nWe would like CIROH Consortium members to contribute to CIROH DocuHub. Please contribute by adding product/project documentation, tutorials, training data, or conference presentations. The CIROH DocuHub repository provides a collaborative platform for sharing technical documentation for projects. **Learn more about how you can contribute and access the CIROH DocuHub repository here:**\n\n[How to Contribute?](https://docs.ciroh.org/docs/contribute) [Your Feedback Matters](https://forms.office.com/r/5ww7qRWwwf)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 14,
    "content": "## Why DocuHub? Elevate Your Research Journey"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 15,
    "content": "### Knowledge Hub\nAccess a centralized repository of research and educational resources to enhance your hydrological expertise."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 16,
    "content": "### Research & Growth\nExplore cutting-edge research and innovative projects driving innovation in hydrology."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 17,
    "content": "### Blog & News\nStay updated with the latest news, community insights, and updates about CIROH projects."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 18,
    "content": "### Education & Training\nElevate your skills with tutorials, training programs, and educational materials tailored for hydrological research."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 19,
    "content": "### CyberInfrastructure\nLeverage cloud infrastructures, including AWS, GCP, CIROH JupyterHub, and on-premises infrastructures tailored to hydrological research needs."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 20,
    "content": "### Global Collaboration\nJoin a global network of researchers and collaborators to share knowledge, resources, and innovations within the CIROH community."
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 21,
    "content": "## Meet DocuHub Team\n\n- **Arpita Patel** - DevOps Manager and Enterprise Architect\n- **Benjamin Lee** - DevOps Engineer\n- **Trupesh Patel** - Research Software Engineer\n- **Manjila Singh** - Graduate Research Assistant\n- **Nia Minor** - Graduate Research Assistant\n- **Zimuzo Ernest-Eze** - Undergraduate Student Assistant"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 22,
    "content": "## Loved by Researchers"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 23,
    "content": "### Patrick Clemins (April 2025)\n> \"CIROH's DocuHub allows us at the University of Vermont's NETWA (Northeast Evaluation Testbed for Water Resources Applications) to effectively engage with our community by providing a listing of our services, documentation for how to use them, and an introductory \"How to Get Started \" page. It also allows us to share our progress and stay engaged with the broader CIROH community so that we can grow our impact as others discover and use our products and services.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 24,
    "content": "### James Halgren (April 2025)\n> \"Having a community editable common place for sharing code, data resources, and community guidance was one of the critical first steps to being able to gather a community for operational hydrological modeling. Docuhub has begun to fill that gap. I am excited to see it growing and hope it becomes a valuable resource for the CIROH researchers and broader community.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 25,
    "content": "### Supath Dhital (April 2025)\n> \"DocuHub has transformed my research process. I discovered several hydrological frameworks I wasn't previously aware of when searching for specific tools. The platform not only helps me learn from others' work, but also provides a professional space to document and publish my own research findings. When colleagues ask for my work, sharing a DocuHub link adds credibility and enhances collaboration. It's become an essential tool in my research workflow.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 26,
    "content": "### Gio (April 2025)\n> \"What I like the most about using DocuHub is that it is easy to contribute to the documentation when I update or want to add my research work. Also, I like the idea that I can contribute with blogs and tutorials depending on the needs of the community.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 27,
    "content": "### Dr. Shahab Alam (April 2025)\n> \"CIROH DocuHub plays a vital role in supporting researchers by providing a centralized, accessible platform to share tools, documentation, and applications with the broader hydrologic science and forecasting community. Research tools such as the CSES and its Tethys-CSES web app reside on DocuHub, enhancing their visibility and usability. This platform bridges research and practice through transparency and open access, amplifying collaborative impact across the community.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 28,
    "content": "### Quinn Lee (April 2025)\n> \"I frequently use DocuHub to access documentation and best practices for the computing infrastructure I use for my research.\""
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 29,
    "content": "## Our Research\n\nOur research focuses on advancing hydrological science through innovative research, collaboration, and technology development.\n[Learn More](https://ciroh.ua.edu/research/)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 30,
    "content": "## Cyberinfrastructure & Community NextGen Monthly Office Hours\n[Learn More](https://docs.ciroh.org/docs/products/ngiab/office-hours)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 31,
    "content": "## Consortium Sponsors\n- NOAA (https://www.noaa.gov/)\n- USGS (https://www.usgs.gov/)\n- 2I2C (https://2i2c.org/)\n- Lynker (https://lynker.com/)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 32,
    "content": "## Consortium Members\n- Utah State University (https://uwrl.usu.edu/)\n- The University of Vermont (https://www.uvm.edu/ciroh/)\n- University of Utah (https://www.civil.utah.edu/)\n- The University of Alabama in Huntsville (https://www.itsc.uah.edu/home/projects/adapt-precipitation-super-resolution-and-data-fusion-deep-learning-techniques-operational-flood-forecasting)\n- University of California San Diego (https://cw3e.ucsd.edu/)\n- University of Arizona (https://www.arizona.edu/)\n- University of Hawai'i at MƒÅnoa (https://manoa.hawaii.edu/)\n- University of Iowa (https://www.uiowa.edu/)\n- University of Saskatchewan (https://water.usask.ca/)\n- University of Alabama (https://www.ua.edu/)\n- University of Minnesota (https://environment.umn.edu/)\n- Tuskegee University (https://www.tuskegee.edu/)\n- Brigham Young University (https://www.byu.edu/)\n- Colorado School of Mines (https://ciroh.mines.edu/)"
  },
  {
    "idurl": 1,
    "idtype": "text",
    "order": 33,
    "content": "## Consortium Partners\n- Baron Weather (https://baronweather.com/)\n- Oak Ridge National Laboratory (https://www.ornl.gov/)\n- Dauphin Island Sea Lab (https://www.disl.edu/)\n- Jupiter Intelligence (https://www.jupiterintel.com/)\n- RTI International (https://www.rti.org/centers/rti-center-water-resources)\n- CUAHSI (https://www.cuahsi.org/)\n- Stevens Institute of Technology (https://www.stevens.edu/)\n- Gulf of Mexico Coastal Ocean Observing System (https://gcoos.org/)\n- New Mexico State University (https://nmsu.edu/)\n- Penn State University (https://www.psu.edu/)\n- University of Southern California (https://sc.edu/)\n- UC Davis (https://www.ucdavis.edu/)\n- Coastal Carolina University (https://www.coastal.edu/index.php)\n- University of Illinois Urbana-Champaign (https://illinois.edu/)"
  },
  {
    "idurl": 2,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Products Introduction\nsource: https://docs.ciroh.org/docs/products/intro\nscraped_date: 2025-01-31\n---\n\nAt CIROH, our team of researchers, hydrologists, and engineers is committed to advancing our understanding of hydrologic processes, improving operational hydrologic forecasting techniques and workflows, collaborating on community water modeling, converting forecasts into practical solutions, and utilizing water predictions to help guide decision-making processes.\n\n![Water Tools](https://docs.ciroh.org/img/graphics/water_products.png)"
  },
  {
    "idurl": 3,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Services Introduction\nsource: https://docs.ciroh.org/docs/services/intro\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 3,
    "idtype": "text",
    "order": 2,
    "content": "## Computing infrastructure access for consortium members and partners\n\nThe **CIROH CyberInfrastructure** empowers CIROH consortium members by providing a scalable, efficient, and user-friendly computing platform. We understand the challenges researchers face in managing **computational resources**, and the CIROH CyberInfrastructure alleviates these burdens by offering a suite of pre-configured environments and resources. Our team of engineers and developers meticulously optimizes both **cloud-based** (AWS and Google Cloud) and **on-premise infrastructure** (Pantarhei HPC cluster) to ensure unparalleled flexibility and scalability.\n\nThis translates into a powerful platform that includes:\n\n- **Simplified Access:** The CIROH CyberInfrastructure streamlines access to computational environments, eliminating the need for time-consuming installations and maintenance.\n\n- **Unmatched Flexibility:** Our multi-cloud and on-premise infrastructure provides a diverse range of options to suit your specific research needs.\n\n- **Scalable Resources:** The CIROH CyberInfrastrucure readily scales to accommodate your growing data analysis and computational demands.\n\n- **Pre-Installed Software:** Leverage pre-installed hydrological software packages to jumpstart your research endeavors.\n\n- **Streamlined Development:** Benefit from the secure and rapid application development and deployment capabilities offered by Cloud."
  },
  {
    "idurl": 3,
    "idtype": "text",
    "order": 3,
    "content": "The CIROH CyberInfrastructure is meticulously designed to empower CIROH researchers and innovators to achieve groundbreaking results in hydrology. **Join us and unlock the full potential of your research today!**\n\n[Get started](https://docs.ciroh.org/docs/services/access/)\n\n* * *\n\nCIROH Research CyberInfrastructure and DevOps Team - YouTube\n\n[Photo image of CIROH DocuHub](https://www.youtube.com/channel/UCKq6WsNXMdngTyVPhZKsgRA?embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F&embeds_referring_origin=https%3A%2F%2Fdocs.ciroh.org)\n\nCIROH DocuHub\n\n2 subscribers\n\n[CIROH Research CyberInfrastructure and DevOps Team](https://www.youtube.com/watch?v=zE3uFvDCSs4)\n\nCIROH DocuHub\n\nSearch\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\nWatch later\n\nShare\n\nCopy link\n\nWatch on\n\n0:00\n\n/\n‚Ä¢Live\n\n‚Ä¢\n\n* * *"
  },
  {
    "idurl": 3,
    "idtype": "text",
    "order": 4,
    "content": "## CIROH CyberInfrastructure Goals\n\n- Promote **reproducible hydrologic computing experiments** with the NextGen Water Resource Modeling Framework\n- Provide support for, and reduce the barrier to entry for performing **NextGen-related experiments** at various scales; and\n- Accelerate the **interconnection and integration of research products and hydroinformatics innovations** from the various ongoing CIROH experiments.\n\n* * *\n\n![Water Tools](https://docs.ciroh.org/img/graphics/2i2c-gcp.png)[CIROH JupyterHub Service](https://docs.ciroh.org/docs/services/cloudservices/2i2c/)\n\n![AWS](https://blog.adobe.com/en/publish/2021/08/31/media_1649ebc3fbbce0df508081913819d491fc3f7c7a9.png?width=750&format=png&optimize=medium)[CIROH AWS Services](https://docs.ciroh.org/docs/services/cloudservices/aws/)\n\n![On-Premises](https://docs.ciroh.org/img/graphics/onprem.jpg)[Pantarhei HPC](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/)\n\n![Google-Cloud](https://docs.ciroh.org/img/graphics/google-cloud.webp)[CIROH Google Cloud Services](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n\n- [Computing infrastructure access for consortium members and partners](https://docs.ciroh.org/docs/services/intro/#computing-infrastructure-access-for-consortium-members-and-partners)\n- [CIROH CyberInfrastructure Goals](https://docs.ciroh.org/docs/services/intro/#ciroh-cyberinfrastructure-goals)"
  },
  {
    "idurl": 4,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Policies Introduction\nsource: https://docs.ciroh.org/docs/policies/intro\nscraped_date: 2025-01-31\n---\n\nIn the following sections, we provide some practical guidance for CIROH researchers designed to help them meet the terms and conditions of CIROH's Data, Code, and Infrastructure Policy, as discussed above, for different types of research products. Each section is focused on providing guidlines and recommendations for data, code, and infrastructure use.\n\n![Policies](https://docs.ciroh.org/img/graphics/policies_best_practices.png)"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Impact\nsource: https://docs.ciroh.org/impact\nscraped_date: 2025-01-31\n---\n\n![AWS Projects icon](https://docs.ciroh.org/img/logos/corp/aws-black.svg)"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 2,
    "content": "### AWS Projects\n\n24\n\n60 active users\n\n![GCP and JupyterHub Projects icon](https://docs.ciroh.org/img/logos/corp/google-cloud.jpg)"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 3,
    "content": "### GCP and JupyterHub Projects\n\n3\n\n171 active users\n\n![On-premise HPC Projects icon](https://docs.ciroh.org/img/logos/pantarhei.jpg)"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 4,
    "content": "### On-premise HPC Projects\n\n20\n\n50 active users\n\n![NSF ACCESS Allocations Projects icon](https://docs.ciroh.org/img/logos/nsf-logo.png)"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 5,
    "content": "### NSF ACCESS Allocations Projects\n\n7\n\n27 active users\n\n* * *\n\nTo learn more about our projects and the impact we're making, check out our blogs for in-depth insights and updates!\n\nAllAWSGoogleConferenceNextGen"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 6,
    "content": "- ![AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks](https://docs.ciroh.org/img/blog/2025-07-01-aorc-data-access/aorc-data-retrieval-hydroshare.png)\n[**AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks** \\\\\nAccessing AORC data has never been easier with our new Jupyter Notebooks, designed for both point and zone retrieval from the AORC datasets hosted on AWS.\\\\\nPublished on 6/30/2025](https://docs.ciroh.org/blog/aorc-data-access)\n- ![Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB](https://docs.ciroh.org/img/blog/2025-06-27-ngiab-based-research/blog_CIROH_DevCon.png)\n[**Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB** \\\\\nA summary of the research presented at the 2025 CIROH DevCon, focusing on streamflow forecasting in the Hackensack River Watershed using NGIAB.\\\\\nPublished on 6/26/2025](https://docs.ciroh.org/blog/ismart-ngiab-application)\n- ![DevCon 2025: A DevOps and Cyberinfrastructure Success Story](https://docs.ciroh.org/img/blog/2025-06-devcon25/it_team.png)\n[**DevCon 2025: A DevOps and Cyberinfrastructure Success Story** \\\\\nOur team demonstrated how modern DevOps principles and cloud infrastructure can seamlessly support large-scale technical workshops for over 200 attendees.\\\\\nPublished on 6/3/2025](https://docs.ciroh.org/blog/devcon25-infra)\n- ![DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_0.jpeg)\n[**DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote** \\\\\nKeynote Recap - CIROH Developers Conference 2025\\\\\nPublished on 6/3/2025](https://docs.ciroh.org/blog/devcon25-keynote)\n- ![Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights](https://docs.ciroh.org/img/blog/2025-06-ewri-devcon/DevCon_AWI_Team.png)\n[**Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights** \\\\\nA recap of presentations and workshops from the 2025 EWRI Congress and CIROH Developers Conference, focusing on the practical application of the NOAA-OWP NextGen Framework.\\\\\nPublished on 6/3/2025](https://docs.ciroh.org/blog/ewri-devcon25-ngiab)\n- ![Œ¥HBV2.0: How NGIAB and Wukong HPC Streamlined Advanced Hydrologic Modeling](https://docs.ciroh.org/img/blog/2025-05-hbv/img.jpg)\n[**Œ¥HBV2.0: How NGIAB and Wukong HPC Streamlined Advanced Hydrologic Modeling** \\\\\nHere's how NGIAB and Wukong HPC enabled researchers at Pennsylvania State University to create advanced models!\\\\\nPublished on 5/15/2025](https://docs.ciroh.org/blog/may-2025-update)\n- ![Google Cloud Next 2025: Innovation at Scale ‚ú®](https://docs.ciroh.org/impact/img/blog/2025-04-gcp/gcp-1.jpg)\n[**Google Cloud Next 2025: Innovation at Scale ‚ú®** \\\\\nGoogle Cloud Next 2025\\\\\nPublished on 4/14/2025](https://docs.ciroh.org/blog/april-2025-update)\n- ![üåü UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolutionüåü](https://docs.ciroh.org/impact/img/logos/ngiab.png)\n[**üåü UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolutionüåü** \\\\\nAWI News talks about NGIAB\\\\\nPublished on 3/27/2025](https://docs.ciroh.org/blog/march-2025-update)\n- ![Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling ](https://brand.psu.edu/images/shared-images/PSU-mark-navy.jpg)\n[**Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling** \\\\\nFebruary Monthly Blog Update\\\\\nPublished on 2/27/2025](https://docs.ciroh.org/blog/February%20Monthly%20Blog%20Update)\n- ![CIROH at AGU 2024](https://docs.ciroh.org/impact/img/blog/2024-12-agu24/AGU24_02.gif)\n[**CIROH at AGU 2024** \\\\\nDecember Monthly Blog Update\\\\\nPublished on 12/18/2024](https://docs.ciroh.org/blog/December%20Monthly%20Blog%20Update)\n- ![Community NextGen Updates](https://docs.ciroh.org/impact/img/logos/ngiab.png)\n[**Community NextGen Updates** \\\\\nNovember Monthly Blog Update\\\\\nPublished on 11/28/2024](https://docs.ciroh.org/blog/November%20Monthly%20Blog%20Update)\n- ![CIROH Science Meeting 2024](https://docs.ciroh.org/impact/img/blog/2024-10-sci-meet/2024CSM_WomenOfCIROH.png)\n[**CIROH Science Meeting 2024** \\\\\nOctober Monthly Blog Update\\\\\nPublished on 10/28/2024](https://docs.ciroh.org/blog/October%20Monthly%20Blog%20Update)\n- ![Accessing National Water Model (NWM) Data via Google Cloud BigQuery API](https://docs.ciroh.org/impact/img/gcp_architecture_diagram.png)\n[**Accessing National Water Model (NWM) Data via Google Cloud BigQuery API** \\\\\nSeptember Monthly Blog Update\\\\\nPublished on 9/29/2024](https://docs.ciroh.org/blog/September%20Monthly%20Blog%20Update)\n- ![CIROH Cloud User Success Story](https://docs.ciroh.org/impact/img/blog/2024-08-case-studies/ngen-datastream-august-blog.jpg)\n[**CIROH Cloud User Success Story** \\\\\nAugust Monthly Blog Update\\\\\nPublished on 8/27/2024](https://docs.ciroh.org/blog/August%20Monthly%20Blog%20Update)\n- ![CIROH Research CyberInfrastructure Update](https://docs.ciroh.org/impact/img/blog/2024-07-it/summer-institute.jpg)\n[**CIROH Research CyberInfrastructure Update** \\\\\nJuly Monthly IT Update\\\\\nPublished on 7/30/2024](https://docs.ciroh.org/blog/July%20Monthly%20IT%20Update)\n- ![CIROH Developers Conference 2024](https://docs.ciroh.org/impact/img/blog/2024-05-devcon24/devcon24_02.jpeg)\n[**CIROH Developers Conference 2024** \\\\\nCIROH Developers Conference 2024\\\\\nPublished on 5/1/2024](https://docs.ciroh.org/blog/devcon24-retrospective)\n- ![AWRA 2024 Spring Conference](https://docs.ciroh.org/impact/img/blog/2024-04-AWRA/awra2024.jpeg)\n[**AWRA 2024 Spring Conference** \\\\\nAWRA 2024 Spring Conference\\\\\nPublished on 4/15/2024](https://docs.ciroh.org/blog/AWRA%202024%20Spring%20Conference)\n- ![Google Cloud Next '24: A Flood of Innovation and Inspiration](https://docs.ciroh.org/impact/img/blog/2024-04-gcn/googlenext-1.jpeg)\n[**Google Cloud Next '24: A Flood of Innovation and Inspiration** \\\\\nGoogle Cloud Next '24: Las Vegas\\\\\nPublished on 4/15/2024](https://docs.ciroh.org/blog/Google%20Cloud%20Next%202024)\n- üì∞"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 7,
    "content": "### Monthly News Update\n\nMarch 2024\n\n[**Monthly News Update - March 2024** \\\\\nNews and release notes for Community NextGen\\\\\nPublished on 3/30/2024](https://docs.ciroh.org/blog/Community%20NextGen%20Updates%20March%202024)\n- üì∞"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 8,
    "content": "### Monthly News Update\n\nFebruary 2024\n\n[**Monthly News Update - February 2024** \\\\\nNews and release notes for Community NextGen\\\\\nPublished on 2/28/2024](https://docs.ciroh.org/blog/Community%20NextGen%20Updates%20Feb%202024)\n- üì∞"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 9,
    "content": "### Monthly News Update\n\nJanuary 2024\n\n[**NextGen Monthly News Update - January 2024** \\\\\nNews and release notes for NextGen\\\\\nPublished on 1/30/2024](https://docs.ciroh.org/blog/NextGen%20Updates%20Jan%202024)\n- üì∞"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 10,
    "content": "### Monthly News Update\n\nDecember 2023\n\n[**NextGen Monthly News Update - December 2023** \\\\\nNews and release notes for NextGen\\\\\nPublished on 12/29/2023](https://docs.ciroh.org/blog/NextGen%20Updates%20Dec%202023)\n- ![NextGen In A Box v1.1.0 Release](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/image/README/ngiab.png)\n[**NextGen In A Box v1.1.0 Release** \\\\\nNews and release notes for NGIAB\\\\\nPublished on 11/29/2023](https://docs.ciroh.org/blog/NextGen%20In%20A%20Box%20Release%20Notes)\n- üì∞"
  },
  {
    "idurl": 5,
    "idtype": "text",
    "order": 11,
    "content": "### Monthly News Update\n\nNovember 2023\n\n[**NextGen Monthly News Update - November 2023** \\\\\nNews and release notes for NextGen\\\\\nPublished on 11/29/2023](https://docs.ciroh.org/blog/NextGen%20Updates%20Nov%202023)\n- ![NextGen In A Box Updates](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/image/README/ngiab.png)\n[**NextGen In A Box Updates** \\\\\nNews and release notes for NGIAB\\\\\nPublished on 10/29/2023](https://docs.ciroh.org/blog/NextGen-In-A-Box%20Release%20Notes)\n- ![NextGen Framework Forcings](https://github.com/CIROH-UA/ngen-datastream/raw/main/docs/gifs/T2D_2_TMP_2maboveground_cali.gif)\n[**NextGen Framework Forcings** \\\\\nNextGen Framework Forcings\\\\\nPublished on 10/29/2023](https://docs.ciroh.org/blog/NextGen%20Forcings)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Blog\nsource: https://docs.ciroh.org/blog\nscraped_date: 2025-01-31\n---\n\n![Screenshot of Hydroshare Resource](https://docs.ciroh.org/assets/images/aorc-data-retrieval-hydroshare-8cf1d0e0ef2e88fd3e47058cf2fae473.png)\n\n> _A screenshot of the HydroShare resource page for [Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/)._\n\nThe Analysis of Record for Calibration (AORC) dataset is recognized as a high-value resource for the CUAHSI and CIROH community.\nThis dataset is hosted by NOAA via Amazon Web Services (AWS) and is available in two primary formats:\na [latitude-longitude gridded dataset](https://registry.opendata.aws/noaa-nws-aorc/)\nand the [National Water Model (NWM) projected dataset, part of the NWM Retrospective archive](https://registry.opendata.aws/nwm-archive/).\nTo enhance accessibility and illustrate analysis capabilities, we developed four user-friendly Jupyter Notebooks that enable data retrieval for both specific points of interest and spatial domains defined by shapefiles:"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 2,
    "content": "- **AORC\\_LL\\_PointRetrieval.ipynb**: For retrieving and aggregating data from the latitude-longitude gridded dataset for a specific point using geographic coordinates.\n- **AORC\\_LL\\_ZoneRetrieval.ipynb**: For retrieving and aggregating data from the latitude-longitude gridded dataset for an area defined by a polygon shapefile.\n- **AORC\\_NWMProj\\_PointRetrieval.ipynb**: For retrieving and aggregating data from the NWM projected dataset for a specific point using geographic coordinates.\n- **AORC\\_NWMProj\\_ZoneRetrieval.ipynb**: For retrieving and aggregating data from the NWM projected dataset for an area defined by a polygon shapefile.\n\nThese Jupyter Notebooks, containing instructions and Python code to access the data, enable researchers to retrieve AORC data from AWS.\nFrom there, the notebooks offer options to subset and aggregate the data over user-defined time intervals (beyond the original hourly resolution) and spatial area.\nThese serve as examples for how you could write or modify code to access AORC data in your work.\nThe notebooks are publicly [available on HydroShare](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/)\nand are compatible with JupyterHub computing platforms such as [CIROH 2i2c JupyterHub linked to HydroShare](https://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98/)."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 3,
    "content": "To use these notebooks, go to the [HydroShare resource](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/),\nselect \"Open With\" at the top right, and choose \"CIROH 2i2c JupyterHub\".\nThis will copy the resource contents (notebooks and data) into the CIROH JupyterHub environment,\nwhere you can open and work through them to access the data.\nNote that you will need a [CUAHSI HydroShare account](https://www.hydroshare.org/) to access \"Open With\" in HydroShare,\nand you will also need to [request CIROH-2i2c JupyterHub access](https://docs.ciroh.org/docs/services/access/#accessing-ciroh-2i2c-jupyterhub) using a GitHub account.\n\nOur work also includes a comparative analysis of the two AORC datasets with a summary of findings.\nWhile we mostly observed small differences, mainly due to projections, users should be aware of potential discrepancies between the datasets.\n\nBy providing these user-friendly tools and highlighting the characteristics of both AORC datasets,\nour work aims to support and facilitate more efficient hydrological and climate-related research within the CUAHSI and CIROH community."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 4,
    "content": "### References:\n\n- Salehabadi, H., D. Tarboton, A. Nassar, A. M. Castronova, P. Dash (2025). Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis, HydroShare, [http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f](http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f)\n- The development versions of these notebooks are available on GitHub: [https://github.com/CUAHSI/notebooks](https://github.com/CUAHSI/notebooks) in the Data Access Examples / AORC - Retrieval of AORC Data for Hydrologic Analysis folder.\n- Patel, A., A. Castronova (2025). CIROH 2i2c JupyterHub, HydroShare, [http://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98](http://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98)\n\n![A poster, titled &quot;Assessing streamflow forecast over the Hackensack River Watershed using physics- and AI-driven weather prediction models&quot;.](https://docs.ciroh.org/assets/images/blog_CIROH_DevCon-bd5de4e3ca6192611aadd30bb47b1586.png)\n\n> _A poster presented by the I-SMART team at the CIROH Developers Conference, held at the University of Vermont in Burlington from May 28 to 30, 2025._\n\nThe densely populated Hackensack River watershed lies within the New York City Metropolitan Area, which spans northern New Jersey and southern New York.\nAccurate streamflow forecasting within this region is therefore essential to enable effective water resource management, flood prediction, and disaster preparedness."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 5,
    "content": "Precipitation data is critical for effective hydrological modeling, making the identification of reliable data sources a key priority.\nThis is why the Integrated Spatial Modeling and Remote Sensing Technologies Laboratory (I-SMART),\nan interdisciplinary research unit within the Davidson Laboratory at Stevens Institute of Technology in Hoboken, New Jersey,\nuses the latest developments in both atmospheric and hydrological modeling to address flood risks in the Hackensack Watershed\nwith solutions that could be expanded to the entire New York City Metropolitan Area.\n\nIn the past, this work has included key early applications of the Next Generation Water Resources Modeling Framework (NextGen).\nNotably, the I-SMART group was among the first to force the NextGen framework with multiple atmospheric models for comparative analysis during a real-world event:\nthe passage of Superstorm Ida over the New York metropolitan area in September 2021.\nThe recent advent of NextGen In a Box (NGIAB) has provided an opportunity to accelerate these applications even further\nby taking full advantage of NGIAB's containerized, user-friendly, and easily deployed environment."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 6,
    "content": "Recently, the I-SMART team has been testing various regional atmospheric models grounded in physical equations,\nincluding traditional models like WRF and next-generation atmospheric models such as MPAS.\nAdditionally, given the increasing popularity and adoption of AI/ML-based approaches,\nthe team has also begun exploring their potential.\nThe goal of this work is to assess the performance of these approaches in the Hackensack Watershed,\nalong with investigating the sensitivity of the model to various meteorological forcings, including forcings based on the National Water Model.\n_(The initial and/or boundary conditions for all the models were determined using the Global Forecast System.)_\n\nThis investigation required handling a large volume of precipitation data from various models,\neach with different spatial resolutions and in some cases, such as MPAS, using unstructured grids.\nAs such, one of the key challenges was finding a hydrological modeling framework flexible enough to accommodate such diversity.\nThis made the NextGen framework a natural choice, allowing them to integrate precipitation forcings from various sources\nwith the appropriate pre-processing to align them with the model requirements in terms of spatial and temporal scales."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 7,
    "content": "The complex implementation and execution of these models was faciliated by NextGen In A Box (NGIAB),\nwhich successfully enabled the integration of diverse precipitation sources.\nBy simplifying local deployment and providing full control over model inputs, configurations, and runtime operations,\nNGIAB has given the I-SMART team the tools to conduct their groundbreaking research with even greater efficiency.\n\nThe recent DevCon 2025 event showcased not just cutting-edge development practices, but also demonstrated how modern DevOps principles and cloud infrastructure can seamlessly support large-scale technical workshops. Our team had the privilege of providing IT infrastructure and support for over 200 attendees, creating a robust learning environment through an exemplary public-private partnership.\n\n![Image of CIROH&#39;s Research Cyberinfrastructure and DevOps team. On the left, two graphs are shown depicting usage for the Google Cloud-2i2c and Jetstream2 environments.](https://docs.ciroh.org/assets/images/it_team-224ec60c3871a16d47ce478c486fe8c2.png)\n\n> _CIROH's Research Cyberinfrastructure and DevOps team._\n>\n> _Left to right, top to bottom:_\n>\n> _Manjila Singh, Arpita Patel, Nia Minor, Trupesh Patel, James Halgren; Benjamin Lee._"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 8,
    "content": "Last week, I had the incredible opportunity to co-present a keynote at the CIROH\nDevelopers Conference (DevCon 2025), which attracted over 200 attendees. This\npresentation, which I presented alongside Dan Ames, focused on \"CIROH HydroInformatics\nand Research Cyberinfrastructure.\" It was a fantastic experience to share insights\ninto the powerful tools and technologies that CIROH engineers, students, researchers\nhave been developing to advance hydrological research and operations.\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_7.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_0.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_1.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_2.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_3.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_4.jpeg)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 9,
    "content": "- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_5.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_6.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_7.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_0.jpeg)\n\n![AWI Science and Technology Team @ CIROH DevCon2025](https://docs.ciroh.org/assets/images/DevCon_AWI_Team-c9f08b48601493edf8e47d7478fdcfff.png)\n\n> _CIROH-AWI Science and Technology Team._\n>\n> _Left to right: Sagy Cohen, Steven Burian, Manjila Singh, Saide Zand, Savalan N. Neisary, Arpita Patel, Nia Minor, Trupesh Patel, Sifan A. Koriche, Jonathan Frame, Reza S. Alipour, Hari T. Jajula, Chad Perry; Josh Cunningham._"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 10,
    "content": "May was a pivotal month for representing the [Cooperative Institute for Research to Operations in Hydrology (CIROH)](https://ciroh.ua.edu/) and our collective work in advancing water science. As one of CIROH's Ambassadors, I had the privilege of connecting with the broader scientific community at two key events: the [Environmental and Water Resources Institute (EWRI) Congress](https://www.ewricongress.org/) in Anchorage, Alaska, and the [2025 CIROH Developers Conference](https://ciroh.ua.edu/devconference/2025-ciroh-developers-conference/) in Burlington, Vermont.\n\n![Image of graphical outputs from the Œ¥HBV2.0 model](https://docs.ciroh.org/assets/images/img-e699e9df162042eda9a4a1b4a242ffb1.jpg)\n\nPredicting water flow with precision across the vast U.S. landscape is a complex challenge. That's why Song et al. 2024 developed Œ¥HBV2.0, a cutting-edge hydrologic model. It's built with high-resolution modeling of physics to deliver seamless, highly accurate streamflow simulations, even down to individual sub-basins. It's already proven to be a major improvement, performing better than older tools at about 4,000 measurement sites. We also provide a comprehensive 40-year water dataset for ~180,000 river reaches to support this."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 11,
    "content": "Penn State research group pushed Œ¥HBV2.0 further, training it with even more detailed river data and integrating other trusted models, aiming to make it a key part of the NextGen national water modeling system (as a potential NWM3.0 successor). But here's a common hurdle: making powerful scientific tools like this easy and reliable for everyone to use within a larger framework can be tough. Setup issues, runtime errors, and inconsistent results can frustrate users.\n\nNGIAB stepped in to solve exactly this problem. Team has taken the complexity out of using the operations-ready models within NextGen by creating one unified, reliable package. Thanks to NGIAB, users don't have to worry about tricky setups or whether the model will run correctly. NGIAB ensures that our models are compatible everywhere and, most importantly, that they run exactly as designed, consistently and faithfully, every single time, no babysitting required. This means users get the full power of our advanced modeling, without the headaches."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 12,
    "content": "Last week at Google Cloud Next representing our CIROH cloud-based computing efforts! With more than 30,000 participants, Google Next always amazes me! It's huge, engaging on so many levels! Engaging booths, networking opportunities, great presentations, workshops, AI coach for basketball, incredible keynote from an amazing team! Event was not just a conference, but a celebration of innovation and a glimpse into the future of cloud computing!\nGreat to see how Gemini is transforming data manipulation in BigQuery. The ability to use natural language to query, transform, and visualize data is revolutionizing how we interact with massive datasets. Gabe Weiss's demo particularly showcased the potential for non-specialists to derive insights from complex data.\n\nIf you missed the keynote, I highly recommend watching the recording here: [GCN25 Keynote Video](https://www.youtube.com/live/VABwMpL3JCo?t=3564s)\n\n- ![BigQuery and Earth Engine demo at Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-6.jpg)\n\n- ![Arpita Patel at the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-1.jpg)\n\n- ![Title slide from the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-2.jpg)\n\n- ![Slide depicting Google's AI stack](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-3.jpg)\n\n- ![Slide announcing Gemini for Google Distributed Cloud](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-4.jpg)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 13,
    "content": "- ![Slide announcing Google Agentspace](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-5.jpg)\n\n- ![BigQuery and Earth Engine demo at Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-6.jpg)\n\n- ![Arpita Patel at the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-1.jpg)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 14,
    "content": "## üåç AWI News\n\nThe Alabama Water Institute (AWI) at the University of Alabama (UA) recently published an article highlighting how NextGen In A Box (NGIAB) could transform hydrological modeling. This article provides great insight into NGIAB's real-world impact:\n\n- üöÄ **30-minute setup** vs days/weeks of configuration\n- üìñ **Provo River Basin Case Study** demonstrating rapid deployment\n\n![ngiab image](https://docs.ciroh.org/img/logos/ngiab.png)\n\n[‚û°Ô∏è Read the full press release here!](https://awi.ua.edu/news/nextgen-in-a-box-ngiab-revolutionizing-hydrological-modeling-with-a-30-minute-setup/)\n\nPennsylvania State University (PSU) researchers have been leveraging CIROH Cyberinfrastructure to tackle complex hydrological modeling challenges. This post highlights their innovative approach using the [Wukong computing platform](https://docs.ciroh.org/docs/services/on-prem/Wukong/) in conjunction with Amazon S3 bucket storage to efficiently process and analyze large-scale environmental datasets. üöÄ\n\nAGU24 brought together the world's leading minds in Earth and space sciences. CIROH participated actively, showcasing advances in water prediction, modeling techniques and many more technologies."
  },
  {
    "idurl": 6,
    "idtype": "table",
    "order": 15,
    "content": "|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| Product | Cloud Console SQL | CIROH API | Historical | Daily Updates |\n| Medium-range forecasts | X | X | X | X |\n| Long-range forecasts | X | X | X | X |\n| Analysis and Assimilation | X | X | X | X |\n| Retrospective Data (NWM v3) | X |  | X |  |\n| Return Periods | X |  | X |  |"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 16,
    "content": "## **Presentations and Posters** üìä\n\nThe conference provided an excellent platform for CIROH researchers to present their groundbreaking work. Our team delivered impactful presentations and poster sessions highlighting CIROH's innovative work, including advancements in water prediction systems\nand community water modeling.\n\nThese sessions sparked thought-provoking discussions and fostered collaborations with other researchers. For those who missed it, **posters and presentation slides are now available** [here](https://github.com/CIROH-UA/Conferences/tree/main/AGU24). Feel free to explore these materials and share your thoughts. üìù\n\nThe Community NextGen framework has seen significant advancements in November 2024, with major updates across multiple components and exciting new resources for users. Let's dive into the key developments that are making hydrologic modeling more accessible and powerful than ever.\n\nThe 2024 CIROH Science Meeting was a huge success, bringing together researchers, federal partners, and consortium members both in person and virtually. We're excited to share the valuable resources from this year's meeting with the wider CIROH community.\n\nSlides and pictures from the various sessions, keynotes, and the Federal Town Hall have all been uploaded to a shared drive for easy access. You can find links to these materials here: [Access the Shared Drive with Presentation Slides](https://drive.google.com/drive/folders/1NsJEWHQBD92ndc3FD_bFEKgqTUqTxVM3)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 17,
    "content": "![gcp architectrure diagram](https://docs.ciroh.org/img/gcp_architecture_diagram.png)_Image Source: [https://github.com/BYU-Hydroinformatics/api-nwm-gcp](https://github.com/BYU-Hydroinformatics/api-nwm-gcp)_\n\nSeveral important historical and ongoing National Water Model (NWM) datasets are now available on Google Cloud BigQuery, which makes them queryable through SQL using Google Cloud console. Some of these data sets are also accessible through an API (e.g. using Python). These datasets and their current status are as follows:\n\n\n\n\nThis month, we are excited to showcase two case studies that utilized our cyberinfrastructure tools and services. These case studies demonstrate how CIROH's cyberinfrastructure is being utilized to support hydrological research and operational advancements."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 18,
    "content": "## 1\\. ngen-datastream and NGIAB\n\n![ngen-datastream image](https://docs.ciroh.org/img/blog/2024-08-case-studies/ngen-datastream-august-blog.jpg)\n\nWe're excited to share some recent developments and updates from CIROH's Research CyberInfrastructure team:"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 19,
    "content": "### Cloud Infrastructure\n\n- CIROH's Google Cloud Account is now fully operational and managed by our team. You can find more information [here.](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n- We're in the process of migrating our 2i2c JupyterHub to CIROH's Google Cloud account.\n- We've successfully deployed the Google BigQuery API (developed by BYU and Google) for NWM data in our cloud. To access this API, please contact us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu). Please refer to [NWM BigQuery API](https://docs.ciroh.org/docs/products/data-management/bigqeury-api/) to learn more.\n\n**CIROH Developers Conference 2024**\n\n![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_01.jpeg)\n\nThe CIROH team recently participated in the **2nd Annual CIROH Developers Conference (DevCon24)**, held from May 29th to June 1st,2024. The conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field of hydrological forecasting.\n\n**AWRA 2024 Spring Conference**\n\nThe **CIROH CyberInfrastructure team** recently participated in the **AWRA 2024 Spring Conference**, co-hosted by **the Alabama Water Institute at the University of Alabama.**\n\nThemed \"Water Risk and Resilience: Research and Sustainable Solutions,\" the conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 20,
    "content": "**Google Cloud Next '24**\n\nHello everyone, and thanks for stopping by!\n\nI recently had the incredible opportunity to attend Google Cloud Next 2024 in person for the first time, and it was truly an **amazing experience**. From **insightful keynote presentations and workshops to vibrant booths** buzzing with connections, the event was a whirlwind of innovation and inspiration.\n\n**Accelerating Innovation: CIROH's March 2024 Update**\n\nThe CIROH team has been diligently accelerating research cyberinfrastructure capabilities this month. We're thrilled to share key milestones achieved in enhancing the Community NextGen project and our cloud/on-premises platforms.\n\nWelcome to the February edition of the CIROH DocuHub blog, where we bring you the latest updates and news about the Community NextGen project and CIROH's Cloud and on-premise Infrastructure.\n\nOur team has been hard at work enhancing CIROH's Infrastructure and Community NextGen tools. Here are some highlights from February 2024:\n\n1. We successfully launched our new On-premises Infrastructure, which is now fully operational. You can find documentation for it [here](https://docs.ciroh.org/docs/services/on-prem/)."
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 21,
    "content": "Welcome to the January edition of the CIROH DocuHub blog, where we share the latest updates and news about the Community NextGen project monthly. NextGen is a cutting-edge hydrologic modeling framework that aims to advance the science and practice of hydrology and water resources management. In this month's blog, we will highlight some of the recent achievements and developments of the Community NextGen team.\n\nHappy New Year!!! We are back from SFO after attending AGU last month. We are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings, and Community Support from December 2023.\n\n[Visit NGIAB News](https://docs.ciroh.org/news)\n\nWe've released NGIAB v1.1.0! This release fixes issues:\n\n- [#21](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/21)\n- [#67](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/67)\n- [#44](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/44)\n\nMore info: [https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0)\n\n[Visit NGIAB News](https://docs.ciroh.org/news)\n\nWe are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings and Community Support.\n\n[Visit NGIAB News](https://docs.ciroh.org/news)\n\nWe've introduced a fresh addition within DocuHub, offering the most up-to-date insights on NGIAB and NextGen monthly updates.\n\n[Visit NGIAB Release Notes Page](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 6,
    "idtype": "text",
    "order": 22,
    "content": "## NextGen Framework Forcings\n\nA new [forcing processor](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor) tool has been made public. This tool converts any National Water Model based forcing files into ngen forcing files. This process can be an intensive operation in compute, memory, and IO, so this tool facilitates generating ngen input and ultimately makes running ngen more accessible."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH News\nsource: https://docs.ciroh.org/news\nscraped_date: 2025-01-31\n---\n\n* * *\n\n**Stay connected with the latest developments in NextGen water modeling!** This news hub brings you updates, breakthroughs, and opportunities from across our community of practice.\n\nDiscover how researchers and practitioners are applying NextGen frameworks to solve pressing water challenges, learn about upcoming training events, and explore new resources to enhance your modeling workflow. Our community-driven approach ensures you'll always be informed about the innovations that matter most.\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 2,
    "content": "## June 2025 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 3,
    "content": "### NGIAB-data-preprocess\n\nA new release of ngiab_data_preprocess, version **v4.3.0**, is now available. This update primarily includes small improvements and bug fixes. Among the more noticeable changes, the map application can now display data source chunking. Additionally, missing values in the AORC forcing data will now be filled using interpolation.\n\nBeyond these changes, an **'alias'** for ngiab_data_preprocess has been created. This allows users to execute the command-line interface (CLI) more concisely, for example, using `uvx ngiab-prep -i cat-123` instead of the longer `uvx --from ngiab_data_preprocess cli -i cat-123`. This alias functions as a separate pip package that simply depends on ngiab_data_preprocess.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/releases/tag/v4.3.0)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 4,
    "content": "### ngen-datastream\n\nFor this month, we primarily focused on cleaning up the code and updating to the **latest Python version, 3.12**. In terms of system operations, we have scaled back to **VPU16** and are now exclusively running only VPU16.\n\nIt's been observed that the NWM analysis_assim_extend forcings are intermittent, which is causing similar intermittent behavior in the analogous datastream runs.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2025-06-01&until=2025-06-30)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 5,
    "content": "## May 2025 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 6,
    "content": "### CIROH Developer's Conference 2025: NextGen Workshops\n\nüìö Content from many of DevCon2025's workshops has been made publicly available! Links to several of them are provided below.\n\n> _Note that some workshops made use of resources that were deployed specifically during DevCon. However, their steps should still work when adapted for local installations._\n\n- [Navigating the NextGen Ecosystem and NextGen In A Box (NGIAB)](https://github.com/CIROH-UA/Conferences/blob/main/CIROHDevCon2025/CIROH%20Community%20NextGen%202025_Final.pdf)\n- [BMI Basics for NGIAB](https://github.com/CIROH-UA/Conferences/blob/main/CIROHDevCon2025/BMI%20Basics%20Workshop.md)\n- [Output visualization through Tethys and evaluation customization using TEEHR](https://github.com/CIROH-UA/Conferences/blob/main/CIROHDevCon2025/OutputVisualizationThroughTethysAndTEEHREvaluation_DevCon2025_IntroSlides.pdf)\n- [NextGen Calibration Workshop](https://github.com/skoriche/NGIAB-Calibration-DevCon25/blob/main/README.md)\n- [NextGen Research Datastream: How to Contribute to Improving NextGen Forecasts](https://github.com/CIROH-UA/Conferences/blob/main/CIROHDevCon2025/ResearchDataStream_workshop.md) ( [Slides](https://github.com/CIROH-UA/Conferences/blob/main/CIROHDevCon2025/ResearchDataStream_presentation.pdf))\n- [Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub](https://www.hydroshare.org/resource/fc8539358fe64ca6a47468728a0687a1/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 7,
    "content": "### CIROH-2i2c JupyterHub: New NextGen National Water Model Image\n\nüöÄ The NextGen In A Box ecosystem is now available for use on CIROH-2i2c JupyterHub! Select the \"NextGen National Water Model (NWM)\" image to get started.\n\n[Read more...](https://ciroh.awi.2i2c.cloud/)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 8,
    "content": "## April 2025 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 9,
    "content": "### New NGIAB-Calibration Featureüéâ\n\n‚öôÔ∏è Major update to NextGen In A Box! It now supports extended calibration for CFE and NOAA OWP modules. The new calibration framework provides more flexible parameter tuning and improved model performance.\n\nüë®‚Äçüíª Uses ngen-cal branch: [https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal](https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal)\n\nüìñ For detailed instructions on how to use the new calibration capabilities, please check out: [https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal#how-to-use-this](https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal#how-to-use-this)\n\n![A snaphot of the NGIAB CLI's \"guide.sh\" script.](https://docs.ciroh.org/img/news/April-2025-ngiab-cli.png)\n\n[Read more...](https://github.com/CIROH-UA/ngiab_cal)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 10,
    "content": "### New NextGen In A Box Product Website\n\nüéâ We're excited to announce the launch of our new NGIAB dedicated website! Visit our NGIAB webiste at [https://ngiab.ciroh.org/](https://ngiab.ciroh.org/) to explore all NGIAB tools, documentation, and resources all in one place.\n\n![NGIAB Website Image](https://docs.ciroh.org/img/news/April-2025-ngiab-website.png)\n\n[Read more...](https://ngiab.ciroh.org/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 11,
    "content": "### NGIAB 101 Training Module Now Available\n\nüìö The new NGIAB 101 training module is now available with comprehensive documentation, installation guides, and step-by-step instructions for the complete NGIAB end-to-end workflow. It is perfect for new users and those looking to deepen their understanding of the platform.\n\n![NGIAB 101 Image](https://docs.ciroh.org/img/news/April-2025-ngiab-101.png)\n\n[Read more...](https://docs.ciroh.org/training-NGIAB-101)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 12,
    "content": "### Datastream Visualization: NGIAB-client (Visualizer) v0.1.3 released\n\nüöÄ The visualizer now includes DataStream S3 bucket visualization, which allows users to download data from DataStream S3 bucket and visualize data directly. The new update improves time series data handling, fixes some graph display issues and ensures proper datastream-ensemble associations.\n\n![Datastream Visualization](https://docs.ciroh.org/img/news/April-2025-datastream-vis.png)\n\n[Read more...](https://github.com/CIROH-UA/ngiab-client/commits/main/?since=2025-04-01&until=2025-04-30)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 13,
    "content": "### NGIAB-CloudInfra: New and improved shell scripts\n\n‚ú® The shell scripts in NGIAB have gotten a new look! The entire NGIAB workflow has been polished for a clean, intuitive experience.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/commits/main/?since=2025-04-01&until=2025-04-30)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 14,
    "content": "## March 2025 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 15,
    "content": "### NGIAB-data-preprocess"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 16,
    "content": "#### üöÄ New Release v4.0.5!\n\nThis month's update focuses on speed and reliability! We've improved initial setup times by switching to `boto3` for slightly faster S3 hydrofabric downloads. Plus, we've resolved map display issues by updating our JavaScript dependencies.\n\nüî•A new image of NGIAB-data-preprocess is now available in the [CIROH 2i2c JupyterHub](https://ciroh.awi.2i2c.cloud/hub/login) production environment.\n\n![NGIAB-Preprocess Image](https://docs.ciroh.org/img/news/March-2025-ngiab-preprocess-jupyterhub-image.png)\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2025-03-01&until=2025-04-02)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 17,
    "content": "### NGIAB-CloudInfra: NGIAB Docker Image"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 18,
    "content": "#### ‚ö°New Release v1.4.3!\n\nNew calibrated dataset added for Provo River near Woodland, UT AWI_16_10154200_009.tar.gz in README at the link below. Also available in HydroShare!!!\n\nThe new script - runTeehr.sh now available to run NGIAB-TEEHR Evaluation on both ARM64 and AMD64 platforms.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/README.md)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 19,
    "content": "### Updated Community NextGen Documentation Page\n\nüì¢ We've new updates under Community NextGen products tab. üìö This comprehensive resource includes an overview of NGIAB and its extensions.\n\n- üîç Learn more about Data Preprocess, TEEHR Evaluation, Data Visualizer, and DataStreamCLI.\n\n- üõ†Ô∏è Key features, Capabilities and Access Method are included.\n\nüîó Visit the new Community Hydrologic Modeling Framework documentation page [here](https://docs.ciroh.org/docs/products/ngiab/):\n\n[Read more...](https://docs.ciroh.org/docs/products/ngiab/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 20,
    "content": "### ngen-datastream\n\n‚ú® Added CIROH Community Hydrofabric Testing! Automated CI tests are added for each VPU to ensure the geopackage is compatible with DataStreamCLI, NextGen and NGIAB.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2025-03-01&until=2025-04-02)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 21,
    "content": "### NGIAB-HPCInfra: NGIAB Singularity Image for HPC!\n\n‚ú® March updates include 2 PRs:\n\n- New calibrated dataset added for Provo River near Woodland, UT AWI_16_10154200_009.tar.gz in README at the link below.\n- Synced the NGIAB HPC singularity image with the latest features from NGIAB docker image .\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra/commits/main/?since=2025-03-01&until=2025-04-02)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 22,
    "content": "### Community FIM Documentation on DocuHub\n\nüì¢ Both FIM as a service (FIMserv) and FIM Evaluation Framework (FIMPEF) documentation now available on DocuHub!! Go to Products tab -> Community Flood Inundation Mapping\n\n[Read more...](https://docs.ciroh.org/docs/products/Community%20Flood%20Inundation%20Mapping/FIM%20as%20a%20Service/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 23,
    "content": "### CIROH Portal New Look!!\n\nüì¢ BYU Team released newer version of CIROH Portal using docusaurus framework. This brings consistency with CIROH DocuHub!\n\n[Read more...](https://portal.ciroh.org/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 24,
    "content": "### DocuHub Contribute page Updated\n\nüì¢ For more ways to contribute to DocuHub. Read below\n\n[Read more...](https://docs.ciroh.org/docs/contribute)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 25,
    "content": "## February 2025 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 26,
    "content": "### NGIAB-data-preprocess\n\nüöÄ New Release v4.0.3! February brings exciting enhancements to our data preprocessing tool! We've now integrated AORC as an additional data source option for forcings. The codebase has been thoroughly refactored to simplify future integration of new gridded data sources. This architectural improvement makes the tool more extensible and easier to maintain going forward.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2025-01-01&until=2025-02-28)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 27,
    "content": "### NGIAB-CloudInfra: NGIAB Docker Image\n\nüî• New Release v1.4.2! Our February update delivers significant improvements to the NGIAB Docker image! The container now incorporates the latest code from CIROH-UA/ngen:ngiab and CIROH-UA/t-route:ngiab branches. We've updated CFE and Noah-OWP-Modular to their latest versions, removed the GDAL dependency to streamline the image, and implemented remoteless partitioning for enhanced parallel performance.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/commits/main/?since=2025-01-01&until=2025-02-28)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 28,
    "content": "### ngen-datastream\n\n‚ú® Community Research Tools Now Public! The terraform configurations and executions for the community Research Datastream in AWS are now publicly available in the ngen-datastream repository (path: research_datastream/terraform_community). We've also added an interactive text-based graphical interface script to guide new users through the Datastream CLI tool (path: scripts/datastream_guide), making the onboarding experience more intuitive.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2025-01-01&until=2025-02-28)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 29,
    "content": "### NGIAB-HPCInfra: NGIAB Singularity Image\n\n‚ö° Critical HPC Fix Deployed! The NGIAB Singularity Image now leverages the latest code from CIROH-UA/ngen:ngiab and CIROH-UA/t-route:ngiab branches. A critical pull request was merged to resolve the 'srun permission denied' issue on HPC compute nodes by transitioning to OpenMPI. This fix has been successfully validated on both the Pantarhei and Anvil clusters, ensuring smooth operation in HPC environments.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra/commits/main/?since=2025-01-01&until=2025-02-28)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 30,
    "content": "### CIROH-UA/ngen:ngiab\n\n‚ú® Performance Optimization Update! The NGIAB-CloudInfra and NGIAB-HPCInfra containers now utilize our optimized ngiab branch of CIROH-UA/ngen. T-route performance has been significantly enhanced for parallel execution with an alternative partitioning scheme specifically optimized for single-machine parallel runs. Additionally, we've introduced new configuration options that allow users to disable netCDF forcing caching and catchment output writing for more flexible deployment scenarios.\n\n[Read more...](https://github.com/CIROH-UA/ngen/commits/ngiab/?since=2025-01-01&until=2025-02-28)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 31,
    "content": "### CIROH-UA/t-route:ngiab\n\n‚ú® 3x Faster Performance! The NGIAB-CloudInfra and NGIAB-HPCInfra containers now incorporate our optimized ngiab branch of CIROH-UA/t-route. We've drastically improved t-route output writing efficiency, resulting in approximately three times faster performance. This enhancement significantly reduces processing time for hydrological simulations.\n\n[Read more...](https://github.com/CIROH-UA/t-route/commits/ngiab/?since=2025-01-01&until=2025-02-28)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 32,
    "content": "## December 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 33,
    "content": "### NGIAB-data-preprocess\n\nPro Tip: The data-preprocess tool offers VPU subsetting capabilities and can generate configuration files for empirical models like the 'Demonstration LSTM'. Check out these and other new features in the latest release! Discover more updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2024-12-01&until=2024-12-20)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 34,
    "content": "### NGIAB-CloudInfra: NGIAB Docker Image\n\nVersion 1.4.0 is now available! This release introduces the \"demonstration LSTM\" within NGIAB, along with sample AWI-008 input data. The LSTM model serves as an educational example to help users get started, though it's not optimized for maximum prediction accuracy.\" Explore more updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.4.0)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 35,
    "content": "### ngen-datastream\n\nImportant Update: Datastream has been upgraded to v2.2 hydrofabric and now generates both NextGen outputs and forcings for VPUs 02, 03W, and 16. For more details, visit the Datastream portal at https://datastream.ciroh.org/index.html\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/AGU_2024_Poster_FINAL.jpg)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 36,
    "content": "### NGIAB-HPCInfra: NGIAB Singularity Image\n\nDecember updates include 4 PRs adding 'Demostration LSTM' in NGIAB-HPCInfra. Learn more about the updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra/commits/main/?since=2024-12-01&until=2024-12-20)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 37,
    "content": "### NWM BigQuery API Update\n\nAccess NWM streamflow prediction data directly through BigQuery instead of downloading netCDF files. Learn more about this efficient data access method in our documentation here:\n\n[Read more...](https://docs.ciroh.org/docs/products/data-management/bigquery-api/)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 38,
    "content": "## November 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 39,
    "content": "### NGIAB-CloudInfra: NGIAB Docker Image\n\nRelease v1.3.0 has been released with several significant improvements: * Integration of forked ngen and t-route repositories from CIROH-UA's GitHub. * Updated sample input data based on hydrofabric v2.2 * 15 PRs merged * TEEHR integration * CI pipeline improvements with unified Dockerfile * Enhanced NGIAB Visualizer. Explore more updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.3.0)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 40,
    "content": "### NGIAB-HPCInfra: NGIAB Singularity Image\n\nNovember updates include: * 4 PRs updating singularity image to use CIROH-UA repositories * Updated README with new sample input data for hydrofabric v2.2 * Alignment with Docker image modifications\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra/commits/main/?since=2024-11-01&until=2024-11-25)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 41,
    "content": "### ngen-datastream\n\n22 new commits pushed to the main repository\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2024-11-01&until=2024-11-25)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 42,
    "content": "### NGIAB-data-preprocess\n\nVersion 3.1.2 released featuring: * 7 PRs merged * Major update for compatibility with hydrofabric v2.2. Discover more updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2024-11-01&until=2024-11-25)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 43,
    "content": "### Hydrofabric v2.2\n\nThe latest Hydrofabric v2.2 data model is now available.\n\n[Read more...](https://lynker-spatial.s3-us-west-2.amazonaws.com/hydrofabric/v2.2/hfv2.2-data_model.html)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 44,
    "content": "### NextGen Model Framework (CIROH-UA/ngen) - fork of NOAA-OWP/ngen\n\nNGIAB-CloudInfra and NGIAB-HPCInfra now utilize the main branch of CIROH-UA/ngen.\n\n[Read more...](https://github.com/CIROH-UA/ngen)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 45,
    "content": "### T-route (CIROH-UA/t-route) - fork of NOAA-OWP/t-route\n\nNGIAB-CloudInfra and NGIAB-HPCInfra now utilize the datastream branch of CIROH-UA/t-route. Main branch is in sync with NOAA-OWP/t-route\n\n[Read more...](https://github.com/CIROH-UA/t-route)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 46,
    "content": "## September 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 47,
    "content": "### Best Practice Guide for NextGen Framework\n\nTechnical guidance for the inclusion of models/modules in the NextGen Water Resources Modeling Framework is now available in DocuHub.\n\n[Read more...](https://docs.ciroh.org/docs/policies/NextGen/)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 48,
    "content": "### NextGen Model Framework (NOAA-OWP/ngen)\n\n2 PRs merged in September. Explore more updates here.\n\n[Read more...](https://github.com/NOAA-OWP/ngen/commits/master/?since=2024-09-01&until=2024-09-26)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 49,
    "content": "### T-route (NOAA-OWP/t-route)\n\n4 PRs were merged in September.\n\n[Read more...](https://github.com/NOAA-OWP/t-route/commits/master/?since=2024-09-01&until=2024-09-26)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 50,
    "content": "### NextGen Community Office Hours\n\nWe are excited to invite you to join our NextGen Community Office Hours. It is an excellent opportunity for everyone to stay up to date with the latest developments in hydrologic modeling at CIROH. Join us to engage with the team and learn more about ongoing projects and advancements.\n\n[Read more...](https://docs.ciroh.org/docs/products/ngiab/office-hours)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 51,
    "content": "### NGIAB-CloudInfra : NGIAB Docker Image\n\nWe are happy to announce that NGIAB version 1.2.1 has been released, available for both ARM and x86 architectures. This release is up to date with the latest ngen commit ID: f321889 and t-route commit ID: d0982a5. Learn more about the updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.2.1)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 52,
    "content": "### ngen-datastream\n\nIn September, 10 pull requests were merged.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2024-09-01&until=2024-09-26)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 53,
    "content": "### NGIAB-data-preprocess\n\n5 PRs were merged in September.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2024-09-01&until=2024-09-26)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 54,
    "content": "## August 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 55,
    "content": "### NextGen Model Framework (NOAA-OWP/ngen)\n\nFive PRs merged in August. Explore more updates here.\n\n[Read more...](https://github.com/NOAA-OWP/ngen/commits/master/?since=2024-08-01&until=2024-08-29)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 56,
    "content": "### T-route (NOAA-OWP/t-route)\n\nEight PRs were merged in August.\n\n[Read more...](https://github.com/NOAA-OWP/t-route/commits/master/?since=2024-08-01&until=2024-08-29)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 57,
    "content": "### NGIAB-CloudInfra : NGIAB Docker Image\n\nNGIAB version 1.2.0 has been released, available for both ARM and X86 architectures. This release is up to date with the latest ngen commit ID: 2ffedf8 and t-route commit ID: 9d9d711\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/commits/main/?since=2024-08-01&until=2024-08-29)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 58,
    "content": "### NGIAB-HPCInfra: NGIAB Singularity Image\n\nIn August, a new Singularity image was released for NGIAB-HPCInfra. Learn more about the updates here.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra/commits/main/?since=2024-08-01&until=2024-08-29)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 59,
    "content": "### ngen-datastream\n\nIn August, nine pull requests were merged. These include the addition of the forcingprocessor feature, increased test coverage, updates to the research datastream Terraform, and new documentation.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream/commits/main/?since=2024-08-01&until=2024-08-29)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 60,
    "content": "### NGIAB-data-preprocess\n\nFive PRs were merged in August. We are excited to announce the new release v2.0.0! This release uses catchment ids in place of waterbody ids.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB_data_preprocess/commits/main/?since=2024-08-01&until=2024-08-29)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 61,
    "content": "## July 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 62,
    "content": "### NextGen Model Framework (ngen)\n\nFour PRs merged in July. Explore more updates here.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 63,
    "content": "### NGIAB-CloudInfra\n\nIn July, one PR was merged. We are preparing to present \"Advancing Hydrological Modeling: CIROH's NextGen In A Box (NGIAB) and Enhanced Tools for Community-Driven Research\" at AGU24. You can find more updates here.\n\n[Read more...](https://agu.confex.com/agu/agu24/prelim.cgi/Paper/1697622)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 64,
    "content": "### ngen-DataStream\n\nIn July, 6 PRs were merged. We are also presenting \"NextGen National Water Model Framework Datastream\" paper at AGU24. More updates can be found here.\n\n[Read more...](https://agu.confex.com/agu/agu24/prelim.cgi/Paper/1589746)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 65,
    "content": "### NGIAB-data-preprocessor\n\nNine PRs were merged in July.\n\n[Read more...](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 66,
    "content": "### T-route\n\nFive PRs were merged in July.\n\n[Read more...](https://github.com/NOAA-OWP/t-route)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 67,
    "content": "## June 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 68,
    "content": "### ngen-DataStream\n\nIn June, 9 PRs were merged. Ngen-datastream was updated to be compatible with the new hydrofabric data format provided by hfsubset version 2.1.1.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 69,
    "content": "### NGIAB-data-preprocessor\n\nFour PRs were merged in June. We are happy to announce the new release v1.0.1. This release includes various improvements and bug fixes.\n\n[Read more...](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 70,
    "content": "### NextGen Model Framework (ngen)\n\nEleven PRs merged in June. Explore more updates here.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 71,
    "content": "### CIROH DevCon24 Conference Slides\n\nAccess the slides from the 8 NextGen Track workshops held at CIROH DevCon24. Slides can be found here.\n\n[Read more...](https://hydroshare.org/resource/99a1bdcc97af4d159f1116e2573a12ba/)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 72,
    "content": "## May 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 73,
    "content": "### NGIAB-CloudInfra\n\nThe NextGen In A Box (NGIAB) workshop last week at CIROH DevCon24 was a huge success. New sample dataset is created \"AWI-006\". We had 50+ attendees and most of them were successfully able to run NGIAB on their local machine. For those without the admin access, we provided them the AWS instances for the workshop. This workshop introduced a new geospatial visualization tool for visualizing outputs from NGIAB runs. During May, 16 PRs were merged. Slides can be found at:\n\n[Read more...](https://github.com/CIROH-UA/Conferences/tree/main/CIROHDevCon2024/NextGenTrack)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 74,
    "content": "### NGIAB-HPCInfra\n\nRun NGIAB on HPC using NGIAB-HPCInfra. This repo is upgraded to use AWI-006 dataset. For more details, please refer README at\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 75,
    "content": "### ngen-DataStream\n\nNextGen Simulation Development Tools workshop was a huge success. It provided attendees with a hands-on experience using the ngen-datastream tool. In May, 21 PRs were merged. Explore the updates. Slides for the DevCon can be found at:\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 76,
    "content": "### NGIAB-data-preprocessor\n\nThe NextGen Simulation Development Tools workshop at CIROH DevCon24 showcased the tool's capabilites of simplifying data preparation for next-gen simulations.\n\n[Read more...](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 77,
    "content": "### NextGen Model Framework (ngen)\n\n3 PRs merged in May. Explore more updates here.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 78,
    "content": "## April 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 79,
    "content": "### NGIAB-data-preprocessor\n\nIntroducing a tool for NGIAB data preparation. A workshop is planned for CIROH DevCon24 under NextGen Simulation Development Tools.\n\n[Read more...](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 80,
    "content": "### NGIAB-CloudInfra\n\nIn April, we are gearing up to present the NextGen In A Box (NGIAB) workshop at CIROH DevCon24\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 81,
    "content": "### NGIAB-HPCInfra\n\nHere is the updated link to NGIAB-HPCInfra\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 82,
    "content": "### ngen-DataStream\n\nIn April, we are getting ready to present NextGen DataStream at CIROH DevCon24. A workshop is planned under NextGen Simulation Development Tools\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 83,
    "content": "### NextGen Model Framework (ngen)\n\nThirteen PRs merged in April. Explore the updates.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 84,
    "content": "## March 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 85,
    "content": "### Improvement of NGIAB HPCInfra Repositories\n\nAutomating build of NGIAB HPCInfra using GitHub Actions\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-HPCInfra)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 86,
    "content": "### ngen-DataStream\n\nIn March, the python internal to the datastream was containerized. This included implementing catchment-specific configuration for PET, CFE, and Noah OWP (https://github.com/CIROH-UA/ngen-datastream/pull/46). The scripts and plotting tools were developed to perform benchmarking and preliminary results have been collected.\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 87,
    "content": "### NextGen Model Framework (ngen)\n\nSeven PRs merged in March.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 88,
    "content": "## Feb 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 89,
    "content": "### Improvement of NGIAB CI Pipeline for Pull Requests from Forked Repositories\n\nThree pull requests have been successfully merged. Pull requests submitted using forked repositories are now automatically tested in the CI Pipeline. Upon merging changes to the main repository, another CI pipeline is triggered to retest the build and push the image to DockerHub. Additionally, new AWI_004 sample data is now available for 09 VPU for unit testing.\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 90,
    "content": "### ngen-DataStream\n\nIn February, two pull requests were merged. For small runs, you can access ngen-DataStream at: https://github.com/CIROH-UA/ngen-datastream/tree/main/examples\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 91,
    "content": "### NextGen Model Framework (ngen)\n\nNine PRs merged in February.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 92,
    "content": "### T-route\n\nTwelve PRs merged in February.\n\n[Read more...](https://github.com/NOAA-OWP/t-route)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 93,
    "content": "## Jan 2024 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 94,
    "content": "### Run NextGen In A Box(NGIAB) with Singularity on HPC without docker support!!!\n\nIf you want to use NGIAB on HPC that does not support docker, we have a solution for you. Please follow the steps at this link.\n\n[Read more...](https://github.com/CIROH-UA/Ngen-Singularity)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 95,
    "content": "### NextGen In A Box (NGIAB)\n\nWe have made some improvements to NGIAB: 10 PRs merged. Sample input data updated to use AWI_003(with new data provider names). Boost upgraded to v1.79. Self-hosted Runner can start and stop automatically with GitHub Actions. NGIAB image can run in auto mode. Geopackage support added. Documentation available at CIROH DocuHub\n\n[Read more...](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 96,
    "content": "### ngen-DataStream\n\n3 PRs merged in January. Documentation available at: /products/Hydrologic%20Modeling%20Framework/nextgenDatastream/\n\n[Read more...](https://github.com/CIROH-UA/ngen-datastream)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 97,
    "content": "### NextGen Model Framework (ngen)\n\n6 PRs merged in January.\n\n[Read more...](https://github.com/NOAA-OWP/ngen)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 98,
    "content": "### T-route\n\n3 PRs merged in January.\n\n[Read more...](https://github.com/NOAA-OWP/t-route)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 99,
    "content": "## Dec 2023 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 100,
    "content": "### NGIAB presentation at AGU!!!\n\nNextGen In A Box: Advancing Collaborative Modeling for Enhanced Water Resource Management presented by Arpita. We had a full house!\n\n[Read more...](https://docs.ciroh.org/news/Conference%20Material#agu-2023)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 101,
    "content": "### NextGen In A Box\n\nNGIAB Updates: Merged CI pipeline changes PR#74 by Benjamin Lee, Added case study map and a plot with output results PR#72 by Shahab Alam, PR#70 by Josh Cu"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 102,
    "content": "### NextGen Model Framework\n\n5 PRs merged in December."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 103,
    "content": "### T-route\n\n12 PRs merged in December."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 104,
    "content": "### Hydrofabric\n\nLynker is proud to support CIROH with a freely accessable resource for geospatial data: https://www.lynker-spatial.com/. Updated transects/cross section runners"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 105,
    "content": "### NextGen Framework Forcings\n\nDecember updates include an (almost) complete stream script that will produce daily ngen outputs. Documentation (readme) included. ngen-datastream/subsetting has been deprecated and hfsubset integrated into the stream. Benchmarking has begun."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 106,
    "content": "### Community Support\n\nDecember is always an AGU Month! San Francisco (the traditional home of AGU) brings us to the season where we reflect on our own work for the year and finally learn what our coworkers have been doing all this time! Checkout the presentation slides from CIROH at below link.\n\n[Read more...](https://github.com/CIROH-UA/Conferences)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 107,
    "content": "### NOAA-OWP AGU Presentations\n\nNOAA-OWP AGU Presentations\n\n[Read more...](https://github.com/NOAA-OWP/OWP-Presentations/tree/main/AGU/AGU_2023)\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 108,
    "content": "## Nov 2023 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 109,
    "content": "### NGIAB Updates\n\nNGIAB v1.1.0 Release!\n\n[Read more...](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 110,
    "content": "### NextGen Research Lightning Talk\n\nPresented at CIROH NextGen Research Lightning Talk Webinar\n\n[Read more...](https://docs.ciroh.org/news/Conference%20Material#community-nextgen-advancement-lightning-talk---nov-2023)"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 111,
    "content": "### NextGen Model Framework\n\n14 PRs merged in November. Adding new tests for sqlite and new geopackage-based hydrofabric support."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 112,
    "content": "### T-route\n\n13 PRs merged in November. Added additional json/geojson support"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 113,
    "content": "### Hydrofabric\n\nLynker is proud to support CIROH with a freely accessable resource for geospatial data: https://www.lynker-spatial.com/. Updated hfsubset to support using the v2.0 hydrofabric by default. Added Cross-section support to the hydrofabric. 13 PRs merged in November. Added additional json/geojson support"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 114,
    "content": "### NextGen Framework Forcings\n\n14 PRs merged towards the Research Data Stream. Fixed lingering pyarrow issues. Added new support for all segments to be dockerized. Added S3 support with regards to the control flow design, to support one reusable, configurable sequence"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 115,
    "content": "### Community Support\n\nSingularity support is coming to AWI for HPC users that are able to run Singularity builds. Work on serialization (passing from one run to the next with model states still in memory or imported from a file) is ongoing, relative temporal configurations of NextGen with consideration to model configuration changes between timesteps, and better general support for framework polymorphism.\n\n* * *"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 116,
    "content": "## Oct 2023 Updates"
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 117,
    "content": "### NGIAB Updates\n\n15 runs of the whole build process, 7 successful, and 2 pending. Over 550 pulls of the container image, number of 'canonical' runs (uploading their metadata and results) is coming in a future version."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 118,
    "content": "### NextGen Model Framework\n\nUpdates to SUMMA candidate model, performance improvements for memory usage and spatial domain tooling to decouple the currently used geojson Feature and geometry classes into their own simple features interface. This is an abstract interface, with a coupled Boost.Geometry backend that will be used as the default backend."
  },
  {
    "idurl": 7,
    "idtype": "text",
    "order": 119,
    "content": "### T-route\n\n22 Pull Requests were worked on in October with 11 merged, and a focus on testing and getting the compiling process to be tested and reliable particularly for downstream NGIAB and HPC usage of t-route"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH DocuHub Release Notes\nsource: https://docs.ciroh.org/release-notes/\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 2,
    "content": "## Updates\n\n- Education tab has been **deprecated** ( [#440](https://github.com/CIROH-UA/ciroh-ua_website/pull/440), [#443](https://github.com/CIROH-UA/ciroh-ua_website/pull/443))\n  - _[CIROH Portal](https://portal.ciroh.org/presentations) is now CIROH's primary hub for conference material and online lessons._\n  - Relocated information on NWM-affiliated technologies to Products tab\n  - Homepage and navbars rearranged accordingly\n- Mirrored new documentation pages from NGIAB-CloudInfra ( [#442](https://github.com/CIROH-UA/ciroh-ua_website/pull/442))\n- Added notice regarding storage limits on 2i2c ( [#438](https://github.com/CIROH-UA/ciroh-ua_website/pull/438))\n- Resized a screenshot on the blog post \" [AORC Data in Your Hands](https://docs.ciroh.org/blog/aorc-data-access)\" and added a caption ( [#437](https://github.com/CIROH-UA/ciroh-ua_website/pull/437))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 3,
    "content": "## Blog posts and news updates\n\n- Blog post: [\"AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\"](https://docs.ciroh.org/blog/aorc-data-access) ( [#434](https://github.com/CIROH-UA/ciroh-ua_website/pull/434))\n- Blog post: [\"Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB\"](https://docs.ciroh.org/blog/ismart-ngiab-application) ( [#431](https://github.com/CIROH-UA/ciroh-ua_website/pull/431))\n- News update: June 2025 ( [#432](https://github.com/CIROH-UA/ciroh-ua_website/pull/432))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 4,
    "content": "## Updates\n\n- Site-wide improvements and additions to tags ( [#435](https://github.com/CIROH-UA/ciroh-ua_website/pull/435))\n  - Added new \"Community Spotlight\" tag to blog, highlighting key application of CIROH's tooling and cyberinfrastructure across the community\n  - Added new \"Data Preparation\" tag, highlighting tools and research that accelerate the retrieval and preprocessing of data\n  - Increased distribution of \"NGIAB\" tag across older blog posts"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 5,
    "content": "## New content\n\n- The \"Community Hydrologic Modeling\" tab has been completely reworked! It's now called \"NextGen In A Box\", better reflecting the content of the folder. ( [#427](https://github.com/CIROH-UA/ciroh-ua_website/pull/427))\n  - Lots of new content has been added alongside this change within the \"Intro to NGIAB\" sub-section.\n  - NGIAB product subpages have been reorganized into the \"Distributions\" and \"Components\" sub-sections.\n  - As a part of this transition, content regarding the Reseach Datastream (formerly \"NextGen Datastream\") has been spun out into its own category.\n- The \"Research Datastream\" tab now correctly includes its documentation subpages from GitHub. ( [#425](https://github.com/CIROH-UA/ciroh-ua_website/pull/425))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 6,
    "content": "## Updates\n\n- General corrections and upkeep to past news and release notes ( [#426](https://github.com/CIROH-UA/ciroh-ua_website/pull/426))\n- Reformatted the way that DocuHub renders external README files ( [#425](https://github.com/CIROH-UA/ciroh-ua_website/pull/425))\n- Refactored URLs site-wide to remove spaces ( [#423](https://github.com/CIROH-UA/ciroh-ua_website/pull/423))\n- Upgraded to Docusaurus 3.8; enabled preview functionality for 4.0 ( [#420](https://github.com/CIROH-UA/ciroh-ua_website/pull/420))\n\n> _Please note that this update alters the URLs of many DocuHub pages. We've added redirects from their old locations to avoid breaking external links, but updating links to the new URLs is strongly preferred._"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 7,
    "content": "## Blog posts and news updates\n\n- Blog post: [\"DevCon 2025: A DevOps and Cyberinfrastructure Success Story\"](https://docs.ciroh.org/blog/devcon25-infra) ( [#414](https://github.com/CIROH-UA/ciroh-ua_website/pull/414))\n- Blog post: [\"Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights\"](https://docs.ciroh.org/blog/ewri-devcon25-ngiab) ( [#411](https://github.com/CIROH-UA/ciroh-ua_website/pull/411))\n- Blog post: [\"DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote\"](https://docs.ciroh.org/blog/devcon25-keynote) ( [#407](https://github.com/CIROH-UA/ciroh-ua_website/pull/407))\n- News update: May 2025 ( [#408](https://github.com/CIROH-UA/ciroh-ua_website/pull/408))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 8,
    "content": "## Updates\n\n- Moved several NGIAB tutorials to a new, generalized header ( [#415](https://github.com/CIROH-UA/ciroh-ua_website/pull/415))\n- Added \"CIROH Research Cyberinfrastructure\" video to [Services](https://docs.ciroh.org/docs/services/intro) landing page ( [#413](https://github.com/CIROH-UA/ciroh-ua_website/pull/413))\n- Improvements to CI/CD pipelining ( [#409](https://github.com/CIROH-UA/ciroh-ua_website/pull/409), [#395](https://github.com/CIROH-UA/ciroh-ua_website/pull/395))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 9,
    "content": "## New content\n\n- Added NextGen on CIROH JupyterHub to Products section ( [#404](https://github.com/CIROH-UA/ciroh-ua_website/pull/404))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 10,
    "content": "## Updates\n\n- Updated CIROH Portal under Products section with tables, emojis, and grammar fixes ( [#403](https://github.com/CIROH-UA/ciroh-ua_website/pull/403))\n- Updated documentation for \"FIM Database for Multi-Model Visualization\" ( [#402](https://github.com/CIROH-UA/ciroh-ua_website/pull/402))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 11,
    "content": "## New content\n\n- Added CIROH RIVR App to Products section ( [#389](https://github.com/CIROH-UA/ciroh-ua_website/pull/389))\n- Added FIM Database for Multi-Model Visualization to Products section ( [#393](https://github.com/CIROH-UA/ciroh-ua_website/pull/393))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 12,
    "content": "## Updates\n\n- Updated NGIAB E2E Workflow Video: Local setup page with the latest instructions ( [#385](https://github.com/CIROH-UA/ciroh-ua_website/pull/385))\n- Added more information on ngiab-cal ( [#387](https://github.com/CIROH-UA/ciroh-ua_website/pull/387))\n- Added link to DevCon25 Workshop JupyterHub ( [#398](https://github.com/CIROH-UA/ciroh-ua_website/pull/398))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 13,
    "content": "## New content\n\n- Added NGIAB-Calibration to Products section ( [#378](https://github.com/CIROH-UA/ciroh-ua_website/pull/378))\n- Added a dashboard for NGIAB and CIROH repositories to Products section ( [#374](https://github.com/CIROH-UA/ciroh-ua_website/pull/374))\n- Added Community Hydrofabic Patcher to Products section ( [#354](https://github.com/CIROH-UA/ciroh-ua_website/pull/354))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 14,
    "content": "## Blog posts and news updates\n\n- Blog post: [\"Œ¥HBV2.0: How NGIAB Streamlined Advanced Hydrologic Modeling\"](https://docs.ciroh.org/blog/may-2025-update) ( [#379](https://github.com/CIROH-UA/ciroh-ua_website/pull/379))\n- News update: April 2025 ( [#376](https://github.com/CIROH-UA/ciroh-ua_website/pull/376))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 15,
    "content": "## Updates\n\n- Added message from CIROH DocuHub team in footer ( [#372](https://github.com/CIROH-UA/ciroh-ua_website/pull/372))\n- Added descriptive headers to News and Blog pages ( [#371](https://github.com/CIROH-UA/ciroh-ua_website/pull/371))\n- Updated CIROH funding acknowledgement ( [#370](https://github.com/CIROH-UA/ciroh-ua_website/pull/370))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 16,
    "content": "## New content\n\n- Added researcher testimonials to homepage ( [#358](https://github.com/CIROH-UA/ciroh-ua_website/pull/358))\n- DocuHub now has a release notes feed! ( [#318](https://github.com/CIROH-UA/ciroh-ua_website/pull/318))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 17,
    "content": "## Updates\n\n- Added workshop IT support form to services access page ( [#362](https://github.com/CIROH-UA/ciroh-ua_website/pull/362))\n- Reorganized links to blogs for cloud services ( [#360](https://github.com/CIROH-UA/ciroh-ua_website/pull/360))\n- Video tutorial added to \"HydroShare and CIROH JupyterHub Intergration\" product page ( [#357](https://github.com/CIROH-UA/ciroh-ua_website/pull/357))\n- Reworked homepage display of CIROH sponsors, members, and partners ( [#356](https://github.com/CIROH-UA/ciroh-ua_website/pull/356))\n- Added link to CIROH Portal in DocuHub navbar ( [#355](https://github.com/CIROH-UA/ciroh-ua_website/pull/355))\n- All page tags have been refactored and condensed ( [#352](https://github.com/CIROH-UA/ciroh-ua_website/pull/352))\n- New landing page images for Products, Public Cloud Services, and Policies pages ( [#351](https://github.com/CIROH-UA/ciroh-ua_website/pull/351))\n- DataStream CLI key features clarified ( [#349](https://github.com/CIROH-UA/ciroh-ua_website/pull/349))\n- New social media icons in footer ( [#341](https://github.com/CIROH-UA/ciroh-ua_website/pull/341))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 18,
    "content": "## Blog posts and news updates\n\n- Blog post: [\"Google Cloud Next 2025: Innovation at Scale\"](https://docs.ciroh.org/blog/april-2025-update) ( [#345](https://github.com/CIROH-UA/ciroh-ua_website/pull/345))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 19,
    "content": "## Updates\n\n- Blog's left sidebar now correctly displays all posts ( [#350](https://github.com/CIROH-UA/ciroh-ua_website/pull/350))\n- News and Contribute pages improved on mobile ( [#340](https://github.com/CIROH-UA/ciroh-ua_website/pull/340))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 20,
    "content": "## New content\n\n- Community Hydrologic Modeling page now features tables detailing features and access methods ( [#290](https://github.com/CIROH-UA/ciroh-ua_website/pull/290))\n- Added NGIAB-TEEHR and NGIAB-CLIENT to products section ( [#320](https://github.com/CIROH-UA/ciroh-ua_website/pull/320))\n- Added Flood Inundation Mapping Framework to products section ( [#300](https://github.com/CIROH-UA/ciroh-ua_website/pull/300))\n- Added documentation on debugging Python package conflicts in CIROH JupyterHub ( [#308](https://github.com/CIROH-UA/ciroh-ua_website/pull/308))\n- Added tutorial for running datapreprocess on CIROH JupyterHub ( [#299](https://github.com/CIROH-UA/ciroh-ua_website/pull/299))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 21,
    "content": "## Blog posts and news updates\n\n- Blog post: [\"UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolution\"](https://docs.ciroh.org/blog/march-2025-update) ( [#310](https://github.com/CIROH-UA/ciroh-ua_website/pull/310))\n- News update: March 2025 ( [#306](https://github.com/CIROH-UA/ciroh-ua_website/pull/306))"
  },
  {
    "idurl": 8,
    "idtype": "text",
    "order": 22,
    "content": "## Updates\n\n- Office Hours page updated; added to homepage and Contact page ( [#337](https://github.com/CIROH-UA/ciroh-ua_website/pull/337))\n- Contribute page heavily reworked; now features GitHub issue templates ( [#309](https://github.com/CIROH-UA/ciroh-ua_website/pull/309))\n- Added link to National Water Model API documentation ( [#307](https://github.com/CIROH-UA/ciroh-ua_website/pull/307))\n- Added graphical headers to News tab ( [#305](https://github.com/CIROH-UA/ciroh-ua_website/pull/305))\n- Guidance on accessing services and resources refactored ( [#287](https://github.com/CIROH-UA/ciroh-ua_website/pull/287), [#294](https://github.com/CIROH-UA/ciroh-ua_website/pull/294))"
  },
  {
    "idurl": 10,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen In A Box\n\n![](https://docs.ciroh.org/img/logos/ngiab-blurb.png)\n\n[NextGen In A Box Website](https://ngiab.ciroh.org/) [NGIAB 101 Learning Module](https://docs.ciroh.org/training-NGIAB-101/)\n\n* * *\n\nNextGen In A Box is a community-accessible verison of NextGen Water Resources Modeling Framework ( [NextGen](https://github.com/NOAA-OWP/ngen)).\nIt simplifies deployment through containerization, which allows the entire NextGen workflow to be run using just a single script.\nAdditionally, the broader NextGen In A Box ecosystem offers essential tools to improve, validate, and visualize your models at every stage of the process."
  },
  {
    "idurl": 10,
    "idtype": "text",
    "order": 2,
    "content": "### Deployment Options\n\n- [**NGIAB-CloudInfra**](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker): A Docker-based NGIAB distribution suitable for both local and cloud-based workflows. ( [Installation](https://docs.ciroh.org/docs/products/ngiab/intro/install))\n- [**NGIAB-HPCInfra**](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-singularity): A Singularity-based NGIAB distribution suitable for high-performance computing (HPC) environments.\n- [**NextGen on CIROH-2i2c JupyterHub**](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c): A dedicated image on CIROH-2i2c JupyterHub for running NextGen In A Box."
  },
  {
    "idurl": 10,
    "idtype": "text",
    "order": 3,
    "content": "### Core Components\n\n- [**Data Preprocess**](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor): Simplifies data preparation for the NGIAB workflow through an interactive map interface and straightforward command line tools.\n- [**TEEHR Evaluation**](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr): Provides comprehensive model evaluation capabilities.\n- [**Data Visualizer**](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer): Delivers sophisticated geospatial and time series visualization.\n- [**NGIAB Calibration**](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration): Offers intuitive calibration for models in NGIAB workflows, including the CFE and NOAH-OWP modules.\n\n* * *"
  },
  {
    "idurl": 10,
    "idtype": "text",
    "order": 4,
    "content": "### Getting Started\n\n- [**NGIAB 101 Training Module**](https://docs.ciroh.org/training-NGIAB-101/): The best place to learn how to start using NGIAB.\n- [**NWM, NextGen, and NGIAB**](https://docs.ciroh.org/docs/products/ngiab/intro/what-is): A quick summary of the National Water Model, the NextGen Framework, and NGIAB's place in making them more accessible.\n- [**NGIAB at a Glance**](https://docs.ciroh.org/docs/products/ngiab/intro/at-a-glance): An overview of compatibility, capbilities, and access methods for tools within the NGIAB ecosystem.\n- [**Glossary**](https://docs.ciroh.org/docs/products/ngiab/intro/glossary): A reference for tools, file formats, and terminology that may be unfamiliar.\n- [**Directory Structure**](https://docs.ciroh.org/docs/products/ngiab/intro/directories): The directory structure used for running simulations with NextGen.\n\n* * *"
  },
  {
    "idurl": 10,
    "idtype": "text",
    "order": 5,
    "content": "### Other Resources\n\n- [**NGIAB Website**](https://ngiab.ciroh.org/): A high-level overview of everything NGIAB can do.\n- [**Community NextGen News**](https://docs.ciroh.org/news): Get the latest updates on NGIAB development and more.\n\n* * *\n\n[**üóÉÔ∏èIntro to NGIAB** \\\\\n5 items](https://docs.ciroh.org/docs/products/ngiab/intro/)[**üóÉÔ∏èDistributions** \\\\\n3 items](https://docs.ciroh.org/docs/products/ngiab/distributions/)[**üóÉÔ∏èComponents** \\\\\n5 items](https://docs.ciroh.org/docs/products/ngiab/components/)[**üìÑÔ∏èCyberinfrastructure and Community NextGen Office Hours** \\\\\nNeed help with NGIAB or CIROH Cyberinfrastructure. Join us for office hours!](https://docs.ciroh.org/docs/products/ngiab/office-hours)[**üìÑÔ∏èGitHub Repository Dashboard** \\\\\nA dashboard for viewing CIROH's managed repositories and workflows.](https://docs.ciroh.org/docs/products/ngiab/dashboard/)\n\n- [Deployment Options](https://docs.ciroh.org/docs/products/ngiab/#deployment-options)\n- [Core Components](https://docs.ciroh.org/docs/products/ngiab/#core-components)\n- [Getting Started](https://docs.ciroh.org/docs/products/ngiab/#getting-started)\n- [Other Resources](https://docs.ciroh.org/docs/products/ngiab/#other-resources)"
  },
  {
    "idurl": 11,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen Framework\n\nThe National Water Model (NWM) is a hydrologic model developed by **the National Oceanic and Atmospheric Administration (NOAA)** to simulate and forecast water conditions across the United States.\n\n- This water prediction model creates forecast guidance for over 3.4 million miles of rivers and streams across the United States and its territories\n\n- The NWM supercomputer is fed nonstop data covering everything from current stream-flow to the snowpack in mountain ranges above.\n\n- The job of the supercomputer is to take all of that data and produce stream flow forecasts for every stream and river in the continental United States and its territories. There are three different flavors of forecasts: short, medium, and long range.\n\n- Just like how your local weather station can tell you what the weather will be like tomorrow, in 3 days, or a week from now; the NWM can tell you what the stream flow will be like in 18 hours, 10 days, or even up to 30 days from now.\n\n- NOAA Central Library Seminar Recording from OWP (Next-Generation Water Resources Modeling): [https://youtu.be/DLIi3PruYxo](https://youtu.be/DLIi3PruYxo)"
  },
  {
    "idurl": 11,
    "idtype": "text",
    "order": 2,
    "content": "## Features - NWM3.0 [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/\\#features---nwm30 \"Direct link to Features - NWM3.0\")\n\n- First time provision of NWM Total Water Level guidance for coastal areas of the Continental United States (CONUS), Hawaii and Puerto Rico / U.S. Virgin Island domains. This is accomplished via use of the Semi-implicit Cross-scale Hydroscience Integrated System Model (SCHISM) integrated within the NWM, to couple NWM freshwater discharge estimates with oceanic forcing from the Surge and Tide Operational Forecast System (STOFS) and Probabilistic Tropical Storm Surge (P-SURGE) model. Output will be provided in both NetCDF as well as Standard Hydrometeorological Exchange Format (SHEF) format. Each NetCDF file contains full TWL domain output for one output time step, while each SHEF file contains timeseries station output for the full length of each simulation.\n\n- NWM Domain expansion to south-central Alaska (Cook Inlet, Copper River Basin, and Prince William Sound regions), enabling provision of NWM operational hydrologic model forecast guidance to this region.\n\n- Addition of the National Blend of Models (NBM) as a forcing source for NWM CONUS medium-range forecasts and Alaska short-range and medium-range forecasts.\n\n- Use of Multi-Radar Multi-Sensor (MRMS) precipitation as forcing for the NWM Analysis and Assimilation configuration over the Puerto Rico / U.S. Virgin Island domain."
  },
  {
    "idurl": 11,
    "idtype": "text",
    "order": 3,
    "content": "- Ingest of RFC-supplied reservoir outflow forecasts at 77 additional locations, bringing the total of such sites to 392.\n\n- Enhancements to the treatment of reservoirs, land surface parameters and calibration/regionalization approach leading to improvements in model skill."
  },
  {
    "idurl": 11,
    "idtype": "text",
    "order": 4,
    "content": "## Features - NWM2.1 [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/\\#features---nwm21 \"Direct link to Features - NWM2.1\")\n\nThe NWM currently runs in four configurations:\n\n- Analysis and assimilation provides a snapshotof current hydrologic conditions\n- Short-Range produces hourly forecasts of streamflow and hydrologic states out to 15 hours\n- Medium-Range produces 3-hourly forecasts out to 10 days\n- Long-Range generates 30-day ensemble forecasts.\n\nSource : [https://water.noaa.gov/about/nwm](https://water.noaa.gov/about/nwm)\n\n- [Features - NWM3.0](https://docs.ciroh.org/docs/products/nextgen-framework/#features---nwm30)\n- [Features - NWM2.1](https://docs.ciroh.org/docs/products/nextgen-framework/#features---nwm21)"
  },
  {
    "idurl": 12,
    "idtype": "text",
    "order": 1,
    "content": "# Hydrofabric\n\nHydrofabric is a collection of open-source R packages that provides tools for working with hydrologic data. It is designed to be user-friendly and extensible for researchers and practitioners."
  },
  {
    "idurl": 12,
    "idtype": "text",
    "order": 2,
    "content": "## Getting Started with Hydrofabric\n\nTo get started:\n\n1. Install hydrofabric from Github: \n   ```r\n   remotes::install_github(\"NOAA-OWP/hydrofabric\")\n   ```\n\n2. Attach the package: \n   ```r\n   library(hydrofabric)\n   ```\n\n3. Explore the [Hydrofabric Documentation](https://noaa-owp.github.io/hydrofabric/)"
  },
  {
    "idurl": 12,
    "idtype": "text",
    "order": 3,
    "content": "## Hydrofabric Resources\n\n- [Hydrofabric Code Repository](https://github.com/NOAA-OWP/hydrofabric)\n- [Hydrofabric Data](https://www.lynker-spatial.com/)\n\n**Tags:**\n- Education\n- NOAA\n- Hydrofabric"
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 1,
    "content": "# Research DataStream\n\ninfo\n\nThe Research DataStream forcing files and associated metadata are available through our AWS S3 Explorer. You can browse and access these regularly updated resources at [datastream.ciroh.org/index.html](http://datastream.ciroh.org/index.html). In addition to NextGen forcings, daily NextGen simulation outputs via Datastream will soon be available.\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/README.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/README.md).\n\nThe Research DataStream is an array of daily [NextGen](https://github.com/NOAA-OWP/ngen)-based hydrolgic simulations in the AWS cloud. An exciting aspect of the Research DataStream is the NextGen configuration is [open-sourced](https://github.com/CIROH-UA/ngen-datastream/tree/main/research_datastream/configuration) and [community editable](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/CONTRIBUTE.md), which allows any member of the hydrologic community to contribute to improving streamflow predictions. By making the NextGen forcings, outputs, and configuration publicly available, it is now possible to leverage regional expertise and incrementally improve streamflow predictions configured with the NextGen Framework.\nSee the Research DataStream related documentation:"
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 2,
    "content": "- **Find daily output data at:** [https://datastream.ciroh.org/index.html](https://datastream.ciroh.org/index.html)\n- **Make improvements to NextGen configuration:**\nFind out how you can contribute [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/CONTRIBUTE.md)!\n- **Current status and configuration:** Read [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/STATUS_AND_METADATA.md)!\n- **Infrastructure as Code:** See the template AWS architecture [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/terraform/ARCHITECTURE.md), which users can deploy within their own AWS account to issue and manage AWS server-based jobs.\n\n  - The actual research datastream deployment, which builds upon the template AWS infra, exists [here](https://github.com/CIROH-UA/ngen-datastream/tree/main/research_datastream/terraform_community) and is available for reference only."
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 3,
    "content": "# DataStreamCLI\n\nThe software backend of the Research DataStream is DataStreamCLI, which is a stand alone tool that automates the process of collecting and formatting input data for NextGen, orchestrating the NextGen run through NextGen In a Box (NGIAB), and handling outputs. This software allows users to run NextGen in an efficient, _relatively_ painless, and reproducible fashion while providing flexibility and integrations like hfsubset, NextGen In A Box, and TEEHR.\n\n[![datastream](https://github.com/CIROH-UA/ngen-datastream/raw/main/docs/images/datastreamcli.jpg)](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/images/datastreamcli.jpg)"
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 4,
    "content": "## Getting Started\n\n- **Installation:** Follow the [Installation Guide](https://github.com/CIROH-UA/ngen-datastream/blob/main/INSTALL.md) to prepare your environment for `DataStreamCLI`.\n- **Guide:** Start by running the [DataStreamCLI guide](https://github.com/CIROH-UA/ngen-datastream/blob/main/scripts/datastream_guide)! It is an interactive script that will provide a tour of the repo as well as help you form a command with `DataStreamCLI`.\n- **Status:** Check the [status page](https://github.com/CIROH-UA/ngen-datastream/blob/main/STATUS.md) for availability of tooling/integrations.\n- **Docs**: Make sure to review the [documentation](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/) for\n\n  - Available [NextGen models](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/NGEN_MODELS.md) and automated BMI configuration generation\n  - [Datastream options](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/DATASTREAM_OPTIONS.md)\n  - Input and output [directory structure](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/STANDARD_DIRECTORIES.md)\n  - A [usage guide](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/USAGE.md) for executing `DataStreamCLI` effectively\n  - A step-by-step [breakdown](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/BREAKDOWN.md) of `DataStreamCLI`'s internal workflow\n  - An explanation of the [Research DataStream](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/README.md)"
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 5,
    "content": "## Run DataStreamCLI\n\nThis example will execute a 24 hour NextGen simulation over the Palisade, Colorado watershed with CFE, SLOTH, PET, NOM, and t-route configuration distributed over 4 processes. The forcings used are the National Water Model v3 Retrospective.\n\nFirst, obtain a hydrofabric file for the gage you wish to model. There are several tooling options to use to obtain a geopackage. One of which, [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI), is maintained by Lynker and is integrated in DataStreamCLI.\n\nFor Palisade, Colorado:\n\n```\nhfsubset -w medium_range \\\n          -s nextgen \\\n          -v 2.1.1 \\\n          -l divides,flowlines,network,nexus,forcing-weights,flowpath-attributes,model-attributes \\\n          -o palisade.gpkg \\\n          -t hl \"Gages-09106150\"\n\n```\n\nThen feed the hydrofabric file to DataStreamCLI along with a few cli args to define the time domain and NextGen configuration\n\n```\n./scripts/datastream -s 202006200100 \\\n                    -e 202006210000 \\\n                    -C NWM_RETRO_V3 \\\n                    -d $(pwd)/data/datastream_test \\\n                    -g $(pwd)/palisade.gpkg \\\n                    -R $(pwd)/configs/ngen/realization_sloth_nom_cfe_pet_troute.json \\\n                    -n 4\n\n```\n\nAnd that's it! Outputs will exist at `$(pwd)/data/datastream_test/ngen-run/outputs`"
  },
  {
    "idurl": 13,
    "idtype": "text",
    "order": 6,
    "content": "## License\n\nThe entirety of `ngen-datastream` is distributed under [GNU General Public License v3.0 or later](https://github.com/CIROH-UA/ngen-datastream/blob/main/LICENSE.md)"
  },
  {
    "idurl": 14,
    "idtype": "text",
    "order": 1,
    "content": "# Evaluation\n\n* * *\n\n[**üìÑÔ∏èCSES** \\\\\nCommunity Streamflow Evaluation System](https://docs.ciroh.org/docs/products/evaluation/cses/)[**üìÑÔ∏èTEEHR** \\\\\nTools for Exploratory Evaluation in Hydrologic Research](https://docs.ciroh.org/docs/products/evaluation/rtiteehr/)"
  },
  {
    "idurl": 15,
    "idtype": "text",
    "order": 1,
    "content": "# Data Management\n\n* * *\n\n[**üóÉÔ∏èData Access** \\\\\n1 item](https://docs.ciroh.org/docs/products/data-management/dataaccess/)[**üìÑÔ∏èWater Prediction Node** \\\\\nWater Node Website](https://docs.ciroh.org/docs/products/data-management/waternode/)[**üìÑÔ∏èHydroServer** \\\\\nHydroServer Portal](https://docs.ciroh.org/docs/products/data-management/hydroserver/)[**üóÉÔ∏èNETWA** \\\\\n1 item](https://docs.ciroh.org/docs/products/data-management/netwa/)[**üìÑÔ∏èHydroShare** \\\\\nHydroShare](https://docs.ciroh.org/docs/products/data-management/hydroshare/)[**üìÑÔ∏èNWM BigQuery API** \\\\\nREST API backed by National Water Model data, developed on Google Cloud Platform](https://docs.ciroh.org/docs/products/data-management/bigquery-api/)"
  },
  {
    "idurl": 16,
    "idtype": "text",
    "order": 1,
    "content": "# Snow Sensing and Modeling Tools\n\n* * *\n\n[**üìÑÔ∏èIntro to Snow Observations Modeling Analysis** \\\\\nIntro to Snow Observations Modeling Analysis](https://docs.ciroh.org/docs/products/snow-tools/snow-intro/)[**üìÑÔ∏èSWEMLv2.0** \\\\\nSWEMLv2.0](https://docs.ciroh.org/docs/products/snow-tools/sweml-v2-0/)[**üìÑÔ∏èSnow Sensing** \\\\\nSnow Sensing](https://docs.ciroh.org/docs/products/snow-tools/snow-sensing/)[**üìÑÔ∏èOptimized Snow Sensor Location** \\\\\nOptimized Snow Sensor Location](https://docs.ciroh.org/docs/products/snow-tools/optimize-sensors/)"
  },
  {
    "idurl": 17,
    "idtype": "text",
    "order": 1,
    "content": "# Machine Learning and AI\n\n* * *\n\n[**üìÑÔ∏èSWEML** \\\\\nSnow Water Equivalent Machine Learning](https://docs.ciroh.org/docs/products/ml-ai/sweml/)[**üìÑÔ∏èNWM-ML** \\\\\nNational Water Model - Machine Learning](https://docs.ciroh.org/docs/products/ml-ai/nwm_ml/)"
  },
  {
    "idurl": 18,
    "idtype": "text",
    "order": 1,
    "content": "# Visualization and Analysis Tools\n\n* * *\n\n[**üìÑÔ∏èTethys-CSES** \\\\\nTethys-CSES is a Community Streamflow Evaluation System (CSES) Web Application.](https://docs.ciroh.org/docs/products/visualization/tethys-cses/)"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 1,
    "content": "# CIROH Research Portal"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 2,
    "content": "## Overview\n\nThe CIROH Research Portal is a centralized hub for water research resources, designed to:\n\n- Catalog and coordinate scientific research\n- Share interactive web applications\n- Provide research datasets\n- Host educational courses\n- Showcase publications"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 3,
    "content": "## Core Technologies ‚öôÔ∏è\n\nKey technologies include:\n- [Docusaurus](https://docusaurus.io/) - Documentation framework\n- [Tethys Platform](https://www.tethysplatform.org/) - Geospatial web app framework\n- [HydroShare](https://www.hydroshare.org/) - Water data collaboration\n- [HydroLearn](https://www.hydrolearn.org/) - Hydrologic education platform\n- AWS Cloud hosting"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 4,
    "content": "## Research Applications üì≤"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 5,
    "content": "### Native Applications üè†\n- TethysDash: Hydrological data visualization\n- Water Data Explorer: Multi-source water data analysis\n- SWEML: Snow Water Equivalent visualization\n- Grace Groundwater Tool\n- Snow Inspector\n- CSES: National Water Model evaluation system"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 6,
    "content": "### Proxy Applications üîó\n- FIM Visualization Deck\n- OWP NWM Map Viewer\n- CIROH JupyterHub\n- HydroShare\n- NFFA APP"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 7,
    "content": "## Contribution Opportunities\n\nUsers can contribute:\n- Applications\n- Datasets\n- Publications\n- Courses\n\nThrough forms on the [portal contribute page](https://portal.ciroh.org/contribute)"
  },
  {
    "idurl": 19,
    "idtype": "text",
    "order": 8,
    "content": "## Live Portal\nüåç Explore at: [https://portal.ciroh.org](https://portal.ciroh.org)"
  },
  {
    "idurl": 20,
    "idtype": "text",
    "order": 1,
    "content": "# Community Flood Inundation Mapping\n\n* * *\n\n[**üìÑÔ∏èFIM as a Service** \\\\\nFIM as a Service](https://docs.ciroh.org/docs/products/community-fim/fimserv/)[**üìÑÔ∏èFIM Evaluation Framework** \\\\\nFIM Evaluation Framework](https://docs.ciroh.org/docs/products/community-fim/fimeval/)[**üìÑÔ∏èFIM Database for Multi-Model Visualization** \\\\\nDocumentation for creating a FIM database for flood scenarios visualization and comparison.](https://docs.ciroh.org/docs/products/community-fim/fim-database/)"
  },
  {
    "idurl": 21,
    "idtype": "text",
    "order": 1,
    "content": "# Mobile Apps\n\n* * *\n\n[**üìÑÔ∏èCIROH RIVR App** \\\\\nCIROH RIVR App](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/)"
  },
  {
    "idurl": 22,
    "idtype": "text",
    "order": 1,
    "content": "# Intro to NGIAB\n\n* * *\n\nThis tab contains introductory and ecosystem-wide information for NextGen In A Box.\n\nIf you're starting completely from scratch, the [NGIAB 101](https://docs.ciroh.org/training-NGIAB-101/) learning module offers the best way to get started.\n\n* * *\n\n[**üìÑÔ∏èNWM, NextGen, and NGIAB** \\\\\nAn introduction to the NextGen Framework and its related models.](https://docs.ciroh.org/docs/products/ngiab/intro/what-is)[**üìÑÔ∏èNGIAB at a Glance** \\\\\nA tour of the capabilities of the NGIAB ecosystem.](https://docs.ciroh.org/docs/products/ngiab/intro/at-a-glance)[**üìÑÔ∏èInstalling NGIAB Locally** \\\\\nA guide to install NGIAB locally.](https://docs.ciroh.org/docs/products/ngiab/intro/install)[**üìÑÔ∏èGlossary** \\\\\nA quick reference for terms and jargon pertaining to NGIAB.](https://docs.ciroh.org/docs/products/ngiab/intro/glossary)[**üìÑÔ∏èRun Configuration Directory Structure** \\\\\nThe directory structure used by NGIAB to define model runs.](https://docs.ciroh.org/docs/products/ngiab/intro/directories)"
  },
  {
    "idurl": 23,
    "idtype": "text",
    "order": 1,
    "content": "# Distributions\n\n* * *\n\nThis tab contains documentation on the various distributions of NGIAB currently available.\n\nIf you're unsure of which distribution to use, [NGIAB-CloudInfra](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/) is the most appropriate option for most users.\n\n* * *\n\n[**üóÉÔ∏èNGIAB-CloudInfra** \\\\\n6 items](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/)[**üìÑÔ∏èNGIAB-HPCInfra** \\\\\nA Singularity-based NGIAB distribution; suitable for high-performance computing (HPC) environments.](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-singularity/)[**üìÑÔ∏èNextGen on CIROH JupyterHub** \\\\\nA dedicated image on CIROH-2i2c JupyterHub for running NextGen In A Box.](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/)"
  },
  {
    "idurl": 24,
    "idtype": "text",
    "order": 1,
    "content": "# Components\n\n* * *\n\nThis tab contains documentation on the many components that interface with NextGen In A Box.\n\nThese tools offer powerful, streamlined options to get the most out of the NGIAB ecosystem.\n\n* * *\n\n[**üìÑÔ∏èNGIAB Data Preprocess** \\\\\nAutomatically process data for the NextGen directory structure.](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor/)[**üìÑÔ∏èNGIAB TEEHR Integration** \\\\\nUse the TEEHR toolkit to evaluate your model runs.](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr/)[**üìÑÔ∏èNGIAB Tethys Visualization Integration** \\\\\nVisualize your model output with both spatial navigation and temporal graphing.](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer/)[**üìÑÔ∏èNGIAB Calibration** \\\\\nCallibrate your models to improve their output.](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration/)[**üìÑÔ∏èCommunity Hydrofabric Patcher** \\\\\nCommunity Hydrofabric Patcher](https://docs.ciroh.org/docs/products/ngiab/components/community-hydrofabric/)"
  },
  {
    "idurl": 25,
    "idtype": "text",
    "order": 1,
    "content": "# Cyberinfrastructure and Community NextGen Office Hours\n\nJoin us for our monthly sessions focused on CIROH's research computing resources and the Community NextGen project.\n\nThese office hours provide an excellent opportunity to engage with experts, ask questions, and stay updated on the latest developments."
  },
  {
    "idurl": 25,
    "idtype": "text",
    "order": 2,
    "content": "## How to Join\n\nEmail us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) to receive the Teams Meeting link and calendar invitation."
  },
  {
    "idurl": 25,
    "idtype": "text",
    "order": 3,
    "content": "## What We Cover\n\n- **CIROH Research Cyberinfrastructure**: Cloud computing resources, on-premise computing, and external computing access\n- **Community NextGen**: Advanced hydrologic modeling tools"
  },
  {
    "idurl": 25,
    "idtype": "text",
    "order": 4,
    "content": "## Resources\n\n- [CIROH Cyberinfrastructure Documentation](https://docs.ciroh.org/docs/services/intro)\n- [NGIAB Ecosystem Documentation](https://docs.ciroh.org/docs/products/ngiab)\n\n- [How to Join](https://docs.ciroh.org/docs/products/ngiab/office-hours/#how-to-join)\n- [What We Cover](https://docs.ciroh.org/docs/products/ngiab/office-hours/#what-we-cover)\n- [Resources](https://docs.ciroh.org/docs/products/ngiab/office-hours/#resources)"
  },
  {
    "idurl": 26,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB Dashboard\n\n- Introduction\n- Community NextGen related repositories\n- CIROH Workflow Statuses"
  },
  {
    "idurl": 26,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction\n\nThis page contains the list of Community NextGen related repositories that CIROH is maintaining.\n\nSome repositories originate from the NOAA-OWP organization, but are now forks and maintained by CIROH.\n\nLoading..."
  },
  {
    "idurl": 27,
    "idtype": "text",
    "order": 1,
    "content": "# ngen\n\nngen: Next Generation Water Modeling Engine and Framework Prototype\n\nngen is a model agnostic framework designed for building and integrating models rather than being a model itself. It focuses on a data-centric process that abstracts the addition of processes and data behind a standard. This design allows for greater flexibility and standardization in model creation and integration."
  },
  {
    "idurl": 27,
    "idtype": "text",
    "order": 2,
    "content": "## Key Features of ngen: [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/ngen/\\#key-features-of-ngen \"Direct link to Key Features of ngen:\")\n\n- Model Agnostic: ngen is not a specific model but rather a framework for building and integrating different models.\n- Data-Centric: ngen emphasizes a data-centric approach, ensuring that data is central to the modeling process.\n- Flexible and Standardized: ngen provides a standardized approach to model creation and integration, allowing for greater flexibility and ease of use."
  },
  {
    "idurl": 27,
    "idtype": "text",
    "order": 3,
    "content": "## Resources: [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/ngen/\\#resources \"Direct link to Resources:\")\n\n- Public GitHub Repository: [NOAA-OWP/ngen](https://github.com/NOAA-OWP/ngen)\n- Documentation: [ngen documentation](https://noaa-owp.github.io/ngen/)\n\n- [Key Features of ngen:](https://docs.ciroh.org/docs/products/nextgen-framework/ngen/#key-features-of-ngen)\n- [Resources:](https://docs.ciroh.org/docs/products/nextgen-framework/ngen/#resources)"
  },
  {
    "idurl": 28,
    "idtype": "text",
    "order": 1,
    "content": "# t-route\n\ntroute is a Python package for generating channel networks from digital elevation models (DEMs). It uses a tree-based approach to identify and connect channels, resulting in realistic and hydrologically consistent channel networks."
  },
  {
    "idurl": 28,
    "idtype": "text",
    "order": 2,
    "content": "## Key Features of t-route: [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/troute/\\#key-features-of-t-route \"Direct link to Key Features of t-route:\")\n\n- Tree-Based Approach: t-route uses a tree-based algorithm to identify and connect channels, ensuring hydrologically consistent channel networks.\n- High-Resolution DEMs: t-route can generate channel networks from high-resolution DEMs, capturing detailed channel morphology.\n- Flexible and Extensible: t-route provides a flexible framework for customizing the channel generation process and extending its capabilities."
  },
  {
    "idurl": 28,
    "idtype": "text",
    "order": 3,
    "content": "## Resources: [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/troute/\\#resources \"Direct link to Resources:\")\n\n- Public GitHub Repository: [NOAA-OWP/t-route](https://github.com/NOAA-OWP/t-route)\n\n- [Key Features of t-route:](https://docs.ciroh.org/docs/products/nextgen-framework/troute/#key-features-of-t-route)\n- [Resources:](https://docs.ciroh.org/docs/products/nextgen-framework/troute/#resources)"
  },
  {
    "idurl": 29,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen"
  },
  {
    "idurl": 29,
    "idtype": "text",
    "order": 2,
    "content": "## Speaker: Fred L. Ogden, Ph.D., P.E., Chief Scientist (ST), NOAA/NWS Office of Water Prediction [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/nextgen/\\#speaker--fred-l-ogden-phd-pe-chief-scientist-st-noaanws-office-of-water-prediction \"Direct link to Speaker:  Fred L. Ogden, Ph.D., P.E., Chief Scientist (ST), NOAA/NWS Office of Water Prediction\")\n\nThe Basic Model Interface (BMI) standard employed in the Next Generation Water Resources Modeling (NextGen) framework enables the construction of both mimic and novel model formulations. This involves the ordered execution of modules designed to simulate individual processes or fluxes and the integration of these over time on various control volumes to simulate the temporal evolution of model states. These are referred to as \"multi-BMI\" formulations.\n\nCurrently, the order of execution of modules within a multi-BMI model formulation is determined by the ordering of modules in the run realization file. This presentation showcases model/module developments undertaken to date in this regard and discusses knowledge gaps and needs. Additionally, it proposes potential coding standards for modules that allow for the monitoring of conservation law enforcement at the framework level for different model formulations using various internal discretizations."
  },
  {
    "idurl": 29,
    "idtype": "text",
    "order": 3,
    "content": "## Building Novel & Mimic Model Formulations Using NextGen Framework: [‚Äã](https://docs.ciroh.org/docs/products/nextgen-framework/nextgen/\\#building-novel--mimic-model-formulations-using-nextgen-framework \"Direct link to Building Novel & Mimic Model Formulations Using NextGen Framework:\")\n\nBuilding Novel & Mimic Model Formulations Using NextGen Framework | CIROH Webinar (November 2023) - YouTube\n\n[Photo image of CIROH](https://www.youtube.com/channel/UC-UdUfgsijc7el_be8LSEDQ?embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F)\n\nCIROH\n\n89 subscribers\n\n[Building Novel & Mimic Model Formulations Using NextGen Framework | CIROH Webinar (November 2023)](https://www.youtube.com/watch?v=1Wy6y37kjfY)\n\nCIROH\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos"
  },
  {
    "idurl": 29,
    "idtype": "text",
    "order": 4,
    "content": "## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=1Wy6y37kjfY&embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F)\n\n0:00\n\n0:00 / 1:01:40\n‚Ä¢Live\n\n‚Ä¢\n\n- [Speaker: Fred L. Ogden, Ph.D., P.E., Chief Scientist (ST), NOAA/NWS Office of Water Prediction](https://docs.ciroh.org/docs/products/nextgen-framework/nextgen/#speaker--fred-l-ogden-phd-pe-chief-scientist-st-noaanws-office-of-water-prediction)\n- [Building Novel & Mimic Model Formulations Using NextGen Framework:](https://docs.ciroh.org/docs/products/nextgen-framework/nextgen/#building-novel--mimic-model-formulations-using-nextgen-framework)"
  },
  {
    "idurl": 30,
    "idtype": "text",
    "order": 1,
    "content": "# CLI\n\n[**üìÑÔ∏èInstallation** \\\\\nLearn what you'll need to install DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/install)[**üìÑÔ∏èUsage Guide** \\\\\nHow to get the most out of DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/usage)[**üìÑÔ∏èAvailable Models** \\\\\nThe available models in DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/models)[**üìÑÔ∏èDirectory Structure** \\\\\nThe directory structure in DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/directories)[**üìÑÔ∏èCLI Options** \\\\\nThe available options in DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/options)[**üìÑÔ∏èInternal Breakdown** \\\\\nAn end-to-end tour of the internal workings of DataStreamCLI.](https://docs.ciroh.org/docs/products/research-datastream/cli/breakdown)"
  },
  {
    "idurl": 31,
    "idtype": "text",
    "order": 1,
    "content": "# Components\n\n[**üìÑÔ∏èDatastream Python Tools** \\\\\nScripts to create ngen bmi module configuration files and validate ngen-run packages.](https://docs.ciroh.org/docs/products/research-datastream/components/python_tools/)[**üóÉÔ∏èForcing Processor** \\\\\n1 item](https://docs.ciroh.org/docs/products/research-datastream/components/forcingprocessor/)[**üìÑÔ∏èDatastream Configuration** \\\\\nNextGen Framework Research Datastream](https://docs.ciroh.org/docs/products/research-datastream/components/research_datastream/)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 1,
    "content": "# Status\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/STATUS.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/STATUS.md)."
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 2,
    "content": "# Research DataStream"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 3,
    "content": "- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/research_datastream_terraform.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/research_datastream_terraform.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_fp.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_fp.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_01.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_01.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_02.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_02.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03W.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03W.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03N.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03N.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03S.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_03S.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_04.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_04.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_05.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_05.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_06.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_06.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_07.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_07.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_08.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_08.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_09.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_09.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_10L.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_10L.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_10U.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_10U.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_11.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_11.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_12.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_12.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_13.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_13.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_14.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_14.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_15.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_15.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_16.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_16.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_17.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_17.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_18.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_research_datastream_vpu_18.yaml/badge.svg)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 4,
    "content": "# DataStreamCLI"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 5,
    "content": "- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_options.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_options.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab_troute_v2_1.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab_troute_v2_1.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab_troute_v2_2.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_datastream_ngiab_troute_v2_2.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_hfsubset_v2_1.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_hfsubset_v2_1.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_hfsubset_v2_2.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_hfsubset_v2_2.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_teehr_integration.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_teehr_integration.yaml/badge.svg)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 6,
    "content": "# DataStreamCLI ForcingProcessor Tools\n\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_aws_sources.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_aws_sources.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_gcs_sources.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_gcs_sources.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_plotting.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_plotting.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_weights.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/forcingprocessor_weights.yaml/badge.svg)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 7,
    "content": "# DataStreamCLI Python Tools\n\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/datastream_python.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/datastream_python.yaml/badge.svg)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 8,
    "content": "# Docker\n\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_docker_x86.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_docker_x86.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_push_docker_x86.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_push_docker_x86.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_push_docker_arm.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/build_test_push_docker_arm.yaml/badge.svg)"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 9,
    "content": "# Community Hydrofabric"
  },
  {
    "idurl": 32,
    "idtype": "text",
    "order": 10,
    "content": "- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_01.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_01.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_02.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_02.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03N.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03N.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03N.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03N.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03S.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_03S.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_04.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_04.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_05.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_05.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_06.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_06.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_07.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_07.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_08.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_08.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_09.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_09.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_10L.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_10L.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_10U.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_10U.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_11.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_11.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_12.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_12.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_13.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_13.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_14.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_14.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_15.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_15.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_16.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_16.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_17.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_17.yaml/badge.svg)\n- [![](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_18.yaml/badge.svg)](https://github.com/CIROH-UA/ngen-datastream/actions/workflows/test_community_hydrofabric_vpu_18.yaml/badge.svg)"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 1,
    "content": "# CSES\n\nCommunity Streamflow Evaluation System (CSES) is a Python-based, user friendly, fast, and model agnostic streamflow evaluator tool. This tool can be used to evaluate any hydrological model that uses NHDPlus dataset. It allows a user to evaluate the performance of a hydrological model at the collocated USGS gauges and NHDPlus stream reaches. This Python-based tool helps visualize the results and investigate the model performance interactively. The current version of the tool is available on GitHub and can be accessed using the following link.\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/Community-Streamflow-Evaluation-System/blob/main/README.md](https://github.com/CIROH-UA/Community-Streamflow-Evaluation-System/blob/main/README.md).\n\n[![Github_top](https://user-images.githubusercontent.com/33735397/206313977-e67ba652-3340-4a1b-b1d1-141d8d5001f2.PNG)](https://user-images.githubusercontent.com/33735397/206313977-e67ba652-3340-4a1b-b1d1-141d8d5001f2.PNG)"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 2,
    "content": "# Community Streamflow Evaluation System (CSES)"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 3,
    "content": "[![GitHub](https://camo.githubusercontent.com/40d4219549ca27cfaaeb305a1d71502c6a99de211016c535836ea1d63d58cb68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f6c6f676f3d476974487562267374796c653d706c6173746963)](https://camo.githubusercontent.com/40d4219549ca27cfaaeb305a1d71502c6a99de211016c535836ea1d63d58cb68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f6c6f676f3d476974487562267374796c653d706c6173746963)[![GitHub top language](https://camo.githubusercontent.com/02c8977bdb708eb0580786ed92a3b28030e4e1fd640642ce3cdedabade7e9088/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f746f702f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/02c8977bdb708eb0580786ed92a3b28030e4e1fd640642ce3cdedabade7e9088/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f746f702f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub repo size](https://camo.githubusercontent.com/e8f9a5fc333b2659dc4161da882d904a66c6bcda2d95f9a206ea004be56cd7a7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f6c6f676f3d476974687562267374796c653d706c6173746963)](https://camo.githubusercontent.com/e8f9a5fc333b2659dc4161da882d904a66c6bcda2d95f9a206ea004be56cd7a7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f6c6f676f3d476974687562267374796c653d706c6173746963)[![GitHub language count](https://camo.githubusercontent.com/c24d311b5c6f3aac014746b13ca15a9d92118153ef3b2013cd69f94a9894844b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f636f756e742f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/c24d311b5c6f3aac014746b13ca15a9d92118153ef3b2013cd69f94a9894844b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f636f756e742f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub commit activity](https://camo.githubusercontent.com/fbf45c41f1e74e6f16be5d3ec26a3eb447da2a6d1aec9a686f3fe4385193e33e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/fbf45c41f1e74e6f16be5d3ec26a3eb447da2a6d1aec9a686f3fe4385193e33e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub Pipenv locked Python version](https://camo.githubusercontent.com/8e4280f77875898c4ed058e64a290a53b444f2cbfcb02985d06260fb65b78f9e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f706970656e762f6c6f636b65642f707974686f6e2d76657273696f6e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/8e4280f77875898c4ed058e64a290a53b444f2cbfcb02985d06260fb65b78f9e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f706970656e762f6c6f636b65642f707974686f6e2d76657273696f6e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub branch checks state](https://camo.githubusercontent.com/29e1e12d85f418f1279349c84b9d55125875ac395645a3a6f131d6195158ccee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636865636b732d7374617475732f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d2f6d61696e3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/29e1e12d85f418f1279349c84b9d55125875ac395645a3a6f131d6195158ccee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636865636b732d7374617475732f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d2f6d61696e3f7374796c653d706c6173746963)[![GitHub issues](https://camo.githubusercontent.com/47e1df69ed9cdc180cb2a705afdb93f87d22b18b1099ea77f0aaa1177c0e94a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/47e1df69ed9cdc180cb2a705afdb93f87d22b18b1099ea77f0aaa1177c0e94a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub milestones](https://camo.githubusercontent.com/fb4aa47d2a564e91584795880ca24d83dde2baccbc143a9b4a8372c1356a89d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f636c6f7365642f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/fb4aa47d2a564e91584795880ca24d83dde2baccbc143a9b4a8372c1356a89d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f636c6f7365642f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub milestones](https://camo.githubusercontent.com/d007f4662ccb559290101391ba64979855c89b0bb92cdfff6c0807005ebd26a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f6f70656e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/d007f4662ccb559290101391ba64979855c89b0bb92cdfff6c0807005ebd26a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f6f70656e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)[![GitHub milestones](https://camo.githubusercontent.com/d007f4662ccb559290101391ba64979855c89b0bb92cdfff6c0807005ebd26a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f6f70656e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)](https://camo.githubusercontent.com/d007f4662ccb559290101391ba64979855c89b0bb92cdfff6c0807005ebd26a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6d696c6573746f6e65732f6f70656e2f77686974656c696768746e696e673435302f436f6d6d756e6974792d53747265616d666c6f772d4576616c756174696f6e2d53797374656d3f7374796c653d706c6173746963)"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 4,
    "content": "A Novel Community Streamflow Evaluation System (CSES) to evaluate hydrological model performance using a standardized NHDPlus data model.\nCSES evaluates modeled streamflow to a repository of over 5,000 in situ USGS monitoring sites, with interactive visualizations supporting an in-depth analysis."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 5,
    "content": "## Application Overview National-scale streamflow modeling remains a modern challenge, as changes in the underlying hydrology from land use and land cover (LULC) change, anthropogentic streamflow modification, and general process components (reach length, hydrogeophysical processes, precipitation, temperature, etc) greatly influence hydrological modeling. In a changing climate, there is a need to anticipate flood intensity, impacts of groundwater depletion on streamflow, western mountain low-flow events, eastern rain-on-snow events, storm-induced flooding, and other severe environmental problems that challenge the management of water resources. Given the National Water Model (NWM) bridges the gap between the spatially coarse USGS streamflow observations by providing a near-continuous 2.7 million reach predictions of streamflow using the standardized NHDPlus framework, there lies the potential to improve upon the capabilities of the model by characterizing predictive performance across the heterogeneity of processes and land covers present at the national scale. The python-based Community-Streamflow-Evaluation-System package provides a foundation to evaluate national hydrography dataset (nhd) based model outputs with colocated USGS/NWIS streamflow monitoring stations (parameter: 060) without the need to download in-situ or NWM v2.1 data (NWM v3.0 coming soon!)."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 6,
    "content": "The package contains three key methods for evaluation: state-based LULC, HUC level analysis, and USGS station-based analysis. Below is a description of each method and application. Designed to assess NWM version 2.1 retrospective performance, by using the exemplified data model the tool can evaluate other model predictions, with the motivation to improve regionally dominant hydrological modeling skill. By using Community-Streamflow-Evaluation-System, researchers can identify locations where a model may benefit from further training/calibration/parameterization or a need for new model processes/features (e.g., integration of reservoir release operations) to ultimately create new post-processing methods and/or hydrological modeling formulations to improve streamflow prediction capabilities with respect to modeling needs (e.g., stormflow, supply, emergency management, flooding, etc)."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 7,
    "content": "### Data Access\n\nCommunity-Streamflow-Evaluation-System leverages USGS/NWIS observations from 1980-2020 and colocated and while all data is publically available through the respective agencies, we found the download time to be preventative for a timely model evaluation.\nThe Alabama Water Institute at the University of Alabama hosts NWM v2.1 retrospective for all colocated USGS monitoring stations at a daily temporal resolution and provides the data free of charge via access to Amazon AWS S3 cloud storage.\nCommunity-Streamflow-Evaluation-System can quickly access observed and predicted data supporting a fast and repeatable tool for evaluating modeled streamflow performance."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 8,
    "content": "## Dependencies (versions, environments)\n\nPython: Version 3.9.12."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 9,
    "content": "### Required packages\n\nThe included requirements.txt file should set up the correct Community-Streamflow-Evaluation-System environment.\nTo use Community-Streamflow-Evaluation-System, create a virtual Python environment and run the requirement.txt file to ensure all package versions are correct.\nTo get started, click the [Here](https://github.com/CIROH-UA/Community-Streamflow-Evaluation-System/blob/main/Getting%20Started.md)"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 10,
    "content": "## Streamflow Evaluation Options\n\nEach streamflow evaluation method requires similar inputs, including a start date, end date, and model.\nThere are currently three different evaluation classes, each providing the user with a unique method for evaluating streamflow modeling performance:\n\n- Class Eval_State(): Modeled Streamflow Evaluation by StreamStats\n- Class HUC_Eval(): Modeled Streamflow Evaluation by Hydrologic Unit Code (HUC)\n- Class Reach_Eval(): - NHD - USGS Streamflow Evaluation\n\nFor all examples, the predictions are from the NWM v2.1 retrospective.\nPlease see the Examples folder for more information on applying each specific class."
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 11,
    "content": "### Modeled Streamflow Evaluation by StreamStats\n\nTo determine how LULC affects the predictive performance of streamflow models, Community-Streamflow-Evaluation-System uses StreamStats to categorize the watershed upstream of each USGS monitoring site by watershed characteristics.\nPlease see the State Land Use - Land Cover Evaluation.md readme to use the tool.\n\n[![LULC_mapping](https://user-images.githubusercontent.com/33735397/205775870-5efab8e2-57ce-4ecb-b6c1-012909ece220.PNG)](https://user-images.githubusercontent.com/33735397/205775870-5efab8e2-57ce-4ecb-b6c1-012909ece220.PNG)\n\n_Running the Community-Streamflow-Evaluation-System LULC_Eval class loads, processes, and visualizes model performance for the state, category, and size of interest_\n\n[![LULC_mapping_highlight](https://user-images.githubusercontent.com/33735397/205776459-355507b4-2036-4eca-8bb3-fc88debbebef.PNG)](https://user-images.githubusercontent.com/33735397/205776459-355507b4-2036-4eca-8bb3-fc88debbebef.PNG)\n\n_By clicking on a marker a popup of the modeled vs. observed performance at the inputted temporal frequency will appear_\n\n[![LULC_holoviews](https://user-images.githubusercontent.com/33735397/205777709-65a8e6d8-0d7a-42e5-81b3-819462cb6e6a.PNG)](https://user-images.githubusercontent.com/33735397/205777709-65a8e6d8-0d7a-42e5-81b3-819462cb6e6a.PNG)\n\n_The Community-Streamflow-Evaluation-System supports an interactive engagement with model results_"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 12,
    "content": "### Modeled Streamflow Evaluation by Hydrologic Unit Code (HUC)\n\nThe HUC_Eval class allows the user to evaluate modeled streamflow with observed in situ NWIS monitoring sites for watershed(s) of interest.\nThe user can input multiple watersheds (e.g., Great Salt Lake: ['1601', '1602'])\nThe user must enter a start date, end date, watersheds, and model to compare (NWM v2.1 is set up).\nNWM retrospective data spans from 1980 - 2020, and USGS/NWIS data is location-dependent.\n\n[![LULC_HUC_GSL](https://user-images.githubusercontent.com/33735397/206265320-7c640b40-830e-41ed-8e3f-67a2b20984c5.PNG)](https://user-images.githubusercontent.com/33735397/206265320-7c640b40-830e-41ed-8e3f-67a2b20984c5.PNG)\n\n_The HUC_Eval class loads all USGS and colocated modeled NHD reaches for evaluation._\n_Color coding of the markers allows for quick identification of poor and well-performing reaches and clicking on the markers will put up a modeled vs. observed graph at the desired temporal resolution._\n\n[![LULC_holoviews_HUCEval](https://user-images.githubusercontent.com/33735397/206265779-5417343f-ed40-4704-b8bc-12ada2672259.PNG)](https://user-images.githubusercontent.com/33735397/206265779-5417343f-ed40-4704-b8bc-12ada2672259.PNG)\n\n_Similar to the State_Eval class, the HUC_Eval class supports a more in-depth graphical analysis of the modeled vs. observed using the holoviews package_"
  },
  {
    "idurl": 33,
    "idtype": "text",
    "order": 13,
    "content": "### Modeled Streamflow Evaluation - NHD - USGS Streamflow Evaluation\n\nThe Reach_Eval class allows the user to evaluate modeled streamflow with selected NWIS monitoring sites of interest.\nThe user can input multiple USGS sites (e.g., ['02378780', '02339495', '02342500'])\nSimilar to the other classes, enter a start date, end date, and model to compare (NWM v2.1 is set up).\nNWM retrospective data spans from 1980 to 2020, and USGS/NWIS data is location-dependent\n\n[![LULC_ReachEval_map](https://user-images.githubusercontent.com/33735397/206266617-f06c9836-0193-4f6f-94f9-11982272d34d.PNG)](https://user-images.githubusercontent.com/33735397/206266617-f06c9836-0193-4f6f-94f9-11982272d34d.PNG)\n\n_The Reach_Eval class quickly loads and maps the selected USGS streamflow monitoring locations, along with the colocated model predictions of NHD reaches._\n_The color code of the marker indicates the model performance at the respective USGS monitoring station site, and by clicking on a marker, the interactive map produces a graph at the desired temporal resolution._\n\n[![LULC_holoviews_Reach_Eval](https://user-images.githubusercontent.com/33735397/206267196-749bb94d-aa57-4d24-9b4e-97e7567e1fc0.PNG)](https://user-images.githubusercontent.com/33735397/206267196-749bb94d-aa57-4d24-9b4e-97e7567e1fc0.PNG)\n\n_Similar to the State_Eval and HUC_Eval classes, the Reach_Eval class supports a more in-depth graphical analysis of the modeled vs. observed using the holoviews package_"
  },
  {
    "idurl": 34,
    "idtype": "table",
    "order": 1,
    "content": "|  |  |\n| --- | --- |\n| [![alt text](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 2,
    "content": "# TEEHR\n\nTEEHR (pronounced \"tier\") is a python tool set for loading, storing, processing and visualizing hydrologic data, particularly National Water Model data, for the purpose of exploring and evaluating the datasets to assess their skill and performance.\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/RTIInternational/teehr/blob/main/README.md](https://github.com/RTIInternational/teehr/blob/main/README.md).\n\n[![alt text](https://github.com/RTIInternational/teehr/raw/main/docs/images/teehr.png)](https://github.com/RTIInternational/teehr/blob/main/docs/images/teehr.png)"
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 3,
    "content": "# TEEHR - Tools for Exploratory Evaluation in Hydrologic Research\n\nTEEHR (pronounced \"tier\") is a python tool set for loading, storing,\nprocessing and visualizing hydrologic data, particularly National Water\nModel data, for the purpose of exploring and evaluating the datasets to\nassess their skill and performance.\n\nNOTE: THIS PROJECT IS UNDER DEVELOPMENT - EXPECT TO FIND BROKEN AND INCOMPLETE CODE."
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 4,
    "content": "## Documentation\n\n[TEEHR Documentation](https://rtiinternational.github.io/teehr/)"
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 5,
    "content": "## How to Install TEEHR (macOS/Linux)\n\nThe easiest way to install TEEHR is from PyPI using `pip`. If using `pip` to install TEEHR, we recommend installing TEEHR in a virtual environment. Detailed installation instuctions for macOS/Linux users are available [here](https://rtiinternational.github.io/teehr/getting_started/index.html#installation-guide-for-macos-linux) under 'Installation Guide for macOS & Linux'."
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 6,
    "content": "## How to Install TEEHR (Windows)\n\nCurrently, TEEHR dependencies require users install on Linux or macOS. To use TEEHR on Windows, we recommend Windows Subsystem for Linux (WSL). Detailed installation instructions for Windows users are available [here](https://rtiinternational.github.io/teehr/getting_started/index.html#installation-guide-for-windows) under 'Installation Guide for Windows'."
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 7,
    "content": "## Examples\n\nFor examples of how to use TEEHR, see the [examples](https://rtiinternational.github.io/teehr/user_guide/index.html). We will maintain a basic set of example Jupyter Notebooks demonstrating how to use the TEEHR tools."
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 8,
    "content": "## Resources\n\nIn May of 2023 we put on a workshop at the CIROH 1st Annual Training and Developers Conference. The workshop materials and presentation are available in the workshop GitHub repository: [teehr-may-2023-workshop](https://github.com/RTIInternational/teehr-may-2023-workshop). This workshop was based on version 0.1.0."
  },
  {
    "idurl": 34,
    "idtype": "text",
    "order": 9,
    "content": "## Versioning\n\nThe TEEHR project follows semantic versioning as described here: [https://semver.org/](https://semver.org/).\nNote, per the specification, \"Major version zero (0.y.z) is for initial development. Anything MAY change at any time. The public API SHOULD NOT be considered stable.\". We are solidly in \"major version zero\" territory, and trying to move fast, so expect breaking changes often."
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 1,
    "content": "# Data Access\n\nWithin the CIROH projects, we encounter a wide range of data resources and data access inquiries. One of the most frequently asked questions is, \"How can I obtain access to xyz-resource?\". To help with answering that question, we have documented some of the most common data access methods and resources here, with links to additional sites to dive deeper."
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 2,
    "content": "## Input and Output Data of the National Water Model [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#input-and-output-data-of-the-national-water-model \"Direct link to Input and Output Data of the National Water Model\")\n\nHere, you will find resources that grant access to the input data used and the output data produced by the operational national water model."
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 3,
    "content": "### Official NOMADS Resource [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#official-nomads-resource \"Direct link to Official NOMADS Resource\")\n\nThe official NWM meteorological inputs and hydrology and routing outputs are accessible through both HTTP and FTP. These resources are provided by the National Center for Environmental Prediction (NCEP) at the following locations:\n\n- NOMADS - NOAA Operational Model Archive and Distribution System\n  - [HTTP](https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm)\n  - [FTP](ftp://ftpprd.ncep.noaa.gov/pub/data/nccf/com/nwm)\n\nAs of October 24, 2023, these resources include the following directories:\n\n```codeBlockLines_e6Vv\npara_post-processed/    22-Sep-2023 20:37    -\npost-processed/         02-Nov-2020 14:31    -\nprod/                   24-Oct-2023 00:18    -\nv3.0/                   24-Oct-2023 00:18    -\n\n```\n\nThe `para_post-processed` directory lacks specific documentation, although the \"para\" designation suggests it is a \"parallel\" execution, indicating a candidate production run under testing for operational use. In the post-processed dataset, you will find the following subdirectories:"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 4,
    "content": "- [NOMADS post-processed](https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/post-processed/)\n  - RFC: Outputs filtered down to RFC locations.\n  - WMS: Contains re-indexed/reformatted outputs in per-forecast netCDFs suitable for rapid querying and responsive for graph visualizations on the water.noaa.gov/map site.\n  - IMAGES: .png-formatted renderings of NWM output for various domains and variables.\n  - logs: Logs. :)"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 5,
    "content": "### NODD - NOAA Open Data Dissemination Program [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#nodd---noaa-open-data-dissemination-program \"Direct link to NODD - NOAA Open Data Dissemination Program\")\n\n\"The NOAA Open Data Dissemination (NODD) Program provides public access to NOAA's open data on commercial cloud platforms through public-private partnerships. These partnerships remove obstacles to public use of NOAA data, help avoid costs and risks associated with federal data access services, and leverage operational public-private partnerships with the cloud computing and information services industries.\"\n(For more information, visit [NODD](https://www.noaa.gov/information-technology/open-data-dissemination))\n\nThe NODD datasets made available through several public cloud vendors are an incredible resource for accessing NWM data for research and evaluative purposes. The NWS NODD datasets are listed on [this page](https://www.noaa.gov/nodd/datasets) and include the following:"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 6,
    "content": "#### AWS [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#aws \"Direct link to AWS\")\n\nAWS hosts two repositories as part of their sustainability data initiative.\n\nThe first repository contains the operational data (now hosts 4 week rolling collection of all output; it used to only be short range and the registry entry retains the description only for the short\\_range data [here](https://registry.opendata.aws/noaa-nwm-pds/); alternatively, the same resource is described under the sustainability initiative page [here](https://aws.amazon.com/marketplace/pp/prodview-73iwu7dcfuge2).)\n\n- The catalog of AWS-hosted operational NWM data can be browsed [here](https://noaa-nwm-pds.s3.amazonaws.com/index.html).\n\nThe second (and more useful) AWS repository contains several versions of the retrospective dataset each described on the main page under the open data registry [here](https://registry.opendata.aws/nwm-archive/).\n(The same information is also on the AWS sustainability initiative webpage [here](https://aws.amazon.com/marketplace/pp/prodview-g6lcchc7brshwa) )\n\nThe different catalogs of those \\[currently\\] five versions of that resource are linked below:"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 7,
    "content": "- Two versions of NWM v2.1 retrospective\n  - netCDF, [here](https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/index.html)\n  - zarr, [here](https://noaa-nwm-retrospective-2-1-zarr-pds.s3.amazonaws.com/index.html)\n- Two versions of NWM v2.0 retrospective\n  - netCDF, [here](https://noaa-nwm-retro-v2-0-pds.s3.amazonaws.com/index.html)\n  - zarr, [here](https://noaa-nwm-retro-v2-zarr-pds.s3.amazonaws.com/index.html)\n- NWM v1.2 retrospective data\n  - netCDF, [here](https://nwm-archive.s3.amazonaws.com/index.html)\n\nThe AWS retrospective resource is the primary publicly available source for the version 1.0 of the \"AORC\" Analysis of Record for Calibration dataset, which is a 40-year best-available estimate of most common meteorological parameters required for hydrological modeling. Version 1.1 of the dataset will accompany the release of the NWM model version 3.0 retrospective (or 2.2 version??), hopefully in the next few weeks.\n\nJupyter notebook instructions for processing NWM Zarr and NetCDF output formats [here](https://github.com/CIROH-UA/data_access_example/)\n\nAn example of pulling data from the channel output zarr 2.1 archive and writing the results to csv follows:\n\n```codeBlockLines_e6Vv\n'''\n#install these libraries if they aren't already installed\n!pip install zarr\n!pip install xarray\n!pip install s3fs\n!pip install numpy\n'''"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 8,
    "content": "# Import needed libraries\n\nimport xarray as xr\nimport numpy as np\nimport s3fs\nfrom datetime import datetime, timedelta"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 9,
    "content": "# open the zarr store\nurl = \"s3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr\"\nfs = s3fs.S3FileSystem(anon=True)\nstore = xr.open_zarr(s3fs.S3Map(url, s3=fs))"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 10,
    "content": "# Function to get the time series for a specified reach id and and time range"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 11,
    "content": "# then write it out to a csv file.\ndef GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index):\n    flows = streamflow_array.where(feature_id_array==reach_id, drop=True)\n    df_flows = flows[start_time_index:end_time_index].to_dataframe()\n    df_flows.to_csv(f'flows_{reach_id}.csv')"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 12,
    "content": "# get an xarray array of the various values\ntime_array = store['time']\nfeature_id_array = store['feature_id']\nstreamflow_array = store['streamflow']"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 13,
    "content": "# Define the feature IDs to check for\nfeature_ids = [5781221, 5781223, 5781703]"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 14,
    "content": "# Specify the start and end times of interest\nstart_time = datetime(2015, 5, 23, 0, 0, 0)\nend_time = datetime(2015, 6, 24, 0, 0, 0)"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 15,
    "content": "# Get the indices for the needed dates\nzero_start_time = start_date = datetime(1979, 2, 1, 0, 0, 0)\nstart_time_index = int((start_time - zero_start_time).total_seconds() / 3600)\nend_time_index = int((end_time - zero_start_time).total_seconds() / 3600)\n\nfor reach_id in feature_ids:\n    GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index)\n\n'''\nSimple Script for Retrieving Retrospective NWM Data from AWS Store\nDan Ames, 11/17/2023\ndan.ames@byu.edu\n'''\n\n```"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 16,
    "content": "#### Google ‚Äì Operational NWM Data [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#google--operational-nwm-data \"Direct link to Google ‚Äì Operational NWM Data\")\n\nGoogle hosts the most complete operational data archive of inputs and outputs from the National Water Model, with nearly every file since August 2018. The Google open data registry provides additional explanations [here](https://console.cloud.google.com/marketplace/product/noaa-public/national-water-model?project=explore-ai-387703).\n\n- Operational data can be browsed [here](https://console.cloud.google.com/storage/browser/national-water-model).\n- Google also hosts a copy of the NWM v1.2 retrospective [here](https://console.cloud.google.com/storage/browser/national-water-model-reanalysis).\n\nComing soon: Big Query\n\nEfforts are underway to make some of the datasets from the NWM operational and retrospective simulations available on BigQuery for ultra-high-bandwidth access. Stay tuned..."
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 17,
    "content": "#### Azure/Planetary Computer [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#azureplanetary-computer \"Direct link to Azure/Planetary Computer\")\n\nMicrosoft hosts the NWM input and output datasets in Azure Blob Storage, associated with the Microsoft Planetary Computer.\n[Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/dataset/storage/noaa-nwm)\nTom Augspurger of Microsoft has a series of notebooks providing examples of how to use this data from his workshop at the first CIROH developers conference.\n[Tom Augspurger's Notebooks](https://github.com/TomAugspurger/noaa-nwm)"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 18,
    "content": "### CIROH Resources [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#ciroh-resources \"Direct link to CIROH Resources\")\n\nMore detailed information and example usage will be available soon.\n\n- Kerchunk Retro (points to AWS 2.1 NetCDF Retro)\n  - [Kerchunk Retro](https://ciroh-nwm-zarr-retrospective-data-copy.s3.amazonaws.com/index.html) \\- Forcing complete; model output 2011-2020\n- Kerchunk Operational (points to Google assets ‚Äì a simple text change can point to AWS short range, if desired)\n  - [Kerchunk Operational](https://ciroh-nwm-zarr-copy.s3.amazonaws.com/index.html)"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 19,
    "content": "### Other resources [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#other-resources \"Direct link to Other resources\")"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 20,
    "content": "#### ESRI Living Atlas [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#esri-living-atlas \"Direct link to ESRI Living Atlas\")\n\nESRI Living Atlas provides a map-enabled version of the NWM output, which can be accessed [here](https://www.esri.com/arcgis-blog/products/analytics/analytics/esri-visualizes-noaas-national-water-model/)."
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 21,
    "content": "#### Description of WRF-Hydro code: [‚Äã](https://docs.ciroh.org/docs/products/data-management/dataaccess/\\#description-of-wrf-hydro-code \"Direct link to Description of WRF-Hydro code:\")\n\nA detailed description of various aspects of the WRF-Hydro code, which produces the current NWM, can be found [here](https://ral.ucar.edu/sites/default/files/public/projects/wrf_hydro/technical-description-user-guide/wrf-hydro-v5.1.1-technical-description.pdf).\n\n* * *\n\n[**üìÑÔ∏ènwmurl** \\\\\nnwmurl is a Python library developed by CIROH 2023. It provides utility functions specifically designed to subset and generate National Water Model (NWM) data URLs. This library simplifies the process of accessing NWM data for various purposes such as analysis, modeling, and visualization.](https://docs.ciroh.org/docs/products/data-management/dataaccess/NWMURL%20Library)"
  },
  {
    "idurl": 35,
    "idtype": "text",
    "order": 22,
    "content": "- [Input and Output Data of the National Water Model](https://docs.ciroh.org/docs/products/data-management/dataaccess/#input-and-output-data-of-the-national-water-model)\n  - [Official NOMADS Resource](https://docs.ciroh.org/docs/products/data-management/dataaccess/#official-nomads-resource)\n  - [NODD - NOAA Open Data Dissemination Program](https://docs.ciroh.org/docs/products/data-management/dataaccess/#nodd---noaa-open-data-dissemination-program)\n  - [CIROH Resources](https://docs.ciroh.org/docs/products/data-management/dataaccess/#ciroh-resources)\n  - [Other resources](https://docs.ciroh.org/docs/products/data-management/dataaccess/#other-resources)"
  },
  {
    "idurl": 36,
    "idtype": "text",
    "order": 1,
    "content": "# Water Prediction Node\n\n[The Water Prediction Node (WPN) website](https://waternode.ciroh.org/) is a collaborative effort between CIROH, NOAA CoastWatch, and the National Water Center. The purpose of the website is to intake and disseminate remote sensing data relevant to hydrological modeling, prediction, and analysis. The data will come from a variety of sources from within NOAA, CIROH, as well as potentially other agencies and companies. This data will be disseminated to CIROH researchers, Water Center operations, and the general public.\n\nThe other main purpose of the website is to serve as an educational platform and facilitator for the use of satellite imagery so that stakeholders that aren't experts in the processing and acquisition of remote sensing data can still use satellite imagery in their research efforts."
  },
  {
    "idurl": 36,
    "idtype": "text",
    "order": 2,
    "content": "## Data Catalog [‚Äã](https://docs.ciroh.org/docs/products/data-management/waternode/\\#data-catalog \"Direct link to Data Catalog\")\n\nThe WPN has a [STAC](https://stacspec.org/en) data catalog. The catalog can be accessed via the [graphical browser](https://waternode.ciroh.org/catalog/?.language=en) or programatically via R or Python. The WPN has [a tutorial](https://colab.research.google.com/drive/17IME_lDGYwpLR_Wv-NZW5nm519XK3oMT?usp=sharing) demonstrating how to download WPN data via python. More tutorials for working with WPN data will be created at the [tutorials page](https://waternode.ciroh.org/tutorials.html).\n\nThe two main initial focuses of the data catalog will be:"
  },
  {
    "idurl": 36,
    "idtype": "text",
    "order": 3,
    "content": "1. Flood maps created using satellite data. Maps created by [NESDIS STAR](https://www.star.nesdis.noaa.gov/star/index.php) using VIIRS, Sentinel 1/2, landsat, and GOES ABI will be made available.\n2. ET related data. The WPN will catalog NESDIS STAR soil moisture products as well as remotely sensed vegetation indices that have the potential to improve hydrological model validation and evaluation efforts.\n3. Baseline inundated extents and river widths. Baseline inundated extents are already being produced by the National Water Center and have the potential to inform the flood masp in the catalog. Multi-year baselines can also be informative when evaluating drought stricken regions. River widths have the potential to be assimilated into the National Water Model to improve synthetic rating curves as well as model discharge estimates."
  },
  {
    "idurl": 36,
    "idtype": "text",
    "order": 4,
    "content": "## Current WPN projects [‚Äã](https://docs.ciroh.org/docs/products/data-management/waternode/\\#current-wpn-projects \"Direct link to Current WPN projects\")\n\nThe Water Prediction Node [first project](https://waternode.ciroh.org/fim.html) is making it easier to perform qualitative comparisons between flood maps created from satellite imagery and flood maps created using the inundation models used by the National Water Center (NWC). The WPN will enable these comparisons by focusing on:\n\n1. Exposing WPN data catalog assets as a web mapping service or web mapping tile service. This will allow stakeholders to easily import WPN satellite derived flood maps into their GIS viewer of choice. This satellite derived flood map layer can then be quickly compared to modeled inundation.\n2. Creating easy to access collections of satellite imagery of notable floods so that retrospective evaluation is easier.\n3. Creating a [web processing service](https://www.ogc.org/standard/wps/) that allows for the creation of agreement maps in the style of [gval](https://github.com/noaa-owp/gval). These agreement maps will highlight areas of agreement and disagreement between the modelled and remotely sensed flood maps and will allow the computation of agreement metrics."
  },
  {
    "idurl": 36,
    "idtype": "text",
    "order": 5,
    "content": "## Website repository [‚Äã](https://docs.ciroh.org/docs/products/data-management/waternode/\\#website-repository \"Direct link to Website repository\")\n\nThe source for the website implementation can be found [here](https://github.com/dylanlee/wnweb/tree/main)\n\n- [Data Catalog](https://docs.ciroh.org/docs/products/data-management/waternode/#data-catalog)\n- [Current WPN projects](https://docs.ciroh.org/docs/products/data-management/waternode/#current-wpn-projects)\n- [Website repository](https://docs.ciroh.org/docs/products/data-management/waternode/#website-repository)"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 1,
    "content": "# HydroServer\n\nFuture iterations of the National Water Model will need to assimilate local-scale data to improve forecasts. The United States Geological Survey (USGS) operates the largest network of operational streamflow gages in the U.S., but generally does not incorporate data from gages operated by other agencies/organizations. A cooperative network of the many streamflow gages outside of the USGS network would enhance data available to national-scale modeling. No cyberinfrastructure currently exists to support such a network.\n\nHydroServer is an open-source software stack with functionality for collecting, managing, and sharing operational hydrologic data - e.g., time series of hydrologic observations from fixed monitoring sites like streamflow gages. HydroServer development for CIROH is focused on creating an enhanced, national-scale stream gage network to make more data available to operational modeling.\n\n![CIROH Portal](https://docs.ciroh.org/assets/images/hydroserver-b444068e12a63e2c95946a182893e0eb.png)"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 2,
    "content": "## Software and Technologies [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroserver/\\#software-and-technologies \"Direct link to Software and Technologies\")\n\nThe HydroServer software stack includes:\n\n- A user-oriented web application for creation of monitorings sites, site metadata, information about observed variables, etc.\n- A Python package and desktop app for loading time series data from monitoring sites into HydroServer\n- Application Programming Interfaces (APIs) for data ingest into HydroServer, data querying and retrieval, and data and metadata management\n- A highly performant time series database for storing and managing time series data\n\nAdditional planned tools include:\n\n- A Python client package for retrieving time series data from HydroServer\n- Automated archival of time series data to the HydroShare repository\n- Integration of data quality control functionality\n- Web app(s) for data visualization\n\nThe HydroServer software stack is being build using the following technologies:"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 3,
    "content": "- [Vue.js](https://vuejs.org/) \\- A JavaScript framework for building web user interfaces\n- [Python/Django](https://www.djangoproject.com/) \\- A Python web framework for backend web development\n- [Open Geospatial Consortium SensorThings API](https://www.ogc.org/standard/sensorthings/) \\- An API specification and data model for managing and retrieving observations and metadata from sensor systems.\n- [Timescale Cloud](https://www.timescale.com/) \\- A cloud native implementation of PostgreSQL and its Timescale extension for storing and managing time series data\n- [Amazon Web Services (AWS)](https://aws.amazon.com/) \\- The HydroServer web application and APIs are deployed using AWS."
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 4,
    "content": "## Access [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroserver/\\#access \"Direct link to Access\")\n\nAs of August 28, 2023, we are currently working on setting up domains, associated security certificates, and additional settings, but we anticipate that the CIROH HydroServer instances will be:\n\n- [https://hydroserver.ciroh.org](https://hydroserver.ciroh.org/) \\- Production instance of HydroServer for CIROH\n- [https://hydroserver-dev.ciroh.org](https://hydroserver-dev.ciroh.org/) \\- Development instance for internal development and testing\n- [https://hydroserver-beta.ciroh.org](https://hydroserver-beta.ciroh.org/) \\- Beta instance for testing and demonstration of latest functionality"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 5,
    "content": "## Open-Source Code Development [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroserver/\\#open-source-code-development \"Direct link to Open-Source Code Development\")\n\nThe HydroServer software stack is developed as open-source software using the BSD3 open source license. All code development is hosted in our GitHub repositories hosted under the HydroServer GitHub Organization [https://github.com/hydroserver2/](https://github.com/hydroserver2/)"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 6,
    "content": "## Bugs and Issues [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroserver/\\#bugs-and-issues \"Direct link to Bugs and Issues\")\n\nBugs, issues, and feature requests related to HydroServer applications can be reported via their respective GitHub repositories at:\n\n- [https://github.com/orgs/hydroserver2/repositories](https://github.com/orgs/hydroserver2/repositories)"
  },
  {
    "idurl": 37,
    "idtype": "text",
    "order": 7,
    "content": "## Development Team [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroserver/\\#development-team \"Direct link to Development Team\")\n\nThe HydroServer software stack is under development at the Utah Water Research Laboratory at Utah State University. The main contributors include:\n\n- Jeff Horsburgh - Associate Professor, Utah Water Research Laboratory and Civil and Environmental Engineering, Utah State University\n- Ken Lippold - Software Engineer, Utah Water Research Laboratory, Utah State University\n- Daniel Slaugh - Software Engineer, Utah Water Research Laboratory, Utah State University\n- Maurier Ramirez - Software Engineer, Utah Water Research Laboratory, Utah State University\n\n- [Software and Technologies](https://docs.ciroh.org/docs/products/data-management/hydroserver/#software-and-technologies)\n- [Access](https://docs.ciroh.org/docs/products/data-management/hydroserver/#access)\n- [Open-Source Code Development](https://docs.ciroh.org/docs/products/data-management/hydroserver/#open-source-code-development)\n- [Bugs and Issues](https://docs.ciroh.org/docs/products/data-management/hydroserver/#bugs-and-issues)\n- [Development Team](https://docs.ciroh.org/docs/products/data-management/hydroserver/#development-team)"
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 1,
    "content": "# NETWA"
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 2,
    "content": "## About [‚Äã](https://docs.ciroh.org/docs/products/data-management/netwa/\\#about \"Direct link to About\")\n\nThe [forecast-workflow](https://github.com/CIROH-UVM/forecast-workflow) repository was initally created to implement the AEM3D model to generate 7-day forecasts of Harmful Algal Blooms (HABs) in Lake Champlain. However, we've created some handy data grabbers along the way that we've modified in order to make them more useful for other CIROH researchers. The scripts for these data grabbers can be found in the `data/` folder within the repository. This page will include documentation on how to use some of these data grabber tools we've created."
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 3,
    "content": "## Cloning the repository [‚Äã](https://docs.ciroh.org/docs/products/data-management/netwa/\\#cloning-the-repository \"Direct link to Cloning the repository\")\n\nThe first thing you need to do in order to be able to use the tools in forecast-workflow is clone the repository into your user space using git.\n\n1. Once logged on the testbed, open a new terminal and navigate to the directroy in which you'd like to store the repo\n2. Then, run `git clone https://github.com/CIROH-UVM/forecast-workflow.git`\n3. That's it! You now have the repo on your own personal user space.\n1. Be sure to checkout the repo on GitHub every now and then to make sure you have the most recent version of the repo.\n2. If you need to pull any updates to your local repo, simply run `git pull` from your `forecast-workflow/` directory"
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 4,
    "content": "## Using Jupyter Notebooks [‚Äã](https://docs.ciroh.org/docs/products/data-management/netwa/\\#using-jupyter-notebooks \"Direct link to Using Jupyter Notebooks\")\n\nWe recommmend using Jupyter notebooks for interactive computing, but you can also import forecast-workflow tools in a plain old python script as well (see code blocks below).\n\n1. Log onto the CIROH VM using Remote Desktop Viewer or similar software\n2. Open a new terminal and `cd` to the top level project directory for your python scripts (it could be your home directory)\n3. Activate the `standard` mamba environment with `mamba activate standard`\n4. Run `jupyter lab` to launch Jupyter\n5. If starting a new notebook, click on a kernel underneath the \"Notebooks\" banner\n6. Or if you have a notebook you're working on, simply open that one and pick up where you left off!"
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 5,
    "content": "#### Add the repo to your python path variable [‚Äã](https://docs.ciroh.org/docs/products/data-management/netwa/\\#add-the-repo-to-your-python-path-variable \"Direct link to Add the repo to your python path variable\")\n\nEvertime you start or restart a jupyter kernel, you will need to add the forecast-workflow directory to your `sys.path` variable so that python knows where to look for forecast-workflow code. You can do that with the following code block:\n\n```codeBlockLines_e6Vv\nimport sys\nsys.path.append(\"/absolute/path/to/your/forecast-workflow\")\n\n```\n\nThis cell should be at the top of your notebook, but you only need to run it once; comment out these lines after runing so that you do not add the same path to you `sys.path` over and over again.\n\nNow, you can import the data grabbers into your notebook like any other module!\n\n```codeBlockLines_e6Vv\nimport data.nwm_fc as nwm\nimport data.gfs_fc as gfs\n\n```\n\nEtc."
  },
  {
    "idurl": 38,
    "idtype": "text",
    "order": 6,
    "content": "## Data Grabber Demo [‚Äã](https://docs.ciroh.org/docs/products/data-management/netwa/\\#data-grabber-demo \"Direct link to Data Grabber Demo\")\n\nThere is a neat demo notebook that includes more in-depth instructions on how to use our data grabber tools. You can find that notebook at `forecast-workflow/examples/get_data_demo.ipynb`\n\n- [About](https://docs.ciroh.org/docs/products/data-management/netwa/#about)\n- [Cloning the repository](https://docs.ciroh.org/docs/products/data-management/netwa/#cloning-the-repository)\n- [Using Jupyter Notebooks](https://docs.ciroh.org/docs/products/data-management/netwa/#using-jupyter-notebooks)\n- [Data Grabber Demo](https://docs.ciroh.org/docs/products/data-management/netwa/#data-grabber-demo)"
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 1,
    "content": "# HydroShare\n\n[HydroShare](https://www.hydroshare.org/) is a collaboration environment and repository for data, models, and other research products. Developed and maintained by the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI), HydroShare is a file agnostic, general purpose repository for data, models, computational notebooks, and other content types that accepts upload of any file type. HydroShare allows users to publish data and other content and receive a citable Digital Object Identifier (DOI). Users can keep content files private, share individually with collaborators, make content public, or publish content permanently so that it can be easily cited. HydroShare users can also share their content with groups and communities, such as the [CIROH Community](https://www.hydroshare.org/community/4/), allowing for increased inter-group collaboration. Reach out to [help@cuahsi.org](mailto:help@cuahsi.org) for more information or see [https://help.hydroshare.org/](https://help.hydroshare.org/).\n\n![CIROH Portal](https://docs.ciroh.org/assets/images/hydroshare_screenshot-af63e7baac219da20eba3df9dd9bff56.png)"
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 2,
    "content": "## Software and Technologies [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#software-and-technologies \"Direct link to Software and Technologies\")\n\nHydroShare is an operational repository at [https://www.hydroshare.org](https://www.hydroshare.org/). HydroShare includes the following functionality:\n\n- A user-oriented web application for creation of \"resources\" within which you can share data, models, computational notebooks, and other content files.\n- A flexible, file-based data model for storing content created within HydroShare resources.\n- A REST application programming interface (API) for programmatic access to HydroShare resources. You can automate and code most everything through HydroShare's API that you can do through the web user interface.\n- A Python client package called \"hsclient\" that enables easier interaction with HydroShare's REST API\n- Ability to link to and launch computational notebooks, code, and content files into linked JupyterHub environments, including the CIROH JupyterHub, CUASHI JupyterHub, and CyberGIS JupyterHub."
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 3,
    "content": "## Access [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#access \"Direct link to Access\")\n\nAnyone can access HydroShare by navigating to [https://www.hydroshare.org](https://www.hydroshare.org/) and creating a user account. All users are automatically allocated a 20GB quota for content within HydroShare, but if you need more space you can make a request to CUAHSI."
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 4,
    "content": "## Open-Source Code Development [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#open-source-code-development \"Direct link to Open-Source Code Development\")\n\nHydroShare is an open source software development project, with repositories and source code available at [https://github.com/hydroshare](https://github.com/hydroshare). HydroShare is developed as open-source software using the BSD 3-clause open source license."
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 5,
    "content": "## Bugs and Issues [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#bugs-and-issues \"Direct link to Bugs and Issues\")\n\nBugs, issues, and feature requests related to HydroShare can be reported via the main HydroShare GitHub repository at:\n\n- [https://github.com/hydroshare/hydroshare/issues](https://github.com/hydroshare/hydroshare/issues)"
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 6,
    "content": "## Development Team [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#development-team \"Direct link to Development Team\")\n\nHydroShare is the work of many individuals and organizations who have contributed to its design and development over many years. For details, see [https://github.com/orgs/hydroshare/people](https://github.com/orgs/hydroshare/people)."
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 7,
    "content": "## How to cite HydroShare [‚Äã](https://docs.ciroh.org/docs/products/data-management/hydroshare/\\#how-to-cite-hydroshare \"Direct link to How to cite HydroShare\")\n\nThe following citations should be used when citing HydroShare:\n\nTarboton, D. G., Ames, D. P., Horsburgh, J. S., Goodall, J. L., Couch, A., Hooper, R., Bales, J., Wang, S., Castronova, A., Seul, M., Idaszak, R., Li, Z., Dash, P., Black, S., Ramirez, M., Yi, H., Calloway, C., Cogswell, C. (2024). HydroShare Retrospective: A Review of Science and Technology Advances of a Comprehensive Data and Model Publication Environment for the Water Science Domain, Environmental Modelling & Software, 172, 105902, [https://doi.org/10.1016/j.envsoft.2023.105902](https://doi.org/10.1016/j.envsoft.2023.105902).\n\nHorsburgh, J. S., M. M. Morsy, A. M. Castronova, J. L. Goodall, T. Gan, H. Yi, M. J. Stealey, and D. G. Tarboton (2016). HydroShare: Sharing diverse environmental data types and models as social objects with application to the hydrology domain, JAWRA Journal of the American Water Resources Association, 52(4), 873-889, [https://doi.org/10.1111/1752-1688.12363](https://doi.org/10.1111/1752-1688.12363)."
  },
  {
    "idurl": 39,
    "idtype": "text",
    "order": 8,
    "content": "Tarboton, D. G., R. Idaszak, J. S. Horsburgh, J. Heard, D. Ames, J. L. Goodall, L. Band, V. Merwade, A. Couch, J. Arrigo, R. Hooper, D. Valentine and D. Maidment (2014). HydroShare: Advancing Collaboration through Hydrologic Data and Model Sharing, in D. P. Ames, N. W. T. Quinn and A. E. Rizzoli (eds.), Proceedings of the 7th International Congress on Environmental Modelling and Software, San Diego, California, USA, International Environmental Modelling and Software Society (iEMSs), ISBN: 978-88-9035-744-2, [https://scholarsarchive.byu.edu/iemssconference/2014/Stream-A/7/](https://scholarsarchive.byu.edu/iemssconference/2014/Stream-A/7/).\n\n- [Software and Technologies](https://docs.ciroh.org/docs/products/data-management/hydroshare/#software-and-technologies)\n- [Access](https://docs.ciroh.org/docs/products/data-management/hydroshare/#access)\n- [Open-Source Code Development](https://docs.ciroh.org/docs/products/data-management/hydroshare/#open-source-code-development)\n- [Bugs and Issues](https://docs.ciroh.org/docs/products/data-management/hydroshare/#bugs-and-issues)\n- [Development Team](https://docs.ciroh.org/docs/products/data-management/hydroshare/#development-team)\n- [How to cite HydroShare](https://docs.ciroh.org/docs/products/data-management/hydroshare/#how-to-cite-hydroshare)"
  },
  {
    "idurl": 40,
    "idtype": "text",
    "order": 1,
    "content": "# NWM BigQuery API\n\ninfo\n\nMore details about \"Design and implementation of a BigQuery dataset and application programmer interface (API) for the U.S. National Water Model\" paper can be found [here.](https://www.sciencedirect.com/science/article/pii/S1364815224001841)\n\n**National Water Model API Documentation available here:** [https://nwm-api.ciroh.org/docs](https://nwm-api.ciroh.org/docs)"
  },
  {
    "idurl": 40,
    "idtype": "text",
    "order": 2,
    "content": "# Steps to use CIROH NWM API\n\n1. Submit the form below to request access to [NWM BigQuery API](https://nwm-api.ciroh.org/).\n\n[NWM BigQuery API Access Request Form](https://forms.office.com/r/FeNpjZstkr)\n\n2. For an example usage, please refer to [this script](https://github.com/CIROH-UA/api-nwm-gcp/blob/main/examples/notebooks/nwm_usgs_streamflow_plot.ipynb). Replace your API key and API_URL = ' [https://nwm-api.ciroh.org](https://nwm-api.ciroh.org/)'.\n\n3. Estimate your query before running the actual query. If you're not sure this step, please contact [ciroh it support](mailto:ciroh-it-support@ua.edu).\n\nFor implementation detail, please refer to [this repository](https://github.com/CIROH-UA/api-nwm-gcp).\n\nNews Blog: [https://docs.ciroh.org/blog/September%20Monthly%20Blog%20Update](https://docs.ciroh.org/blog/September%20Monthly%20Blog%20Update)"
  },
  {
    "idurl": 41,
    "idtype": "text",
    "order": 1,
    "content": "# Intro to Snow Observations Modeling Analysis\n\n**Tags:**\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [Snow Sensing](/docs/tags/snow-sensing)"
  },
  {
    "idurl": 42,
    "idtype": "text",
    "order": 1,
    "content": "# Optimized Snow Sensor Location\n\n**Tags:**\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [Snow Sensing](/docs/tags/snow-sensing)"
  },
  {
    "idurl": 43,
    "idtype": "text",
    "order": 1,
    "content": "# Snow Sensing\n\n**Tags:**\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [Snow Sensing](/docs/tags/snow-sensing)"
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 1,
    "content": "# SWEMLv2.0\n\n* * *\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/README.md](https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/README.md).\n\n[![NSM_Cover](https://github.com/CIROH-Snow/SWEMLv2.0/raw/main/Images/ML_SWE.jpg)](https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/Images/ML_SWE.jpg)"
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 2,
    "content": "# Snow Water Equivalent Machine Learning (SWEMLv2.0): The evolution of the Snow Water Equivalent Machine Learning modeling workflow - flexible spatial resolution, flexible model regions, and flexible grid/mesh"
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 3,
    "content": "## Deep Learning SWE prediction model\n\nThe current iteration of the SWEMLv2.0 produces SWE inferences for select locations throughout the Western U.S. plus the Upper Colorado River Basin.\nThere is a heavy focus on SWE inferences in the Sierra Nevada mountains, Colorado Rockies, and Wind River Range in Wyoming.\nThe ML pipeline retrieves all SWE observations from SNOTEL and CDEC snow monitoring locations for the date of interest and processes the SWE observations into a model-friendly data frame alongside lidar-derived terrain features, seasonality metrics, previous SWE estimates, and location.\nSWEMLv2.0 predicts SWE using a multilayered perceptron network model for each supports an interactive visualization of the SWE estimates across the western U.S.\n\n[![SWEinteractive](https://github.com/CIROH-Snow/SWEMLv2.0/raw/main/Images/SWE_2019.gif)](https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/Images/SWE_2019.gif)\nFigure 1. Example hindcast simulation in the Colorado domain demonstrates snow accumulation as the season progresses, especially at higher elevations, and only predicts SWE where NASA VIIRS fSCA imagery indicates snow covering greater than 20% of the terrain."
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 4,
    "content": "## Data Sources (training, inference, where/how used)\n\nGround measurements for training from the SNOTEL and CDEC features: ground_measure_features_templatev2.csv\n\nLatitude, Longitude, and Elevation for all measurement locations: ground_measures_metadatav2.csv\n\nGeoJSON data for the grid cell delineation: grid_cells.geoJSON file.\n\nSWE training measurements separate from ASO observations: train_labels.csv\n\nThe prediction grid cell IDs were identified by\nLatitude and longitude form the prediction cell ids and we descritize the western US into 23 regions based on Sturm's snow classification scheme and location\nThe ML workflow utilizes Previous SWE features for each prediction location and Delta SWE for each ground measurement site (e.g., SNOTEL/CDEC).\nFor each prediction location at the respective spatial resolution, the data processing scripts determine slope angle, aspect, and northness from a 30m Copernicus DEM."
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 5,
    "content": "## Dependencies (versions, environments)\n\nPython: Version 3.8 or 3.9.\nPlease refer to the [![GettingStarted](https://github.com/CIROH-Snow/SWEMLv2.0/raw/main/Getting_Started.md)](https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/Getting_Started.md) file to set up the appropriate virutal environment to run all functions."
  },
  {
    "idurl": 44,
    "idtype": "table",
    "order": 6,
    "content": "| os | ulmo | pandas |\n| :-: | :-: | :-: |\n| io | shapely | datetime |\n| re | rasterio | matplot.pyplot |\n| copy | lightgbm | numpy |\n| time | tensorflow | pystac_client |\n| tables | platfrom | planetray_computer |\n| xarray | tqdm | random |\n| rioxarray | geopandas | requests |\n| pyproj | richdem | cartopy |\n| h5py | elevation | cmocean |\n| mpl_toolkits | hdfdict | warning |\n| math | pickle | contextily |\n| folium | branca | earthpy |\n| netCDF4 | osgeo | requests |\n| warnings | geojson | fiona |\n| fiona.crs | webbrowser |  |"
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 7,
    "content": "### Required packages"
  },
  {
    "idurl": 44,
    "idtype": "text",
    "order": 8,
    "content": "## Project support through [CIROH](https://ciroh.ua.edu/)\n\n[![Alpine](https://github.com/CIROH-Snow/SWEMLv2.0/raw/main/Images/CIROHsupport.png)](https://github.com/CIROH-Snow/SWEMLv2.0/blob/main/Images/CIROHsupport.png)"
  },
  {
    "idurl": 45,
    "idtype": "text",
    "order": 1,
    "content": "# SWEML"
  },
  {
    "idurl": 45,
    "idtype": "text",
    "order": 2,
    "content": "## Advancing Snow Modeling\n\nThe Snow Water Equivalent Machine Learning (SWEML) is a tool that:\n\n- Incorporates ground-based snow measuring sites\n- Uses remotely-sensed snow cover information\n- Employs an Artificial Neural Network\n- Provides point estimations of Snow Water Equivalent"
  },
  {
    "idurl": 45,
    "idtype": "text",
    "order": 3,
    "content": "## Key Features\n- Trained on historical data from NASA's ASO missions\n- Divided into regions\n- Uses LightGradientBoost Model for feature selection\n- Includes functions for:\n  - Downloading data\n  - Pre-processing\n  - Running inference\n  - Producing visualizations"
  },
  {
    "idurl": 45,
    "idtype": "text",
    "order": 4,
    "content": "## Tags\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [Snow Sensing](/docs/tags/snow-sensing)\n- [AI and Machine Learning](/docs/tags/ai-and-machine-learning)"
  },
  {
    "idurl": 46,
    "idtype": "text",
    "order": 1,
    "content": "# NWM-ML\n\n**Tags:**\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [National Water Model](/docs/tags/national-water-model)\n- [AI and Machine Learning](/docs/tags/ai-and-machine-learning)"
  },
  {
    "idurl": 47,
    "idtype": "text",
    "order": 1,
    "content": "# Tethys-CSES\n\nCommunity Streamflow Evaluation System (CSES) Web Application\n\n**Tags:**\n- [Products](/docs/tags/products)\n- [CIROH](/docs/tags/ciroh)\n- [NOAA](/docs/tags/noaa)\n- [Alabama Water Institute](/docs/tags/alabama-water-institute)\n- [Tethys Platform](/docs/tags/tethys-platform)\n- [CSES](/docs/tags/cses)"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 1,
    "content": "# FIM as a Service\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/sdmlua/FIMserv/blob/main/README.md](https://github.com/sdmlua/FIMserv/blob/main/README.md)."
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 2,
    "content": "# Flood Inundation Mapping Tool using the OWP HAND-FIM operational framework\n\n* * *"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 3,
    "content": "[![Version](https://camo.githubusercontent.com/a28c0a6266df7e69f0f0a5f66d805a319e238cc1ef4d1b5ee60c46b49a7afbdd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f73646d6c75612f46494d73657276)](https://github.com/sdmlua/FIMserv/releases)[![Issues](https://camo.githubusercontent.com/3a15bac30ecb1e628dfbe92a7a94a2189f1c5903f70f35d69015f0af39ce92d1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f73646d6c75612f46494d73657276)](https://github.com/sdmlua/FIMserv/issues)[![License: GPL v3](https://camo.githubusercontent.com/8a398fc9fbf479a323d2d91b9fcb6fb9c6b4d08e96dbb544488ccbed312115fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667)](https://opensource.org/licenses/GPL-3.0)[![PyPI version](https://camo.githubusercontent.com/8b0eb4a232f183926054ab0c52152c326c9d71fc2f7971f0e099ef843d0afef0/68747470733a2f2f62616467652e667572792e696f2f70792f66696d73657276652e7376673f69636f6e3d7369253341707974686f6e)](https://badge.fury.io/py/fimserve)[![PyPI Downloads](https://camo.githubusercontent.com/f6d38c557f7f19082ce6ed31890bb2d233717fe6a2eda30f72682e8ad0737904/68747470733a2f2f7374617469632e706570792e746563682f62616467652f66696d7365727665)](https://pepy.tech/projects/fimserve)[![Publish Status](https://github.com/sdmlua/FIMserv/actions/workflows/python-publish.yml/badge.svg)](https://github.com/sdmlua/FIMserv/actions/workflows/python-publish.yml)"
  },
  {
    "idurl": 48,
    "idtype": "table",
    "order": 4,
    "content": "|  |  |\n| --- | --- |\n| [![SDML Logo](https://camo.githubusercontent.com/3144a76016c24b3f32281f151c46857601e5662e7e593013f3cb8024ff33af46/68747470733a2f2f73646d6c2e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032332f30312f53444d4c5f6c6f676f5f53715f677265792e706e67)](https://sdml.ua.edu/) | This package presents a streamlined, user-friendly and cloud-enabled pipeline to generate Operational flood inundation map using the NOAA Office of Water Prediction (OWP) Height Above Nearest Drainage (HAND) Flood Inundation Mapping (FIM) framework using the National Water Model retrospective and forecasted streamflow. It is developed under the Surface Dynamics Modeling Lab (SDML) as part of a project funded by the Cooperative Institute for Research to Operations in Hydrology (CIROH). |"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 5,
    "content": "### **OWP HAND-FIM 'as a service' (FIMserv)**\n\n* * *"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 6,
    "content": "### **Background**\n\n* * *\n\nOWP HAND-FIM is a national-scale operational flood forecasting framework ( [https://github.com/NOAA-OWP/inundation-mapping](https://github.com/NOAA-OWP/inundation-mapping)). It is a terrain-based fluvial flooding model that uses model-predicted streamflow and reach-averaged Synthetic Rating Curves (SRCs) to generate inundation extent and depth rasters at HUC-8 scale (Hydrologic Unit Code-8). The model can produce FIMs for all order streams within the watershed at a very low computational cost. This notebook streamline the FIM generation process or the OWP HAND-FIM framework on the cloud. It allow users to run over mutiple HUC-8s simultaneously. This model can run using any temporal resolution available from the input streamflow data (hourly, daily, monthly etc). ### **Currently we are hosting the FIM-4.4, 4.5 and 4.8 version for Inundation Mapping.**"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 7,
    "content": "### **Package structures**\n\n* * *\n\nThis FIMserv framework is published as python package and published on [PyPI](https://pypi.org/project/fimserve/0.1.62/) [![PyPI version](https://camo.githubusercontent.com/8b0eb4a232f183926054ab0c52152c326c9d71fc2f7971f0e099ef843d0afef0/68747470733a2f2f62616467652e667572792e696f2f70792f66696d73657276652e7376673f69636f6e3d7369253341707974686f6e)](https://badge.fury.io/py/fimserve). It contains multiple modules to perform different functionalities which are structured into the fimserve folder while development of python packaging."
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 8,
    "content": "```\nFIMserv/\n‚îú‚îÄ‚îÄ docs/                   # Documentation (contains 'FIMserv' Tool usage sample codes)\n‚îÇ     ‚îú‚îÄ‚îÄ source/           #Contains  sphinx documentation (under development)\n‚îÇ     ‚îú‚îÄ‚îÄcode_usage.ipynb  #Contains the detailed documentation\n‚îÇ     ‚îî‚îÄ‚îÄ FIMin3Steps.ipynb  #Focusing only on FIM generation within 3 steps\n‚îú‚îÄ‚îÄ GeoGLOWS/               # Streamflow download using GeoGLOWS hydrofabrics\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ fimserve/\n‚îÇ       ‚îú‚îÄ‚îÄ streamflowdata/ # Handles streamflow data\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ nwmretrospectivedata.py   # Processes NWM retrospective data\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ geoglows.py   # Module to retrieve geoglows streamflow data\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ usgsdata.py   # Retrieve USGS gauge station data\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ forecasteddata.py        # Processes all range forecasted streamflow data\n‚îÇ       ‚îú‚îÄ‚îÄ plots/          # Vizualization functionalities\n‚îÇ       ‚îú‚îÄ‚îÄ FIMsubset/      # Subsetting functionalities for FIM\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ xycoord.py  # Subset using Lat, Lon\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ shpsubset.py # Subset using boundary\n‚îÇ       ‚îú‚îÄ‚îÄ statistics/     # Statistical analysis\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ calculatestatistics.py  # Statistical analysis between NWM and USGS gauge data\n‚îÇ       ‚îú‚îÄ‚îÄ datadownload.py # Includes HUC8 data retrival and folder management module\n‚îÇ       ‚îú‚îÄ‚îÄ runFIM.py       # OWPHAND model execution\n‚îÇ       ‚îú‚îÄ‚îÄ vizualization.py # Interactive visualization of user-defined inundation files (in Jupyter Notebook)\n‚îî‚îÄ‚îÄ tests/                  # Includes test cases for different functionality"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 9,
    "content": "```\n\n**The structure of the framework consisting its applications and connection between different functionalities.** The right figure, **b)**, is the directory structure used in this package (for e.g. after using this code by following [docs/code\\_usage.ipynb](https://github.com/sdmlua/FIMserv/blob/main/docs/code_usage.ipynb)) to download and process one or multiple hucs.\n\n[![Flowchart](https://github.com/sdmlua/FIMserv/raw/main/images/flowchart.jpg)](https://github.com/sdmlua/FIMserv/blob/main/images/flowchart.jpg)\n\n_Fig. (a)A complete workflow demonstrating the framework architecture, and (b) directory structure as it appears on the user's system after using the FIMserv for FIM generation._"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 10,
    "content": "### **Tool Usage**\n\n* * *\n\nAlthough not mandatory,\n**we strongly recommend users create a virtual environment and install this package on that virtual environment to avoid the conflict between system dependencies and package dependencies.**\n\n**‚ÄºÔ∏è If your system doesnot have git, install it first. Download link of git for windows or MacOS: [https://git-scm.com/downloads](https://git-scm.com/downloads)**\n\n**For conda users**\n\n```\n#creating a virtual environment using conda\nconda create --name fimserve python==3.10\n\n#Activate environment\nconda activate fimserve\n```\n\n**If you don't have conda**\n\n```\n#create a virtual env using 'venv'\npython -m venv fimserve python==3.10\n\n#activate environment\n#For MAC Users\nsource fimserve/bin/activate\n\n#For WINDOW Users\nfimserve\\Scripts\\activate\n```\n\n**Once Virtual env is ready, install or add fimserve into your workflows**\n\n```\n#Using pip\npip install uv\nuv pip install fimserve\n'OR'\npip install fimserve\n\n#OR add using poetry to your framework development for quick FIM generation\npoetry add fimserve\n```\n\n**FIM generation only in 3 steps using fimserve framework**"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 11,
    "content": "This framework have multiple other funtionalities, but the most important is to generate the FIM. The following step is the standard way to generate FIM using OWP Operation FIM framework with FIMserv. This is just a quick 3 steps for one huc and one event, user can use this framework as many case and event as per thier requirement. The google colab version of **FIMin3Steps is here**\\- [![Google Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/drive/1LLagzuLKTDu3WzwjqRoIgnn2iiAveU2f?usp=sharing)\n\n```\n#Import framework once it is installed in your workflows\nimport fimserve as fm"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 12,
    "content": "# Initialize necessary variables\nhuc = \"03020202\"        #HUC8 ID\nstart_date = \"2016-10-08\"   #Start date of the streamflow data\nend_date = \"2016-10-10\"     #End date of the streamflow data\n\nvalue_time = [\"2016-10-09 03:00:00\"]   #Time of the streamflow data user want to generate FIM within start_date and end_date\n```\n\n**Step 1. Download HUC8 data**\n\nOWP HAND FIM model runs at HUC-8 watershed scale. User need to identify the HUC8 ID for their specific region of interest. In this example we are using the Neuse River Flooding in North Carolina from Hurricane Mathhew,2016. The HUC8 id is 03020202. The locations and informations about the HUC8 IDs are available here in: **[ArcGIS Instant App](https://ualabama.maps.arcgis.com/apps/instant/basic/index.html?appid=88789b151b50430d8e840d573225b36b)**.\n\n```\nfm.DownloadHUC8(huc)    #Download the HUC8 data\n```\n\n**Step 2. Get the NWM Streamflow data**\n\nUsers can retrieve NWM forecasted and retrospective data for a specified date range (start date to end date). Additionally, they can store streamflow data for a specific date, as defined by value\\_times, during the initialization process to generate FIM.\n\n```\nfm.getNWMretrospectivedata(start_date, end_date, huc, value_time)\n```\n\n**Step 3. Generate the Flood Inundation Mapping**\n\nThis functionality automatically uses the recently downloaded and stored streamflow to generate FIM. This automation is based on the HUCID."
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 13,
    "content": "```\nfm.runOWPHANDFIM(huc)       #Run the OWP-HAND FIM with the NWM retrospective streamflow data\n```\n\nThen there are a lot of different modules/funtionalities related to Syntetic Rating Curve (SRCs) analysis, NWM and USGS streamflow Evaluation, Subsetting of FIM, Domain filtering etc. For reference to run, [Here (docs/code\\_usage.ipynb)](https://github.com/sdmlua/FIMserv/blob/main/docs/code_usage.ipynb) is the sample usage of this FIMserv tool and which covers all modules in detailed and to generate FIM only, follow this shorter, FIM in 3 steps version [Here (docs/FIMin3steps.ipynb)](https://github.com/sdmlua/FIMserv/blob/main/docs/FIMin3steps.ipynb).\n\nUse Google Colab. Here is **Detailed code Usage of FIMserv in Google Colab**: [![Google Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/drive/1mAjgkkCvR3Tcbdz48SwlkHmEo1GZvsh7?usp=sharing)\n\n**Different HUC8 IDs, USGS gauge stations and flowline information that might be required to further understand/running this framework can be found in this [ArcGIS Instant App](https://ualabama.maps.arcgis.com/apps/instant/basic/index.html?appid=88789b151b50430d8e840d573225b36b).**"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 14,
    "content": "### **Citing This Tool**\n\nAnupal Baruah, Supath Dhital, Sagy Cohen, et al. **FIMserv v.1.0: A Tool for Streamlining Flood Inundation Mapping (FIM) Using the United States Operational Hydrological Forecasting Framework**. _Environmental Modelling & Software_, **Volume 192**, 2025, 106581. [https://doi.org/10.1016/j.envsoft.2025.106581](https://doi.org/10.1016/j.envsoft.2025.106581)"
  },
  {
    "idurl": 48,
    "idtype": "table",
    "order": 15,
    "content": "|  |  |\n| --- | --- |\n| [![alt text](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |\n|  | We would like to acknowledge the TEEHR script developed by RTI International ( [https://github.com/RTIInternational/teehr](https://github.com/RTIInternational/teehr)). We use this script to get NWM discharge quickly. |"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 16,
    "content": "### **Acknowledgements**\n\n* * *"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 17,
    "content": "### **For More Information**\n\n* * *"
  },
  {
    "idurl": 48,
    "idtype": "text",
    "order": 18,
    "content": "#### **Contact**\n\n[Dr. Sagy Cohen](https://geography.ua.edu/people/sagy-cohen/)\n( [sagy.cohen@ua.edu](https://github.com/sdmlua/FIMserv/blob/main/mailto:sagy.cohen@ua.edu)),\nDr Anupal Baruah,( [abaruah@ua.edu](https://github.com/sdmlua/FIMserv/blob/main/mailto:abaruah@ua.edu)), Supath Dhital ( [sdhital@crimson.ua.edu](https://github.com/sdmlua/FIMserv/blob/main/mailto:sdhital@crimson.ua.edu))"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 1,
    "content": "# FIM Evaluation Framework\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/sdmlua/fimpef/blob/main/README.md](https://github.com/sdmlua/fimpef/blob/main/README.md)."
  },
  {
    "idurl": 49,
    "idtype": "table",
    "order": 2,
    "content": "|  |  |\n| --- | --- |\n| [![SDML Logo](https://camo.githubusercontent.com/3144a76016c24b3f32281f151c46857601e5662e7e593013f3cb8024ff33af46/68747470733a2f2f73646d6c2e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032332f30312f53444d4c5f6c6f676f5f53715f677265792e706e67)](https://sdml.ua.edu/) | This repository provides a user-friendly Python package and source code for the automatic evaluation of flood inundation maps. It is developed under Surface Dynamics Modeling Lab (SDML), Department of Geography and the Environment at The University of Alabama, United States. |"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 3,
    "content": "## Flood Inundation Mapping Predictions Evaluation Framework (FIMeval)\n\n* * *\n\n[![Version](https://camo.githubusercontent.com/fd548941d0e6fc60ef94d90961b7ab3226c4cc417f86352983c6f88fd2dd1386/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f73646d6c75612f66696d6576616c)](https://github.com/sdmlua/fimeval/releases)[![Issues](https://camo.githubusercontent.com/78e06e2e4da2ed256fae169b620d15eebe4812b910755f48f3b9d0feb8d9897b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f73646d6c75612f66696d6576616c)](https://github.com/sdmlua/fimeval/issues)[![License: GPL v3](https://camo.githubusercontent.com/8a398fc9fbf479a323d2d91b9fcb6fb9c6b4d08e96dbb544488ccbed312115fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667)](https://opensource.org/licenses/GPL-3.0)[![PyPI version](https://camo.githubusercontent.com/a38932411d3c588776c3685b1f3dbbbddd6121534ea298e8528f3ec400308b10/68747470733a2f2f62616467652e667572792e696f2f70792f66696d6576616c2e7376673f69636f6e3d7369253341707974686f6e)](https://badge.fury.io/py/fimeval)[![PyPI Downloads](https://camo.githubusercontent.com/475d8ab2e84af2d7000ba608972d473683c68f1988e68bd44ae667a4debe4bc0/68747470733a2f2f7374617469632e706570792e746563682f62616467652f66696d6576616c)](https://pepy.tech/projects/fimeval)"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 4,
    "content": "### **Background**\n\n* * *\n\nThe accuracy of the flood inundation mapping (FIM) is critical for model development and disaster preparedness. The evaluation of flood maps from different sources using geospatial platforms can be tedious and requires repeated processing and analysis for each map. These preprocessing steps include extracting the correct flood extent, assigning the same projection system to all the maps, categorizing the maps as binary flood maps, removal of permanent water bodies, etc. This manual data processing is cumbersome and prone to human error.\n\nTo address these issues, we developed Flood Inundation Mapping Prediction Evaluation Framework (FIMeval), a Python-based FIM evaluation framework capable of automatically evaluating flood maps from different sources. FIMeval takes the advantage of comparing multiple target datasets with large benchmark datasets. It includes an option to incorporate permanent waterbodies as non-flood pixels with a user input file or pre-set dataset. In addition to traditional evaluation metrics, it can also compare the number of buildings inundated using a user input file or a pre-set dataset."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 5,
    "content": "### **Repository structure**\n\n* * *\n\nThe architecture of the `fimeval` integrates different modules to which helps the automation of flood evaluation. All those modules codes are in source ( `src` ) folder.\n\n```\nfimeval/\n‚îú‚îÄ‚îÄ docs/                       # Documentation (contains 'FIMserv' Tool usage sample codes)\n‚îÇ   ‚îî‚îÄ‚îÄ sampledata/              # Contains the sample data to demonstrate how this frameworks works\n‚îÇ   ‚îî‚îÄ‚îÄ fimeval_usage.ipynb            #Sample code usage of the Evaluation framework\n‚îú‚îÄ‚îÄ Images/                       # have sample images for documentation\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ fimeval/\n‚îÇ       ‚îú‚îÄ‚îÄBuildingFootprint/ # Contains the evaluation of model predicted FIM with microsoft building footprint\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ evaluationwithBF.py\n‚îÇ       ‚îî‚îÄ‚îÄ ContingencyMap/      # Contains all the metrics calculation and contingency map generation\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ evaluationFIM.py # main evaluation moodule\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ methods.py  # Contains 3 different methods of evaluation\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py  # metrics calculation module\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ plotevaluationmetrics.py  # use to vizualize the different performance metrics\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ printcontingency.py  # prints the contingency map to quickly generate the Map layout\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ PWBs3.py  # module which helps to get permanent water bodies from s3 bucket\n‚îÇ       ‚îî‚îÄ‚îÄ utilis.py   #Includes the resampling and reprojection of FIMs\n‚îî‚îÄ‚îÄ tests/                  # Includes test cases for different functionality\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 6,
    "content": "The graphical representation of fimeval pipeline can be summarized as follows in **`Figure 1`**. Here, it will show all the steps incorporated within the `fimeval` during packaging and all functionality are interconnected to each other, resulting the automation of the framework.\n\n[![image](https://github.com/sdmlua/fimpef/raw/main/Images/flowchart.jpg)](https://github.com/sdmlua/fimpef/blob/main/Images/flowchart.jpg)\n\nFigure 1: Flowchart showing the entire framework pipeline."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 7,
    "content": "### **Framework Installation and Usage**\n\n* * *\n\nThis framework is published as a python package in PyPI ( [https://pypi.org/project/fimeval/).For](https://pypi.org/project/fimeval/).For) directly using the package, the user can install this package using python package installer 'pip' and can import on their workflows:\n\n```\n#Install to use this framework\npip install fimeval\n\n#Use this framework in your workflows using poetry\npoetry add fimeval\n```\n\nImport the package to the jupyter notebook or any python IDE.\n\n```\n#Import the package\nimport fimeval as fp\n```\n\n**Note: The framework usage provided in detailed in [Here (docs/fimeval\\_usage.ipynb)](https://github.com/sdmlua/fimpef/blob/main/docs/fimeval_usage.ipynb)**. It has detail documentation from installation, setup, running- until results."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 8,
    "content": "#### **Main Directory Structure**\n\nThe main directory contains the primary folder for storing the case studies. If there is one case study, user can directly pass the case study folder as the main directory. Each case study folder must include a Benchmark FIM (B-FIM) with a 'benchmark' word assigned within the B-FIM file and different Model Predicted FIM (M-FIM)\nin tif format.\nFor mutilple case studies,the main directory could be structure in such a way that contain the seperate folders for individual case studies.For example, if a user has two case studies they should create two seperate folders as shown in the Figure below.\n\n[![image](https://github.com/sdmlua/fimpef/raw/main/Images/directorystructure.png)](https://github.com/sdmlua/fimpef/blob/main/Images/directorystructure.png)\n\nFigure 2: Main directory structure for one and multiple case study.\n\nThis directory can be defined as follows while running framework.\n\n```\nmain_dir = Path('./path/to/main/dir')\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 9,
    "content": "#### **Permanent Water Bodies (PWB)**\n\nThis framework uses PWB to first to delineate the PWB in the FIM and assign into different class so that the evaluation will be more fair. For the Contiguous United States (CONUS), the PWB is already integrated within the framework however, if user have more accurate PWB or using fimeval for outside US they can initialize and use PWB within fimeval framework. Currently it is using PWB publicly hosted by ESRI: [https://hub.arcgis.com/datasets/esri::usa-detailed-water-bodies/about](https://hub.arcgis.com/datasets/esri::usa-detailed-water-bodies/about)\n\nIf user have more precise PWB, they can input their own PWB boundary as .shp and .gpkg format and need to assign the shapefile of the PWB and define directory as,\n\n```\nPWD_dir = Path('./path/to/PWB/vector/file')\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 10,
    "content": "#### **Methods for Extracting Flood Extents**\n\n1. **`smallest_extent`**\n\nThe framework will first check all the raster extents (benchmark and FIMs). It will then determine the smallest among all the rasters. A shape file will then be created to mask all the rasters.\n\n2. **`convex_hull`**\n\nAnother provision of determining flood extent is the generation of the minimum bounding polygon along the valid shapes. The framework will select the smallest raster extent followed by the generation of the valid vector shapes from the raster. It will then generate the convex hull (minimum bounding polygon along the valid shapes).\n\n3. **`AOI`**\n\nUser can give input an already pre-defined flood extent vector file. This method will only be valid if user is working with their own evaluation boundary,\n\nDepending upon user preference, they need to pass those method name as a argument while running the evaluation.\n\nThe FIM evaluation extent for `smallest_extent` and `convex_hull` can be seen in below **Figure 3** which is GIS layout version of an contengency map output of `EvaluateFIM` module defined in Table 1.\n\n[![image](https://github.com/sdmlua/fimpef/raw/main/Images/methodslayout.jpg)](https://github.com/sdmlua/fimpef/blob/main/Images/methodslayout.jpg)\n\nFigure 3: Layout showing the difference between smallest extent and convex hull FIM extent and evaluation result.\n\nMethods can be defined as follows.\n\n```\nmethod_name = \"smallest_extent\"\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 11,
    "content": "For the method 'AOI', user also need to pass the shapefile of the AOI along with method name as AOI.\n\n```\n#For AOI based FIM evaluation\nmethod_name = \"AOI\"\nAOI  = Path('./path/to/AOI/vectorfile')\n```"
  },
  {
    "idurl": 49,
    "idtype": "table",
    "order": 12,
    "content": "| Module Name | Objective | Arguments | Outputs |\n| --- | --- | --- | --- |\n| `EvaluateFIM` | It runs all the evaluation of FIM between B-FIM and M-FIMs. | `main_dir`: Main directory containing the case study folders, <br>`method_name`: How users wants to evaluate their FIM, <br>`outpur_dir`: Output directory where all the results and the intermidiate files will be saved for further calculation, <br>_`PWB_dir`_: The permanenet water bodies vectory file directory if user wants to user their own boundary, <br>_`target_crs`_: this fimeval framework needs the floodmaps to be in projected CRS so define the projected CRS in epsg code format, <br>_`target_resolution`_: sometime if the benchmark is very high resolution than candidate FIMs, it needs heavy computational time, so user can define the resolution if there FIMs are in different spatial resolution, else it will use the coarser resolution among all FIMS within that case. | The outputs includes generated files in TIFF, SHP, CSV, and PNG formats, all stored within the output folder. Users can visualize the TIFF files using any geospatial platform. The TIFF files consist of the binary Benchmark-FIM (Benchmark.tif), Model-FIM (Candidate.tif), and Agreement-FIM (Contingency.tif). The shp files contain the boundary of the generated flood extent. |\n| `PlotContingencyMap` | For better understanding, It will print the agreement maps derived in first step. | `main_dir`, `method_name`, `output_dir` : Based on the those arguments, once all the evaluation is done, it will dynamically get the corresponding contingency raster for printing. | This prints the contingency map showing different class of evaluation (TP, FP, no data, PWB etc). The outputs look like- Figure 4 first row. |\n| `PlotEvaluationMetrics` | For quick understanding of the evaluation metrics, to plot bar of evaluation scores. | `main_dir`, `method_name`, `output_dir` : Based on the those arguments, once all the evaluation is done, it will dynamically get the corresponding file for printing based on all those info. | This prints the bar plots which includes different performance metrics calculated by EvaluateFIM module. The outputs look like- Figure 4 second row. |\n| `EvaluationWithBuildingFootprint` | For Building Footprint Analysis, user can specify shapefile of building footprints as .shp or .gpkg format. By default it consider global Microsoft building footprint dataset. Those data are hosted in Google Earth Engine (GEE) so, It pops up to authenticate the GEE account, please allow it and it will download the data based on evaluation boundary and evaluation is done. | `main_dir`, `method_name`, `output_dir`: Those arguments are as it is, same as all other modules. <br>_`building_footprint`_: If user wants to use their own building footprint file then pass the directory here, _`country`_: It is the 3 letter based country ISO code (eg. 'USA', NEP' etc), for the building data automation using GEE based on the evaluation extent, _`shapefile_dir`_: this is the directory of user defined AOI if user is working with their own boundary and automatic Building footprint download and evaluation. | It will calculate the different metrics (e.g. TP, FP, CSI, F1, Accuracy etc) based on hit and miss of building on different M-FIM and B-FIM. Those all metrics will be saved as CSV format in `output_dir` and finally using that info it prints the counts of building foorpint in each FIMs as well as scenario on the evaluation end via bar plot. |"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 13,
    "content": "#### **Executing the Evaluation framework**\n\nThe complete description of different modules, what they are meant for, arguments taken to run that module and what will be the end results from each is described in below **Table 1**. If user import `fimeval` framework as `fp` into workflows, they can call each module mentioned in **Table 1** as `fp.Module_Name(args)`. Here arguments in italic represents the optional field, depending upon the user requirement.\n\nTable 1: Modules in `fimeval` are in order of execution.\n\n\n\n\n[![](https://github.com/sdmlua/fimpef/raw/main/Images/methodsresults_combined.jpg)](https://github.com/sdmlua/fimpef/blob/main/Images/methodsresults_combined.jpg)\n\nFigure 4: Combined raw output from framework for different two method. First row (subplot a and b) and second row (subplot c and d) is contingency maps and evaluation metrics of FIM derived using `PrintContingencyMaP` and `PlotEvaluationMetrics` module. Third row (subplot e and f) is the output after processing and calculating of evaluation with BF by unsing `EvaluateWithBuildingFoorprint` module."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 14,
    "content": "## üîß Installation Instructions"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 15,
    "content": "### 1\\. ‚úÖ Prerequisites\n\nBefore installing `fimeval`, ensure the following software are installed:\n\n- **Python**: Version 3.10 or higher\n- **Anaconda**: For managing environments and dependencies\n- **GIS Software**: For Visulalisation\n\n  - [ArcGIS](https://www.esri.com/en-us/arcgis/products/index) or [QGIS](https://qgis.org/en/site/)\n- **Optional**:\n\n  - [Google Earth Engine](https://earthengine.google.com/) account\n  - Java Runtime Environment (for using GEE API)\n\n* * *"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 16,
    "content": "### 2\\. Install Anaconda\n\nIf Anaconda is not installed, download and install it from the [official website](https://www.anaconda.com/products/distribution).\n\n* * *"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 17,
    "content": "### 3\\. üåê Set Up Virtual Environment"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 18,
    "content": "#### üíª For Mac Users\n\nOpen **Terminal** and run:\n\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 19,
    "content": "# Create a new environment named 'fimeval'\nconda create --name fimeval python=3.10"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 20,
    "content": "# Activate the environment\nconda activate fimeval"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 21,
    "content": "# Install Jupyter Notebook\npip install notebook"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 22,
    "content": "# Install fimeval package\npip install fimeval\n\n```"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 23,
    "content": "### ‚òÅÔ∏è Google Colab Version\n\nTo use fimeval in Google Colab, follow the steps below:"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 24,
    "content": "## Upload Files\n\nUpload all necessary input files (e.g., raster, shapefiles, model outputs) to your Google Drive."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 25,
    "content": "## Open Google Colab\n\nGo to Google Colab and sign in with a valid Google account."
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 26,
    "content": "## Mount Google Drive\n\nIn a new Colab notebook, mount the Google Drive\n\n```\npip install fimeval\n```"
  },
  {
    "idurl": 49,
    "idtype": "table",
    "order": 27,
    "content": "|  |  |\n| --- | --- |\n| [![alt text](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama. |"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 28,
    "content": "### **Acknowledgements**"
  },
  {
    "idurl": 49,
    "idtype": "text",
    "order": 29,
    "content": "### **For More Information**\n\nContact [Sagy Cohen](https://geography.ua.edu/people/sagy-cohen/)\n( [sagy.cohen@ua.edu](https://github.com/sdmlua/fimpef/blob/main/mailto:sagy.cohen@ua.edu))\nDipsikha Devi, ( [ddevi@ua.edu](https://github.com/sdmlua/fimpef/blob/main/mailto:ddevi@ua.edu))\nSupath Dhital, ( [sdhital@crimson.ua.edu](https://github.com/sdmlua/fimpef/blob/main/mailto:sdhital@crimson.ua.edu))"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 1,
    "content": "# FIM Database for Multi-Model Visualization"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#introduction \"Direct link to Introduction\")\n\nFIM (Flood Inundation Mapping) Database is a framework designed to store compiled flood maps from multiple hydraulic models including reference flood maps.\nThe structure and workflow of the FIM Database for Multi-Model Flood Map Retrieval and Visualization is shown in Figure 1.\n\n![Methodology](https://docs.ciroh.org/assets/images/overall-feafa8f41ee7c2a7e2a4ae378d76ef30.png)\n\n**Figure 1:** Overall framework of the database\n\nThis documentation is intended to guide the water resources community to create a FIM (Flood Inundation Mapping) Database using flood maps generated from multiple hydraulic models. Users can upload and visualize custom flood maps, visualize flood extent scenarios for different flows, and compare flood extents by creating the database, storing it in hydroshare, and finally using the existing web application for the visualization. The application is available in the apps list in [CIROH portal apps](https://portal.ciroh.org/apps) with the name \"FIM (Flood Information Map Visualization Deck)\" as shown in Figure 2.\n\n![Website link](https://docs.ciroh.org/assets/images/FIMapp-966b50d4f9c8c53a9ca715df75d150b8.png)\n\n**Figure 2:** FIM Visualization Application\n\nTo access this application, click the arrow symbol located in the middle of the map image. You will then be navigated to the visualization application as shown in Figure 3."
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 3,
    "content": "![Visualization app](https://docs.ciroh.org/assets/images/webapp-7a1de49057e375821f12fc732590802e.png)\n\n**Figure 3:** FIM Visualization Application"
  },
  {
    "idurl": 50,
    "idtype": "table",
    "order": 4,
    "content": "| **S.N.** | **Generic File Name** | **Quantity** | **Description** |\n| --- | --- | --- | --- |\n| 1. | **FIM\\_input\\_data.csv** | 1 | Insert details of the models that are to be used in FIM database |\n| 2. | **Huc8.csv** | 1 | Assuming that the extent for all models is the same |\n| 3. | **`{River}_{model}_flows.csv`** | 1 for each model type | Contains rating curve information for each model. Enter the file names of the raster and vector files that were generated in step 4. |\n| 4. | **`Feature_ids_{model}.csv`** | 1 for each model type | Contains the list of NWM feature IDs for the area and their Lat, Long values |"
  },
  {
    "idurl": 50,
    "idtype": "table",
    "order": 5,
    "content": "| **ModelTypeID** | **Software** |\n| --- | --- |\n| 0 | HEC-RAS 1D |\n| 1 | HEC-RAS 2D |\n| 2 | HEC-RAS 1D/2D Combo |\n| 3 | SRH-2D |\n| 4 | FIER |\n| 5 | AutoRoute |\n| 6 | HAND |\n| 7 | TRITON |\n| 8 | Satellite Observations |\n| 9 | Surveyed Flood Extents |\n| 10 | Others |"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 6,
    "content": "## Creating a FIM database [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#creating-a-fim-database \"Direct link to Creating a FIM database\")\n\nIt is assumed that users already have at least one hydraulic model from which flood maps can be generated for any area of interest. We guide users to be able to compare the flood map from that model with the baseline flood map for the same area from NWM HAND using FIMServ tool developed by our CIROH partners in University of Alabama. The flowchart for the overall methodology for creating the database and using it for visualization is shown in Figure 4 and explained in detail in subsequent section.\n\n![Methodology](https://docs.ciroh.org/assets/images/process-998be844c6c3471942c3c0d002568a38.png)\n\n**Figure 4:** Methodology\n\n**Step 1:**"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 7,
    "content": "Re-run your flood model with as many flow values as you desire. A basic database model may include flood maps for flow values corresponding to return periods ranging from 2 years to 1000 years. If you are using the NWM flows, you can find the return period values using the [NWM\\_returnperiod\\_finder\\_Tool tool](https://colab.research.google.com/drive/1g3C-A-19w2gwr1kMBcV6d37RChWX0zoh) developed at Hydroinformatics Lab, BYU. You will need to have NWM feature IDs for the streams in your domain. You can use [NWM\\_FeatureID\\_finder\\_Tool](https://colab.research.google.com/drive/18CAyUd4ffUoNWDLvbaBVTw0XVSQrHNWI) to find the feature IDs for your domain. This code gives you the IDs along the mainstream path only.\n\n**Step 2:**\n\nRun the HAND (Height Above Nearest Drainage) model for your area of interest. Follow the following steps to generate flood maps using HAND method."
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 8,
    "content": "- Generate the shape file of the model boundary from the model you have for your area of interest.\n- Use the [FimServe\\_Tool](https://github.com/sdmlua/FIMserv) to create HAND-based flood maps by providing the same model boundary shapefile and the discharge values.\n- This tool uses NWM (National Water Model) retrospective data by default to generate flood maps.\n- To create flood maps for specific return periods, use return-period discharge values, instead of retrospective flow values, for each NWM reach ID using the tool mentioned in Step 1.\n- Make sure the flood maps generated from your model and HAND model correspond to same discharge values if you want to make meaningful comparison.\n- Clip the output maps with the model boundary shape file.\n\n**Step 3:**\n\nExtract all available flood maps generated from your hydraulic model and the HAND model.\nFlood maps may include:\n\n- Shapefiles of flood extents\n- Depth rasters\n- Water surface elevation rasters\n- Velocity rasters (if available)\n\n_Note: The HAND model generates only depth rasters, which can be converted to shapefiles using GIS tools._\n\n**Step 4:**\n\n- Reproject all raster and vector files to EPSG:4326 (WGS 84).\n- Ensure that the spatial extents of all files match, which is necessary for comparison in the FIM Visualization Application.\n- You can use GIS software or python codes for reprojection and alignment.\n\n**Step 5:**"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 9,
    "content": "Prepare input files for populating the database. Here is the list of input files that you need for creating and populating ONE database (corresponding to your area of interest).\n\n**Table 1** Required input files for creating FIM database.\n\n\n\n\nYou can use these [sample input files](https://byu-my.sharepoint.com/my?id=%2Fpersonal%2Fwaglep%5Fbyu%5Fedu%2FDocuments%2FSample%20files%20for%20database&ga=1) for your reference. Remember all the column headers should be exactly as they appear in these sample files.\n\nAlso, the name of the models in \"software\" column in FIM\\_input\\_data.csv file should match with the ones as shown in Table 2.\n\n**Table 2** The list of software names currently included in the database.\n\n\n\n\nTo get the HUC8 number for the area, use the latter part of [NWM\\_FeatureID\\_finder\\_Tool](https://colab.research.google.com/drive/18CAyUd4ffUoNWDLvbaBVTw0XVSQrHNWI).\n\n**Step 6:**\n\nUse the [FIM\\_database\\_tool](https://colab.research.google.com/drive/1zCBRJKPk11wsyHRp3OBVMtEiWEaS823u) for step-by-step instructions to create and upload the FIM database to your HydroShare account.\n\n- Upload all input files (from Step 5) and GIS files (from Step 4) to the content section of Google Collab notebook.\n- Run each code cell carefully following the notebook instructions."
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 10,
    "content": "## Database storage [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#database-storage \"Direct link to Database storage\")\n\nAfter completing the code execution from Step 6:\n\n- All GIS files will be uploaded to your HydroShare account.\n- Two JSON files will be generated:\n- `{filename}.json` is used for custom file visualization\n- `{filename}_vis.json` is used for scenario visualization\n\n![Flood map](https://docs.ciroh.org/assets/images/HydroShare-5477c55f695e7442aa7d8462f5b5a78e.png)\n\n**Figure 5:** HydroShare resource with GIS files and two JSON files used for visualization"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 11,
    "content": "## Visualization of database [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#visualization-of-database \"Direct link to Visualization of database\")"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 12,
    "content": "### Custom files visualization [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#custom-files-visualization \"Direct link to Custom files visualization\")\n\nTo view the individual flood maps, paste the URL of the `{databasename}.json` file in the FIM database uploader section of FIM Visualization Application. This will give you the list of all the flood map files available in your database. You can select the desired files that you want to visualize. The selected files will appear in the \"User Files\" section in the map layers of the visualization application. You can toggle between each individual files as shown in Figure 5.\n\n![Flood map](https://docs.ciroh.org/assets/images/custom-d1cb7f1e7e86b840c93922acab9f5f03.png)\n\n**Figure 6:** Custom files visualization in FIM visualization Application"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 13,
    "content": "### Scenario visualization [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#scenario-visualization \"Direct link to Scenario visualization\")\n\nTo view the scenarios, paste the URL of the `{databasename}_vis.json` file in the FIM Scenarios uploader section of the Visualization application. You can select the model type from the dropdown available in the top right panel. For a model, you can move the slider button and change the flow values to visualize the corresponding flood map as shown in Figure 7.\n\n![Flood map](https://docs.ciroh.org/assets/images/Scenario-b9e6e2ace7c58c5d23b4d1e662a2624e.png)\n\n**Figure 7:** Scenario visualization in FIM visualization Application"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 14,
    "content": "### Comparison of flood maps [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#comparison-of-flood-maps \"Direct link to Comparison of flood maps\")\n\nAny two flood maps from any two models across any scenarios can be compared. To do this, select the FIM comparison option in the Data Uploader section and choose \"Existing Scenarios\". Select the name of database, names of model, and the scenarios that you want to compare. This will allow comparative visualization of those scenarios along with displaying the comparison metrics as shown in Figure 8. Similar comparison can also be performed by uploading any two flood map raster files (with same raster boundary) externally from your machine by selecting \"User file upload\" instead of \"Existing Scenarios\".\n\n![Comparison of flood maps](https://docs.ciroh.org/assets/images/compare-e73c089fc972356c53af9581d3bdf705.png)\n\n**Figure 8:** Comparison of two flood maps indicating different inundation extents and displaying comparison metrics for the two inundation maps, including proportion correction, bias ratio, hit rate, kappa value, fitness statistics, and mixed index"
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 15,
    "content": "## Funding Acknowledgement [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#funding-acknowledgement \"Direct link to Funding Acknowledgement\")\n\nThis research was supported by the Cooperative Institute for Research to Operations in Hydrology (CIROH) with funding under award NA22NWS4320003 from the NOAA Cooperative Institute Program."
  },
  {
    "idurl": 50,
    "idtype": "text",
    "order": 16,
    "content": "## Contact information [‚Äã](https://docs.ciroh.org/docs/products/community-fim/fim-database/\\#contact-information \"Direct link to Contact information\")\n\nIf you have any queries, please contact:\n\nPitamber Wagle\n\nPh.D. Student, Brigham Young University\n\nEmail : [waglep@byu.edu](mailto:waglep@byu.edu)\n\n- [Introduction](https://docs.ciroh.org/docs/products/community-fim/fim-database/#introduction)\n- [Creating a FIM database](https://docs.ciroh.org/docs/products/community-fim/fim-database/#creating-a-fim-database)\n- [Database storage](https://docs.ciroh.org/docs/products/community-fim/fim-database/#database-storage)\n- [Visualization of database](https://docs.ciroh.org/docs/products/community-fim/fim-database/#visualization-of-database)\n  - [Custom files visualization](https://docs.ciroh.org/docs/products/community-fim/fim-database/#custom-files-visualization)\n  - [Scenario visualization](https://docs.ciroh.org/docs/products/community-fim/fim-database/#scenario-visualization)\n  - [Comparison of flood maps](https://docs.ciroh.org/docs/products/community-fim/fim-database/#comparison-of-flood-maps)\n- [Funding Acknowledgement](https://docs.ciroh.org/docs/products/community-fim/fim-database/#funding-acknowledgement)\n- [Contact information](https://docs.ciroh.org/docs/products/community-fim/fim-database/#contact-information)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 1,
    "content": "# RIVR"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction to the CIROH RIVR App [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#introduction-to-the-ciroh-rivr-app \"Direct link to Introduction to the CIROH RIVR App\")\n\nWelcome to the RIVR App, your go-to tool for real-time and forecast river flow information. Whether you're a kayaker, fisherman, or hydrologist, RIVR provides critical data to help you plan and understand river conditions. With the RIVR App, you can monitor river levels, access short-term and long-term forecasts, and manage a list of your favorite rivers for quick access."
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 3,
    "content": "## Background of the US NWM Streamflow Forecasts [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#background-of-the-us-nwm-streamflow-forecasts \"Direct link to Background of the US NWM Streamflow Forecasts\")\n\nThe RIVR App utilizes streamflow forecasts generated by the National Water Model (NWM). The NWM is a hydrologic model for the United States that simulates the water cycle, including streamflow, soil moisture, and evapotranspiration. By leveraging the NWM, RIVR provides users with up-to-date and scientifically sound river flow predictions."
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 4,
    "content": "## Creating a User Account [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#creating-a-user-account \"Direct link to Creating a User Account\")\n\nTo personalize your experience and manage favorite rivers, you'll need to create an account:\n\n1. On the registration screen, you'll be asked to enter your name, email address, and desired password.\n2. Ensure your password meets the strength requirements, including uppercase and lowercase letters, numbers, and special characters.\n3. Click the \"Register\" button to create your account.\n4. If you already have an account, select \"Login now\".\n\n![](https://docs.ciroh.org/assets/images/1-ed31432075c074f72c6f000ed6b42c9e.png)![](https://docs.ciroh.org/assets/images/3-5e15199070421a5ab380fb982f655b01.png)![](https://docs.ciroh.org/assets/images/4-2b9234594767b741b97cc54111720ad6.png)![](https://docs.ciroh.org/assets/images/5-ab266091f5e4b019344c3c2d1395e34a.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 5,
    "content": "## Finding Rivers using the Map Interface [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#finding-rivers-using-the-map-interface \"Direct link to Finding Rivers using the Map Interface\")\n\nThe RIVR App offers a map interface to find river monitoring stations:\n\n1. Upon opening the \"Add River\" section, you'll see a map with location markers.\n2. Zoom in or out to explore different areas.\n3. Markers indicate river monitoring stations. Tap on a marker to see more details. You may be able to also search for a place in the search bar, such as \"Louisiana\".\n4. Choose from different map views such as \"Streets,\" \"Outdoors,\" \"Light,\" \"Standard,\" and \"Satellite\".\n5. You will also be able to search by distance or name.\n\n![](https://docs.ciroh.org/assets/images/8-b3b669bbdb23f7e74c8e9ebbc0aefcd4.png)![](https://docs.ciroh.org/assets/images/11-b44333fb58c4da87a5b72d72ecd827bd.png)![](https://docs.ciroh.org/assets/images/10-b4678c7be1f841b1a25ad8e666452a4c.png)![](https://docs.ciroh.org/assets/images/9-a89f86e6ae4888edf6770a2955cf97da.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 6,
    "content": "## Viewing River Stations and Details [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#viewing-river-stations-and-details \"Direct link to Viewing River Stations and Details\")\n\n1. You will be able to see various stations that displays the Station ID, Type, and Coordinates.\n2. By clicking the station, you can see more details such as the Station ID, coordinates, and an \"Add to Favorites\" option.\n\n![](https://docs.ciroh.org/assets/images/12-c5984cd6626d09bc42d05725eb285400.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 7,
    "content": "## Editing and Managing Your List of Favorite Rivers [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#editing-and-managing-your-list-of-favorite-rivers \"Direct link to Editing and Managing Your List of Favorite Rivers\")\n\nManage your list of rivers within the RIVR App:\n\n1. Your list of favorite rivers will be shown in the \"My Rivers\" section.\n2. If you have no favorite rivers yet, you'll be notified. You can add rivers from the map interface (see \"Finding Rivers using the Map Interface\").\n3. To add a river to favorites, navigate to the river details and click the \"Add to Favorites\" or equivalent button.\n\n![](https://docs.ciroh.org/assets/images/13-de1d9323264a27034fad61fe747075db.png)![](https://docs.ciroh.org/assets/images/28-cc106b71637cce0179fbc246618f5b51.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 8,
    "content": "## Viewing Forecasts for a Specific River [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#viewing-forecasts-for-a-specific-river \"Direct link to Viewing Forecasts for a Specific River\")\n\nTo access detailed forecasts for a river:\n\n1. Select a river from your \"My Rivers\" list, or choose it from the map interface.\n2. You'll be taken to the river's detail page, where you can view current flow, flow status, and forecast information.\n3. The river's name and location (e.g., Louisville, Kentucky) will be displayed.\n4. Current flow is shown in ft¬≥/s.\n5. Flow status (e.g., \"Low\") indicates the current condition.\n6. Return period information is available showing 2-year, 5-year, 10-year, 25-year, 50-year, and 100-year flow benchmarks.\n7. Hydrographs (graphs of flow over time) are provided for different ranges.\n\n![](https://docs.ciroh.org/assets/images/15-a25fd5c53bdb8a8e7e0c3aabcd4f43fc.png)![](https://docs.ciroh.org/assets/images/16-0c2b2afda36173edb8ea4495b0975095.png)![](https://docs.ciroh.org/assets/images/17-b91c8fbda7bd5241e9ecf8b1e0cd88f8.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 9,
    "content": "## Hourly (Short Range) Forecasts [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#hourly-short-range-forecasts \"Direct link to Hourly (Short Range) Forecasts\")\n\nHourly forecasts provide detailed short-range flow predictions:\n\n1. Select the \"Hourly\" tab or option on the river's detail page.\n2. See the hourly flow forecast in ft¬≥/s for the next few hours.\n3. View the hourly hydrograph, which shows predicted flow over time. The graph may be available in different view options such as \"Wave View\".\n\n![](https://docs.ciroh.org/assets/images/18-5c3e4f0ed38fc0e4271658a05a70c79f.png)![](https://docs.ciroh.org/assets/images/19-53e921d0fbec682f42d521c4312e45cc.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 10,
    "content": "## Daily (Medium Range) Forecasts [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#daily-medium-range-forecasts \"Direct link to Daily (Medium Range) Forecasts\")\n\nDaily forecasts give medium-range flow predictions:\n\n1. Select the \"Daily\" tab or option on the river's detail page.\n2. View the daily flow forecast for the next several days.\n3. See the daily hydrograph showing predicted flow trends over the next few days.\n\n![](https://docs.ciroh.org/assets/images/20-6c50914799c81ad24d18f05c8071ba44.png)![](https://docs.ciroh.org/assets/images/21-7696c85d3d9a67b45a725a050a6d317b.png)![](https://docs.ciroh.org/assets/images/22-8d9ec4ad03ac7fa2504b1c83e3c0ee6f.png)![](https://docs.ciroh.org/assets/images/23-5559678b342541e380ef93aad5c45ba6.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 11,
    "content": "## Long Range Forecasts [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#long-range-forecasts \"Direct link to Long Range Forecasts\")\n\nLong-range (Monthly) forecasts provide an overview of flow trends:\n\n1. Select the \"Monthly\" tab or option on the river's detail page.\n2. See the monthly flow forecast for the upcoming month.\n3. View the monthly hydrograph depicting predicted flow trends over the month.\n\n![](https://docs.ciroh.org/assets/images/25-701c4faed9a38c85a9e9fe08ac27c0f4.png)![](https://docs.ciroh.org/assets/images/26-ece9291ec74bbf72b98c3591fe90529f.png)![](https://docs.ciroh.org/assets/images/27-fd6ce2c333c23698cc46a5dd73672e60.png)"
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 12,
    "content": "## User Account Management [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#user-account-management \"Direct link to User Account Management\")\n\nIn the settings you can see Units & Theme, Notifications & Alerts, Data Management, Help & Information, and Feedback & Support."
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 13,
    "content": "## About the Developers [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#about-the-developers \"Direct link to About the Developers\")\n\nThe RIVR App is developed by a dedicated team passionate about hydrology and technology. You can reach out for feedback and support through the app's settings."
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 14,
    "content": "## About CIROH [‚Äã](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/\\#about-ciroh \"Direct link to About CIROH\")\n\nThe RIVR App is developed in collaboration with CIROH. mentioned the CIROH Hydroinformatics Tools and Technologies and that CIROH has 27 universities collaborating together, over 90 research projects completed, more than 200 students supported, 75+ publications in top journals, and $35 million in research funding secured. It also mentioned some of the various resources of CIROH such as DocuHub, CIROH Portal, Google BigQuery NWM API, Tethys Platform, Pantarhei HPC, and NSF Access.\n\n**Note:** Please check the app's settings or contact support for the most up-to-date information and detailed feature explanations."
  },
  {
    "idurl": 51,
    "idtype": "text",
    "order": 15,
    "content": "- [Introduction to the CIROH RIVR App](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#introduction-to-the-ciroh-rivr-app)\n- [Background of the US NWM Streamflow Forecasts](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#background-of-the-us-nwm-streamflow-forecasts)\n- [Creating a User Account](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#creating-a-user-account)\n- [Finding Rivers using the Map Interface](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#finding-rivers-using-the-map-interface)\n- [Viewing River Stations and Details](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#viewing-river-stations-and-details)\n- [Editing and Managing Your List of Favorite Rivers](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#editing-and-managing-your-list-of-favorite-rivers)\n- [Viewing Forecasts for a Specific River](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#viewing-forecasts-for-a-specific-river)\n- [Hourly (Short Range) Forecasts](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#hourly-short-range-forecasts)\n- [Daily (Medium Range) Forecasts](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#daily-medium-range-forecasts)\n- [Long Range Forecasts](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#long-range-forecasts)\n- [User Account Management](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#user-account-management)\n- [About the Developers](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#about-the-developers)\n- [About CIROH](https://docs.ciroh.org/docs/products/mobile-apps/RIVR/#about-ciroh)"
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 1,
    "content": "# What is NGIAB?"
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 2,
    "content": "## What is the National Water Model? [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/\\#what-is-the-national-water-model \"Direct link to What is the National Water Model?\")\n\nThe [National Water Model](https://water.noaa.gov/about/nwm) is the United States' core water prediction framework,\noffering predictions for the Continental United States (CONUS), Alaska, Hawaii, Puerto Rico, and the U.S. Virgin Islands.\n\nWhile the model's core implementation is administered by the NOAA's Office of Water Prediction (NOAA-OWP),\nit is built on free and open-source components, allowing the broader hydrology community to run it\nlocally, make adjustments, and propose changes."
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 3,
    "content": "## What is the NextGen Framework? [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/\\#what-is-the-nextgen-framework \"Direct link to What is the NextGen Framework?\")\n\nThe Next Generation Water Resources Modeling Framework, most frequently referred to as NextGen,\nis a hydrologic modeling framework that forms the core for modern versions of the National Water Model.\n\nThe NextGen Framework aims to resolve current limitations of the National Water Model by transitioning to a highly modular, model-agnostic framework\nthat improves regional interoperability and by making it easier for the hydrology community to append their own models.\nTo enable these tenets, NextGen uses the Basic Model Interface (BMI) to standardize the format of each model's\ninput, output, and properties, thus allowing for models written in any language to quickly be coupled together\nto create new outputs. These models are combined according to external configuration files, which ensures that all\nmodel runs are highly portable and reproducible.\n\nAs of version 3.0 of the National Water Model, which was adopted in 2024, the NWM represents a specific configuration\nof the NextGen Framework, which is freely available for use by the scientific community.\n\n> _Source: [The National Water Model, NOAA-OWP](https://www.weather.gov/media/owp/oh/docs/2021-OWP-NWM-NextGen-Framework.pdf)_"
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 4,
    "content": "## What is NGen? [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/\\#what-is-ngen \"Direct link to What is NGen?\")\n\n[NGen](https://github.com/NOAA-OWP/ngen/tree/master) is the core implementation of the NextGen framework,\nresponsible for administering connections between encapsulated models.\nWhile NGen is still under development, it's become an incredibly powerful tool for novel approaches to hydrology.\n\nUnfortunately, these advantages are stymied somewhat by the difficulty of installing NGen.\nNo standard installation process is currently available,\ninstead requiring users to manage a [lengthy chain](https://github.com/NOAA-OWP/ngen/blob/master/INSTALL.md)\nof dependencies, compilers, and environmental conditions.\nThis can already be a time-consuming task for experienced programmers,\nso for scientists wanting to apply it in their own research,\ninstalling and configuring NGen is severely unapproachable.\nThese challenges fundamentally undermine the NextGen framework's core objectives regarding community accessibility and contribution."
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 5,
    "content": "## What is NextGen In A Box? [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/\\#what-is-nextgen-in-a-box \"Direct link to What is NextGen In A Box?\")\n\nFortunately, there's a solution to this problem!\n[NextGen In A Box](https://ngiab.ciroh.org/) is a _containerized_ distribution of the NextGen framework.\nContainerization is an approach to software distribution and deployment that directly defines\nthe host operating system, dependencies, and run conditions for software,\nwhich allows the software to be run in effectively identical conditions regardless of the host system.\nThis means that with NextGen In A Box, anyone can get NextGen up and running in _as few as 30 minutes!_\n\nAdditionally, NextGen In A Box acts as the host platform for a full ecosystem of free, open-source\nutilities that allow users to customize, evaluate, and visualize their model runs.\nThese tools lower the barrier to entry even further, allowing scientists to seamlessly\nintegrate the NextGen framework into their workflows and produce exciting new discoveries.\n\n* * *\n\nIf you'd like to get started with NextGen In A Box, check out the [NGIAB 101](https://docs.ciroh.org/training-NGIAB-101/) module,\nwhich contains everything you'll need to know to get started.\n\n[NGIAB 101 Module](https://docs.ciroh.org/training-NGIAB-101/)"
  },
  {
    "idurl": 52,
    "idtype": "text",
    "order": 6,
    "content": "- [What is the National Water Model?](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/#what-is-the-national-water-model)\n- [What is the NextGen Framework?](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/#what-is-the-nextgen-framework)\n- [What is NGen?](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/#what-is-ngen)\n- [What is NextGen In A Box?](https://docs.ciroh.org/docs/products/ngiab/intro/what-is/#what-is-nextgen-in-a-box)"
  },
  {
    "idurl": 53,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NGIAB at a Glance\nsource: https://docs.ciroh.org/docs/products/ngiab/intro/at-a-glance\nscraped_date: 2025-07-31\n---"
  },
  {
    "idurl": 53,
    "idtype": "text",
    "order": 2,
    "content": "# NGIAB at a Glance\n\nExplore the NextGen In A Box (NGIAB) ecosystem through the interactive tabs below.\n\nClick on Key Features, Capabilities, or Access Methods to learn more.\n\n* * *"
  },
  {
    "idurl": 53,
    "idtype": "table",
    "order": 3,
    "content": "| NGIAB and Extensions | Key features | NOAA-OWP Tools/Libraries Utilized |\n| --- | --- | --- |\n| [Data Preprocess](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor/) | - Specializes in initial data preparation<br>- Handles subsetting and forcing processing<br>- Supports basic data processing tasks<br>- Helps with running NGIAB | - t-route<br>- hydrotools<br>- hydrofabric tools |\n| NGIAB Implementation<br>( [Cloud](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/), [HPC](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-singularity/)) | - Focused specifically on model execution<br>- Core engine for running simulations<br>- Does not handle pre/post-processing tasks |  |\n| [TEEHR Evaluation](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr/) | - Handles both input and output processing<br>- Supports full workflow, from data preparation to cloud deployment | Built to evaluate OWP model outputs |\n| [Data Visualizer](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer/) | - Focused on analysis and validation<br>- Supports data processing and output analysis | Designed for OWP hydrofabric visualization |\n| [DataStreamCLI](https://docs.ciroh.org/docs/products/research-datastream/) | - Complete workflow for creating inputs for and executing NGIAB and managing outputs<br>- Backend of the NextGen Research DataStream<br>- Discrete tooling for tasks like forcing processing and BMI file generation | - ngen-cal<br>- t-route<br>- hydrofabric tools |\n| [NGIAB-Cal](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration/) | - Simplifies hydrologic model calibration for NGIAB workflows<br>- Creates calibration directory and configurations within the NGIAB folder structure<br>- Runs calibration process using Docker<br>- Copies calibrated parameters to model configurations | - ngen-cal |"
  },
  {
    "idurl": 53,
    "idtype": "text",
    "order": 4,
    "content": "## Key Features"
  },
  {
    "idurl": 53,
    "idtype": "table",
    "order": 5,
    "content": "| Capabilities | [Data Preprocess](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor/) | NGIAB Implementation<br>( [Cloud](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/), [HPC](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/)) | [TEEHR Evaluation](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr/) | [Data Visualizer](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer/) | [DataStreamCLI](https://docs.ciroh.org/docs/products/research-datastream/) | [NGIAB-Cal](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration/) |\n| --- | --- | --- | --- | --- | --- | --- |\n| GUI | ‚úÖ | - | - | ‚úÖ | - | - |\n| Hydrofabric Subsetting | ‚úÖ | - | - | ‚úÖ (view only) | ‚úÖüî® | - |\n| NetCDF Forcing Processing | - | - | ‚úÖ | - | ‚úÖüî® | - |\n| Zarr Forcing Processing | ‚úÖ | - | ‚úÖ | - | - | - |\n| Forcing Metadata Generation | ‚úÖ | - | - | - | ‚úÖüî® | - |\n| Calibration Configuration Generation | ‚úÖ | - | - | - | ‚úÖüî® | ‚úÖ |\n| NGIAB Input Generation | ‚úÖ | - | - | - | ‚úÖüî® | - |\n| Remote NGIAB Run | - | - | - | - | ‚úÖ | - |\n| Local NGIAB Run | - | ‚úÖ | - | - | ‚úÖ | - |\n| NGIAB Output Analysis | - | - | ‚úÖ | ‚úÖ | - | - |\n| NGIAB Output (in cloud) | - | - | ‚úÖ | - | ‚úÖ | - |\n| Calibrated Parameter Output | - | - | - | - | - | ‚úÖ |\n| Integrated into NGIAB | - | N/A | - | ‚úÖ | - | ‚úÖ |"
  },
  {
    "idurl": 53,
    "idtype": "text",
    "order": 6,
    "content": "## Capabilities"
  },
  {
    "idurl": 53,
    "idtype": "table",
    "order": 7,
    "content": "| Access method | [Data Preprocess](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor/) | NGIAB Implementation<br>( [Cloud](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/), [HPC](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/)) | [TEEHR Evaluation](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr/) | [Data Visualizer](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer/) | [DataStreamCLI](https://docs.ciroh.org/docs/products/research-datastream/) | [NGIAB-Cal](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration/) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Docker | - | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |\n| Python Package (pip/uv) | ‚úÖ | ‚úÖ | ‚úÖ | - | - | ‚úÖ |\n| Web Interface | ‚úÖ | - | - | ‚úÖ | - | - |\n| Notebook (ipynb) | - | - | ‚úÖ | - | - | - |\n| Singularity (HPC) | - | ‚úÖ | - | - | - | - |"
  },
  {
    "idurl": 53,
    "idtype": "text",
    "order": 8,
    "content": "## Access Methods"
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 1,
    "content": "# Installing NGIAB\n\nTo install NGIAB locally, you'll want to use the [NGIAB-CloudInfra](https://github.com/ciroh-ua/ngiab-cloudinfra) distribution of NGIAB.\n\nNGIAB is designed to run on Unix systems, which means that it's a bit easier to run on Mac and Linux computers.\nHowever, Windows users can still make full use of NGIAB using Windows Subsystem for Linux (WSL)."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 2,
    "content": "## Installing Prerequisites [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#installing-prerequisites \"Direct link to Installing Prerequisites\")"
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 3,
    "content": "### Windows [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#windows \"Direct link to Windows\")\n\n1. **Install WSL:** Head over to Microsoft's official documentation and follow their comprehensive guide on installing WSL: [https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install)\n2. **Install Docker Desktop:** Begin by downloading and installing Docker Desktop from the official website: [https://docs.docker.com/desktop/install/windows-install/#install-docker-desktop-on-windows](https://docs.docker.com/desktop/install/windows-install/#install-docker-desktop-on-windows)\n3. **Start Docker Desktop:** After installation, launch the Docker Desktop application.\n4. **Open WSL as Admin:** Right-click on the WSL icon and select \"Run as Administrator\".\n5. **Verify Installation:** In the WSL window, type the command docker ps -a to check if Docker is running correctly. This command should display a list of Docker containers.\n\nwarning\n\nIf you've installed WSL before as a part of Docker, be sure to create a second WSL distribution that isn't tied to Docker.\nNextGen In A Box shell commands can't be run from within Docker's dedicated WSL environment."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 4,
    "content": "### Mac [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#mac \"Direct link to Mac\")\n\n1. **Install Docker Desktop:** Download and install Docker Desktop for Mac from: [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)\n2. **Start Docker Desktop:** Launch the Docker Desktop application once the installation is complete.\n3. **Open Terminal:** Open the Terminal application on your Mac.\n4. **Verify Installation:** Similar to Windows, use the command docker ps -a in the Terminal to verify Docker is functioning as expected."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 5,
    "content": "### Linux [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#linux \"Direct link to Linux\")\n\n1. **Install Docker:** The installation process for Linux varies depending on your distribution. Refer to the official documentation for detailed instructions: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)\n2. **Start Docker and Verify:** Follow the same steps as described for Mac to start Docker and verify its installation using the docker ps -a command in the terminal."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 6,
    "content": "## Installing NGIAB-CloudInfra [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#installing-ngiab-cloudinfra \"Direct link to Installing NGIAB-CloudInfra\")\n\n- **Input Data:**\n  - **Download Sample Data:** Use the provided commands to download sample data for the Sipsey Fork case study.\n  - **To generate your own data:** Refer to the [NGIAB-datapreprocessor](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess) for instructions on generating custom input data.\n  - **To generate your own data and run using NGIAB:** Refer to the [ngen-datastream repository](https://github.com/CIROH-UA/ngen-datastream/tree/main) for instructions on generating custom input data.\n\nThis section guides you through downloading and preparing the sample input data for the NextGen In A Box project."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 7,
    "content": "### Step 1: Create Project Directory [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#step-1-create-project-directory \"Direct link to Step 1: Create Project Directory\")\n\n- **Windows users: WSL (Right click and run as Admin):** For ease of access, you may want to store NGIAB's files in your Windows directories. To move there, run the following in your WSL CLI:\n\n```codeBlockLines_e6Vv\ncd /mnt/c/Users/<Folder>\n\n```\n\n- From there, navigate to the directory where you'd like to store NGIAB and its associated data.\n\n```codeBlockLines_e6Vv\nmkdir -p NextGen\ncd NextGen\n\n```"
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 8,
    "content": "### Step 2: Download Sample Data [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#step-2-download-sample-data \"Direct link to Step 2: Download Sample Data\")\n\ninfo\n\nWhile this step isn't strictly necessary, it'll be useful for verifying that NGIAB is working properly on your system.\n\n- Within your project directory, create the `ngen-data` folder to hold the sample data.\n\n```codeBlockLines_e6Vv\nmkdir -p ngen-data\ncd ngen-data\n\n```\n\n- Use wget to download the compressed data file. Then, extract it.\n\n```codeBlockLines_e6Vv\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\ntar -xf AWI_16_10154200_009.tar.gz\n\n```\n\n- Then, return to the root of the project directory.\n\n```codeBlockLines_e6Vv\ncd ..\n\n```"
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 9,
    "content": "### Step 3: Clone and Run NGIAB [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/install/\\#step-3-clone-and-run-ngiab \"Direct link to Step 3: Clone and Run NGIAB\")\n\nwarning\n\n**For WSL users:** Before pulling NGIAB, ensure that Git is configured to pull with LF line breaks instead of CRLF line breaks.\nFailing to do so will prevent NGIAB's shell scripts from correctly running.\nInformation on triaging this issue is available in the [NGIAB 101 training module](https://docs.ciroh.org/training-NGIAB-101/installation.html).\n\n- Clone the NGIAB-CloudInfra repository.\n\n```codeBlockLines_e6Vv\ngit clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\ncd NGIAB-CloudInfra\n\n```\n\n- At this point, everything you need to install NGIAB has been installed!\n- To test your installation, try running the interactive guide script, which will help you navigate your first model run:\n\n```codeBlockLines_e6Vv\n./guide.sh\n\n```\n\ninfo\n\nFor a broader introduction to using the NGIAB ecosystem, please see the [NGIAB 101 training module](https://docs.ciroh.org/training-NGIAB-101/)."
  },
  {
    "idurl": 54,
    "idtype": "text",
    "order": 10,
    "content": "- [Installing Prerequisites](https://docs.ciroh.org/docs/products/ngiab/intro/install/#installing-prerequisites)\n  - [Windows](https://docs.ciroh.org/docs/products/ngiab/intro/install/#windows)\n  - [Mac](https://docs.ciroh.org/docs/products/ngiab/intro/install/#mac)\n  - [Linux](https://docs.ciroh.org/docs/products/ngiab/intro/install/#linux)\n- [Installing NGIAB-CloudInfra](https://docs.ciroh.org/docs/products/ngiab/intro/install/#installing-ngiab-cloudinfra)\n  - [Step 1: Create Project Directory](https://docs.ciroh.org/docs/products/ngiab/intro/install/#step-1-create-project-directory)\n  - [Step 2: Download Sample Data](https://docs.ciroh.org/docs/products/ngiab/intro/install/#step-2-download-sample-data)\n  - [Step 3: Clone and Run NGIAB](https://docs.ciroh.org/docs/products/ngiab/intro/install/#step-3-clone-and-run-ngiab)"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 1,
    "content": "# Glossary"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 2,
    "content": "## Concepts and Terms"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 3,
    "content": "### Catchment\n\nA specific, contiguous geographic area. In hydrologic modeling, larger regions are divided into catchments to allow for localized modeling and outputs.\n\nNote that catchment IDs may not be persistent across hydrofabrics. Instead, [points of interest](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#point-of-interest) should be used as persistent identifiers."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 4,
    "content": "### Calibration\n\nThe process of tuning a hydrologic model's parameters to more accurately match observed outcomes.\n\nThe NGIAB ecosystem offers the [NGIAB Calibration](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-calibration) utility to streamline this process."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 5,
    "content": "### Evaluation\n\nThe process of assessing the quality and performance of a hydrologic model.\n\nThe NGIAB ecosystem offers [integration](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr) with the [TEEHR](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#teehr) toolkit to streamline this process."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 6,
    "content": "### Forcing\n\nValues provided as inputs for a hydrologic model. The term comes from the practice of experimentally \"forcing\" certain input values by overriding the usual flow of data into a model."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 7,
    "content": "### Formulation\n\nSome set of defined links between modules within the NextGen Framework."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 8,
    "content": "### Gage\n\nShort for a stream gauge or streamgage. Fixed locations where streamflow properties such as water level and streamflow are measured and recorded."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 9,
    "content": "### Hydrofabric\n\nBroadly speaking, a hydrofabric defines the physical hydrology of a region, allowing for the region to be modeled.\nThese hydrofabrics are defined by physical attributes and the links between those attributes.\n\n_For the R library, see \" [Hydrofabric (R library)](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#hydrofabric-r-library)\"._\n\n_For the hydrofabric used as a baseline by most NextGen models, see \" [USGS-NOAA Reference Fabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#usgs-noaa-reference-fabric)\"._"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 10,
    "content": "### Nexus\n\nA point of data exchange within a NextGen network. Nexuses are placed at endpoint locations where water flows to within a [catchment](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#catchment), such as along its borders.\n\nNote that nexus IDs may not be persistent across hydrofabrics. Instead, [points of interest](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#point-of-interest) should be used as persistent identifiers."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 11,
    "content": "### Point of Interest\n\nA physical position that is being used in a model or hydrofabric, such as a [gage](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#gage) location. Points of interest are tracked by a wide variety of sources,\nincluding the Army Corp National Inventory of Dams and the USGS Gages III database.\n\nWithin the [USGS-NOAA reference fabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#usgs-noaa-reference-fabric), points of interest are given a persistent POI identifier to allow for tracking across hydrofabric versions."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 12,
    "content": "### Realization\n\nThe instructions that tell the NextGen Framework how to run a simulation.\n\nRealizations are stored in model realization files. These files contain [formulations](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#formulation) of modules, along with the order in which those modules should be run."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 13,
    "content": "### Subsetting\n\nThe practice of taking a subset of a larger dataset. Subsetting is both common and highly recommended for model runs, as it can greatly reduce the computational load required.\n\nBoth GUI and command-line utilities for subsetting the NextGen reference fabric are available within the [Data Preprocess](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor/) utility."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 14,
    "content": "### Vector Processing Unit (VPU)\n\nRegional discretizations used by the [USGS-NOAA reference fabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#usgs-noaa-reference-fabric). These VPUs designate fixed regions for model runs and data collections.\n\nSomewhat related to [catchments](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#catchment), though VPUs are significantly larger and act as collections of contiguous catchments."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 15,
    "content": "## File Formats"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 16,
    "content": "### Geopackage ( `.gpkg`)\n\nAn open, non-proprietary data format for geographic inforamtion systems. NextGen uses geopackages to contain [hydrofabrics](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#hydrofabric)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 17,
    "content": "### JSON ( `.json`)\n\nShort for \"JavaScript Object Notation\", JSON files are used to store objects consisting of name-value pairs. Despite the name, JSON sees wide use in non-JavaScript contexts.\n\nNextGen uses JSON to store its model [realization](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#realization) files ( `ngen-run/config/realization.json`), which tell the NextGen engine how to execute a model run."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 18,
    "content": "### NetCDF ( `.nc`)\n\nAn open standard for arrays of scientific data. NextGen uses the netCDF-4 standard to contain model variable metadata."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 19,
    "content": "### Tarball ( `.tar.gz`)\n\nA common archive format among Linux users. Analogously to formats like `.zip` and `.rar`, it allows for folders of content to be shared as a downloadable file."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 20,
    "content": "### YAML ( `.yaml`/ `.yml`)\n\nShort for \"Yet Another Markup Language\", YAML files are typically used to store metadata.\nNextGen uses a YAML file to store run configurations ( `ngen-run/config/ngen.yaml`)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 21,
    "content": "## Tools and Software"
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 22,
    "content": "### Basic Modeling Interface (BMI)\n\nA standardized set of function and parameter bindings that allow models to interact with external components in a predictable, modular way.\n\nBMI has been adopted as the standard for models compatible with the NextGen framework. Documentation for BMI is available [here](https://bmi.csdms.io/en/stable/)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 23,
    "content": "### Hydrofabric (R library)\n\nA standard library maintained by NOAA-OWP for processing and updating [hydrofabrics](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#hydrofabric) in R.\n\nInformation on Hydrofabric is available [here](https://github.com/NOAA-OWP/hydrofabric)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 24,
    "content": "### National Hydrography Dataset Plus (NHDPlus)\n\nA comprehensive reference [hydrofabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#hydrofabric) for the United States, maintained by the EPA and USGS. [NHDPlusV2](https://www.epa.gov/waterdata/get-nhdplus-national-hydrography-dataset-plus-data) is the most current and relevant version.\n\nRelevant to NextGen due to its use as a base for the [USGS-NOAA reference fabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#usgs-noaa-reference-fabric). As such, many tools and utilities designed for the NHD and its descendents will also work for NextGen hydrofabrics."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 25,
    "content": "### TEEHR\n\nShort for \"Tools for Exploratory Evolution in Hydrologic Research\", TEEHR is a Python library that provides comprehensive tooling for model [evaluation](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#evaluation). TEEHR is [included as a part of the NGIAB ecosystem](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-teehr).\n\nInformation on TEEHR is available [here](https://rtiinternational.github.io/teehr/)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 26,
    "content": "### Tethys Platform\n\nTethys is a Python-based platform for building geospatial web apps. It forms the core of NGIAB's [Data Visualizer](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-visualizer), alongside many other CIROH applications.\n\nInformation on the Tethys Platform is available [here](https://www.tethysplatform.org/)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 27,
    "content": "### t-route\n\nA dynamic channel routing model using in determining routing for river networks. T-route is BMI-compliant and is used to prepare hydrofabrics for both the National Water Model and NGIAB.\n\nInformation on t-route is available [here](https://github.com/NOAA-OWP/t-route)."
  },
  {
    "idurl": 55,
    "idtype": "text",
    "order": 28,
    "content": "### USGS-NOAA Reference Fabric\n\nA [hydrofabric](https://docs.ciroh.org/docs/products/ngiab/intro/glossary/#hydrofabric) jointly developed by the NOAA, USGS, and Lynker Spatial, which provides a shared baseline for NextGen models.\n\nImplementation and usage information for this fabric is available [here](https://noaa-owp.github.io/hydrofabric/articles/02-design-deep-dive.html)."
  },
  {
    "idurl": 56,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen Run Directories\n\ninfo\n\nMirrored from the NGIAB 101 training module's \" [Data Preparation](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html)\" section.\n\nRunning NextGen requires building a standard run directory complete with only the necessary files. This is done automatically with the [Data Preprocess](https://docs.ciroh.org/docs/products/ngiab/components/ngiab-preprocessor) tool. Below is an explanation of the standard run directory."
  },
  {
    "idurl": 56,
    "idtype": "text",
    "order": 2,
    "content": "### Root directory `ngen-run/` [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/directories/\\#root-directory-ngen-run \"Direct link to root-directory-ngen-run\")\n\nA NextGen run directory `ngen-run` contains the following subfolders:\n\n- `config`: model configuration files and hydrofabric configuration files. (required)\n- `forcings`: catchment-level forcing timeseries files. Forcing files contain variables like wind speed, temperature, precipitation, and solar radiation. (required)\n- `lakeout`: for t-route (optional)\n- `metadata` programmatically generated folder used within ngen. Do not edit this folder. (automatically generated)\n- `outputs`: This is where ngen will place the output files. (required)\n- `restart`: For restart files (optional)\n\n```codeBlockLines_e6Vv\nngen-run/\n‚îÇ\n‚îú‚îÄ‚îÄ config/\n‚îÇ\n‚îú‚îÄ‚îÄ forcings/\n‚îÇ\n‚îú‚îÄ‚îÄ lakeout/\n|\n‚îú‚îÄ‚îÄ metadata/\n‚îÇ\n‚îú‚îÄ‚îÄ outputs/\n‚îÇ\n‚îú‚îÄ‚îÄ restart/\n\n```"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 3,
    "content": "|   |"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 4,
    "content": "|   |"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 5,
    "content": "|   |"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 6,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 7,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 56,
    "idtype": "table",
    "order": 8,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 56,
    "idtype": "text",
    "order": 9,
    "content": "### Configuration directory `ngen-run/config/` [‚Äã](https://docs.ciroh.org/docs/products/ngiab/intro/directories/\\#configuration-directory-ngen-runconfig \"Direct link to configuration-directory-ngen-runconfig\")\n\nThis folder contains the NextGen realization file, which serves as the primary model configuration for the ngen framework. This file specifies which models to run (such as NoahOWP/CFE, LSTM, etc), run parameters like date and time, and hydrofabric specifications (like location, gage, catchment).\n\nBased on the models defined in the realization file, [BMI](https://bmi.csdms.io/en/stable/index.html) configuration files may be required. For those models that require per-catchment configuration files, a folder will hold these files for each model in `ngen-run/config/cat-config`. See the directory structure convention below.\n\n```codeBlockLines_e6Vv\nngen-run/\n|\n‚îú‚îÄ‚îÄ config/\n|   ‚îÇ\n|   ‚îú‚îÄ‚îÄ nextgen_09.gpkg\n\n\n|   ‚îú‚îÄ‚îÄ realization.json\n\n\n|   ‚îú‚îÄ‚îÄ ngen.yaml\n\n\n|   ‚îú‚îÄ‚îÄ cat-config/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄPET/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄCFE/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄNOAH-OWP-M/\n\n```\n\nNextGen requires a single geopackage file. This file is the [hydrofabric (Johnson, 2022)](https://mikejohnson51.github.io/hyAggregate/) (spatial data). An example geopackage can be found on [Lynker-Spatial's website](https://www.lynker-spatial.com/data?path=hydrofabric%2Fv2.2%2F). Tools to subset a geopackage into a smaller domain can be found at [Lynker's hfsubset](https://github.com/LynkerIntel/hfsubset)."
  },
  {
    "idurl": 56,
    "idtype": "text",
    "order": 10,
    "content": "- [Root directory `ngen-run/`](https://docs.ciroh.org/docs/products/ngiab/intro/directories/#root-directory-ngen-run)\n- [Configuration directory `ngen-run/config/`](https://docs.ciroh.org/docs/products/ngiab/intro/directories/#configuration-directory-ngen-runconfig)"
  },
  {
    "idurl": 57,
    "idtype": "table",
    "order": 1,
    "content": "|  |  |\n| --- | --- |\n| [![CIROH Logo](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/ciroh-bgsafe.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/ciroh-bgsafe.png) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 2,
    "content": "# NextGen In A Box (NGIAB)\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/README.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/README.md).\n\n> Run the NextGen National Water Resources Modeling Framework locally with ease.\n\n[NGIAB](https://ngiab.ciroh.org/) provides a containerized and user-friendly solution for running the NextGen framework, allowing you to control inputs, configurations, and execution on your local machine.\n\n[![](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/ngiab.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/ngiab.png)\n\n\n\n\n[![ARM Build and push final image](https://github.com/CIROH-UA/NGIAB-CloudInfra/actions/workflows/docker_image_main_branch.yml/badge.svg)](https://github.com/CIROH-UA/NGIAB-CloudInfra/actions/workflows/docker_image_main_branch.yml)"
  },
  {
    "idurl": 57,
    "idtype": "table",
    "order": 3,
    "content": "|  |  |\n| --- | --- |\n| [![Nexus Output](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/Provo_GeoSpatial.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/Provo_GeoSpatial.png) | [![Catchment Time Series](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/Provo_catchments.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/Provo_catchments.png) |"
  },
  {
    "idurl": 57,
    "idtype": "table",
    "order": 4,
    "content": "|  |  |\n| --- | --- |\n| [![Teehr Plot](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/Provo_nexus_point.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/Provo_nexus_point.png) | [![Teehr Metrics](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/Provo_teehr_metrics.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/Provo_teehr_metrics.png) |"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 5,
    "content": "## Features\n\n- **Run NextGen Locally**: Experiment with the framework on your machine\n- **Control Over Inputs**: Choose specific regions/basins and modify input data\n- **Simplified Setup**: Easy deployment using Docker containers\n- **Open Research**: Promotes transparency through open-source tooling\n- **Evaluation Tools**: Integrated TEEHR evaluation capabilities\n- **Visualization**: Built-in support for output visualization via Tethys Platform\n\n\n\n\n\n\n\n> üîó For more information on this case study, including calibration files, input data, and setup details, visit the [HydroShare resource page for the Provo River near Woodland NGIAB Case Study](https://www.hydroshare.org/resource/88e0ebf2719c492381efcb27fba71032/)."
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 6,
    "content": "## Navigating this repository"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 7,
    "content": "### For general use\n\n- **NGIAB Guide Scripts**: This repository holds several guide scripts: `guide.sh`, `runTeehr.sh`, and `viewOnTethys.sh`. These scripts are the recommended way to run NGIAB.\n\n- **Documentation**: The [`docs/` folder](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/00_CONTENTS.md) contains information on all of the finer details that can help you get the most out of the contents of this repository.\n  - For broader ecosystem-wide documentation, please visit DocuHub at [docs.ciroh.org/products/ngiab](https://docs.ciroh.org/products/ngiab), where all of the information from this and other NGIAB repositories is mirrored."
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 8,
    "content": "### For development\n\n- `docker/`: This folder contains the Dockerfile and entrypoint for the NGIAB container. See [Section 3.1](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_01_CONTAINERS.md) of the documentation for more information.\n\n  - Releases built from this folder are available at [https://hub.docker.com/r/awiciroh/ciroh-ngen-image](https://hub.docker.com/r/awiciroh/ciroh-ngen-image).\n- `.github/`: Workflows, issue templates, and other GitHub-focused configuration files.\n- `archive/`: Older files that are no longer maintained."
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 9,
    "content": "## Contributing\n\nInterested in contributing? Please see our [contribution guide](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/05_CONTRIBUTE.md) for more information."
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 10,
    "content": "## Contributors\n\n- Arpita Patel, Alabama Water Institute, CIROH ( [apatel54@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:apatel54@ua.edu))\n- Benjamin Lee, Alabama Water Institute, CIROH ( [blee60@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:blee60@ua.edu))\n- Zach Wills, Lynker ( [zwills@lynker.com](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:zwills@lynker.com))\n- Nels Frazier, Lynker ( [nfrazier@lynker.com](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:nfrazier@lynker.com))\n- Josh Cunningham, Alabama Water Institute, CIROH ( [jcunningham8@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:jcunningham8@ua.edu))\n- Gio Romero, Aquaveo ( [gromero@aquaveo.com](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:gromero@aquaveo.com))\n- Sam Lamont, RTI International ( [slamont@rti.org](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:slamont@rti.org))\n- Matthew Denno, RTI International ( [mdenno@rti.org](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:mdenno@rti.org))\n- James Halgren, Alabama Water Institute, CIROH ( [jshalgren@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/mailto:jshalgren@ua.edu))"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 11,
    "content": "## Sponsorship\n\n- NOAA Cooperative Institute for Research to Operations in Hydrology ( [CIROH](https://ciroh.org/))\nProject: CIROH: Community Water Model Infrastructure, Stewardship, and Integration (PI - Steven Burian)\nProject: [Advancing Community NextGen and NextGen In A Box (NGIAB) ‚Äì Paving the Pathway to Operations](https://ciroh.ua.edu/research-projects/advancing-community-nextgen-and-nextgen-in-a-box-ngiab-paving-the-pathway-to-operations/) (PI - Arpita Patel)"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 12,
    "content": "## Additional resources\n\n- [NGIAB Website](https://ngiab.ciroh.org/)\n- [NGIAB 101 Training Module](https://docs.ciroh.org/training-NGIAB-101/)\n- [NGIAB on DocuHub](https://docs.ciroh.org/)"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 13,
    "content": "### Upstream repositories\n\n- [NextGen Framework Prototype](https://github.com/NOAA-OWP/ngen)\n- [Community ngen Repository](https://github.com/CIROH-UA/ngen)\n- [Community troute Repository](https://github.com/CIROH-UA/t-route)"
  },
  {
    "idurl": 57,
    "idtype": "text",
    "order": 14,
    "content": "### NGIAB ecosystem\n\n- [NGIAB Data Preprocess](https://github.com/CIROH-UA/NGIAB_data_preprocess)\n- [NGIAB TEEHR Integration](https://github.com/CIROH-UA/ngiab-teehr)\n- [NGIAB Data Visualizer](https://github.com/CIROH-UA/ngiab-client)\n- [DataStreamCLI and Research Datastream](https://github.com/CIROH-UA/ngen-datastream/tree/main)\n- [NGIAB Calibration](https://github.com/CIROH-UA/ngiab-cal)"
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB Singularity Distribution\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/README.md](https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/README.md)."
  },
  {
    "idurl": 58,
    "idtype": "table",
    "order": 2,
    "content": "|  |  |\n| --- | --- |\n| [![alt text](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 3,
    "content": "# **NextGen In A Box (NGIAB)**\n\n**Run the NextGen National Water Resources Modeling Framework locally with ease.**\n\nNGIAB provides a containerized and user-friendly solution for running the NextGen framework, allowing you to control inputs, configurations, and execution on your local machine.\n\n[![](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/image/README/ngiab.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/image/README/ngiab.png)\n\n\n\n\n**Why NextGen In A Box?**\n\n- **Run NextGen Locally:** Experiment with the framework and customize configurations on your local machine.\n- **Control Over Inputs:** Choose specific regions or basins for analysis and modify input data as needed.\n- **Simplified Setup:** Utilize Docker containers for effortless deployment, avoiding complex software installations.\n- **Open Research Practices:** Promote transparency and reproducibility through open-source tools like Git and GitHub."
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 4,
    "content": "## Repository Information\n\n- This branch specifically for the users of Singularity container image to run simulation on NextGen Framework\n- The file structure and brife information of each file:\n\n```\n.\n‚îú‚îÄ‚îÄ guide.sh\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ singularity\n      ‚îú‚îÄ‚îÄ singularity_ngen.def\n      ‚îî‚îÄ‚îÄ templates\n          ‚îú‚îÄ‚îÄ guide\n              ‚îî‚îÄ‚îÄ HelloNGEN.sh\n\n```\n\n1. [`guilde.sh`](https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/guide.sh) : The guide script to run the simulations on the singularity image\n2. [`README.md`](https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/README.md) : Documentation of how to run the model and contribute in development on NGIAB\n3. [`singularity_ngen.def`](https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/singularity/singularity_ngen.def) : The singularity definition file to build image\n4. [`HelloNGEN.sh`](https://github.com/CIROH-UA/NGIAB-HPCInfra/blob/main/singularity/templates/guide/HelloNGEN.sh) : This is NGen execution script, which runs when the image is being executed by users."
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 5,
    "content": "## Prerequisites"
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 6,
    "content": "### 1\\. Access a compute node\n\nOn your HPC system, request an interactive session on a compute node using your scheduler (e.g., SLURM):\n\n```\nsrun --partition=<partition_name> --nodes=1 --ntasks=1 --time=<hh:mm:ss> --pty bash\n```\n\nReplace `<partition_name>` and `<hh:mm:ss>` with the appropriate partition and time limits for your HPC system."
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 7,
    "content": "### 2\\. Install SingularityCE on HPC\n\nEnsure SingularityCE is installed and validated on your HPC system. All operations, including data preparation and running the simulation, should be performed on a compute node, not the login node. Consult your system administrator or follow these steps:\n\n**i. Check Singularity Availability**:\n\nVerify that SingularityCE is installed on your HPC environment by running:\n\n```\nsingularity --version\n```\n\n**ii. Install SingularityCE (if necessary)**:\n\nRefer to the [official SingularityCE installation guide](https://docs.sylabs.io/guides/4.0/admin-guide/installation.html#installation-on-linux) for instructions tailored to Linux environments."
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 8,
    "content": "### Input Data\n\n- **Download Sample Data:** Use the provided commands to download sample data for the Sipsey Fork case study.\n- **To generate your own data:** Refer to the [NGIAB-datapreprocessor](https://github.com/AlabamaWaterInstitute/NGIAB_data_preprocess) for instructions on generating custom input data.\n- **To generate your own data and run using NGIAB:** Refer to the [ngen-datastream repository](https://github.com/CIROH-UA/ngen-datastream/tree/main) for instructions on generating custom input data.\n\nThis section guides you through downloading and preparing the sample input data for the NextGen In A Box project.\n\n**Step 1: Create Project Directory**\n\nOn the HPC system, create a directory for the project and data using the following commands:\n\n```\nmkdir -p NextGen/ngen-data\n```\n\n```\ncd NextGen/ngen-data\n```\n\n**Step 2: Download Sample Data**\n\nUse wget to download the compressed data file:\n\n**Option 1: AWI-009 input data (realization file includes - SLOTH, NoahOWP, CFE) - calibrated realization file for Provo River near Woodland, UT**\n\n```\nwget --no-parent https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\n```\n\n**Option 2: AWI-007 input data (relization file includes - SLOTH, NoahOWP, CFE)**\n\n```\nwget --no-parent https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-007/AWI_16_2863657_007.tar.gz\n```\n\n**Option 3: AWI-008 input data (realization file includes - SLOTH, Demostration LSTM)**"
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 9,
    "content": "```\nwget --no-parent https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-008/AWI_16_2863806_008.tar.gz\n```\n\n**Step 3: Extract and Rename**\n\nExtract the downloaded file and optionally rename the folder:\n\n**Option 1**\n\n```\ntar -xf AWI_16_10154200_009.tar.gz\n```\n\n**Option 2:**\n\n```\ntar -xf AWI_16_2863657_007.tar.gz\n```\n\n**Option 3:**\n\n```\ntar -xf AWI_16_2863806_008.tar.gz\n```\n\nNow you have successfully downloaded and prepared the sample input data in the NextGen/ngen-data directory.\n\n**Step 4: Clone and Run**\nNavigate to the NextGen directory, clone the repository, and execute the guide script:\n\n```\ncd ../../NextGen\n```\n\n```\ngit clone https://github.com/CIROH-UA/NGIAB-HPCInfra.git\n```\n\n```\ncd NGIAB-HPCInfra\n```\n\n```\n./guide.sh\n```"
  },
  {
    "idurl": 58,
    "idtype": "text",
    "order": 10,
    "content": "## Run NextGen In A Box\n\nTo run NextGen framework, hydrologist only have to execute the [guide script](https://github.com/CIROH-UA/Ngen-Singularity/blob/main/guide.sh) to run simulations on self-contained NextGen framework container image.\n\n- The guide script feature:\n  - Determine architecher of the underling system (ARM or x86)\n  - Automaticlly download latest Singularity NextGen image from Docker Hub\n  - Allow to attach input data by providing relative path of it\n  - The options of running image:\n    1. Run simulation in **Serial** mode\n    2. Run simulation in **Parallel** mode\n    3. Run image in **Interactive shell** mode"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen on 2i2c JupyterHub\n\nNew CIROH JupyterHub image named \"NextGen National Water Model (NWM)\" has both NGIAB Data Preprocess and NextGen packages installed.\n\n> You can access 2i2c JupyterHub here: [https://ciroh.awi.2i2c.cloud/hub/login](https://ciroh.awi.2i2c.cloud/hub/login).\n\n![JupyterHub](https://docs.ciroh.org/assets/images/jupyterhub-464abfe07a594a5289edeb4c59857754.png)"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 2,
    "content": "## Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial [‚Äã](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/\\#working-with-hydroshare-aorc-data-hydrofabric-and-nextgen-on-ciroh-jupyterhub-tutorial \"Direct link to Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial\")\n\nThis HydroShare resource provides a tutorial on the use of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare repository and linked CIROH JupyterHub computing platform on 2i2c in support of CIROH collaborative research and NextGen modeling. It introduces use of Jupyter Notebooks for retrieval of NOAA Analysis of Record for Calibration (AORC) datasets and setting up and executing NextGen for a small test watershed as a starting point for research with NextGen.\n\nYou can find the resource here: [https://www.hydroshare.org/resource/fc8539358fe64ca6a47468728a0687a1/](https://www.hydroshare.org/resource/fc8539358fe64ca6a47468728a0687a1/)\n\nTo open the resource in CIROH JupyterHub, click the \"Open with...\" button on the HydroShare page and select \"CIROH JupyterHub\":\n\n![Opening with CIROH JupyterHub](https://docs.ciroh.org/assets/images/resource-43b2106555301063f9d29113976d7d36.png)"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 3,
    "content": "## Command Line Examples [‚Äã](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/\\#command-line-examples \"Direct link to Command Line Examples\")\n\nHere are some command-line examples related to working with the data:\n\n```codeBlockLines_e6Vv"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 4,
    "content": "# Virtual environment\nsource /ngen/.venv/bin/activate\npython -m ngiab_data_cli -i \"gage-10109001\" -s"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 5,
    "content": "# Hydrofabric\npython -m ngiab_data_cli -i \"cat-2861446\" -s"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 6,
    "content": "# Forcing\npython -m ngiab_data_cli -i \"cat-2861446\" -f --start \"2021-10-01\" --end \"2022-09-30\""
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 7,
    "content": "# Configuration\npython -m ngiab_data_cli -i \"cat-2861446\" -r --start \"2021-10-01\" --end \"2022-09-30\""
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 8,
    "content": "# Run\n/dmod/bin/ngen-serial config/cat-2861446_subset.gpkg all config/cat-2861446_subset.gpkg all config/realization.json\n\n```\n\n* * *"
  },
  {
    "idurl": 59,
    "idtype": "text",
    "order": 9,
    "content": "## Visualizations [‚Äã](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/\\#visualizations \"Direct link to Visualizations\")\n\nShown below is a sample visualization based on the above command-line example:\n\n![Graph](https://docs.ciroh.org/assets/images/graph-618fe46ba8f67fb53c6fd3644b167381.png)\n\n- [Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/#working-with-hydroshare-aorc-data-hydrofabric-and-nextgen-on-ciroh-jupyterhub-tutorial)\n- [Command Line Examples](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/#command-line-examples)\n- [Visualizations](https://docs.ciroh.org/docs/products/ngiab/distributions/nextgen-2i2c/#visualizations)"
  },
  {
    "idurl": 60,
    "idtype": "table",
    "order": 1,
    "content": "|  |  |\n| --- | --- |\n| [![CIROH Logo](https://github.com/CIROH-UA/NGIAB_data_preprocess/raw/main/ciroh-bgsafe.png)](https://github.com/CIROH-UA/NGIAB_data_preprocess/blob/main/ciroh-bgsafe.png) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 2,
    "content": "# NGIAB Data Preprocessor\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB\\_data\\_preprocess/blob/main/README.md](https://github.com/CIROH-UA/NGIAB_data_preprocess/blob/main/README.md).\n\nThis repository contains tools for preparing data to run a [NextGen](https://github.com/NOAA-OWP/ngen)-based simulation using [NGIAB](https://github.com/CIROH-UA/NGIAB-CloudInfra). The tools allow you to select a catchment of interest on an interactive map, choose a date range, and prepare the data with just a few clicks!\n\n[![map screenshot](https://github.com/CIROH-UA/NGIAB_data_preprocess/raw/main/modules/map_app/static/resources/screenshot.jpg)](https://github.com/CIROH-UA/NGIAB_data_preprocess/blob/main/modules/map_app/static/resources/screenshot.jpg)"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 3,
    "content": "## Table of Contents\n\n1. What does this tool do?\n2. Limitations\n   - Custom realizations\n   - Calibration\n   - Evaluation\n   - Visualisation\n3. Requirements\n4. Installation and running\n   - Running without install\n   - For uv installation\n   - For legacy pip installation\n   - Development installation\n5. Map interface documentation\n   - Running the map interface app\n   - Using the map interace\n6. CLI documentation\n   - Running the CLI\n   - Arguments\n   - Usage notes\n   - Examples\n7. Realization information\n   - NOAH + CFE"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 4,
    "content": "## What does this tool do?\n\nThis tool prepares data to run a NextGen-based simulation by creating a run package that can be used with NGIAB.\n\nIt uses geometry and model attributes from the [v2.2 hydrofabric](https://lynker-spatial.s3-us-west-2.amazonaws.com/hydrofabric/v2.2/conus/conus_nextgen.gpkg) more information on [all data sources here](https://lynker-spatial.s3-us-west-2.amazonaws.com/hydrofabric/v2.2/hfv2.2-data_model.html).\n\nThe raw forcing data is [nwm retrospective v3 forcing](https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/index.html#CONUS/zarr/forcing/) data or the [AORC 1km gridded data](https://noaa-nws-aorc-v1-1-1km.s3.amazonaws.com/index.html) depending on user input\n\n1. **Subsets** (delineates) everything upstream of your point of interest (catchment, gage, flowpath etc) from the hydrofabric. This subset is output as a geopackage (.gpkg).\n2. Calculates **forcings** as a weighted mean of the gridded NWM or AORC forcings. Weights are calculated using [exact extract](https://isciences.github.io/exactextract/) and computed with numpy.\n3. Creates **configuration files** for a default NGIAB model run.\n\n   - realization.json - ngen model configuration\n   - troute.yaml - routing configuration.\n   - **per catchment** model configuration\n4. Optionally performs a non-interactive [Docker-based NGIAB](https://github.com/CIROH-UA/NGIAB-CloudInfra) run."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 5,
    "content": "## Limitations\n\nThis tool cannot do the following:"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 6,
    "content": "### Custom realizations\n\nThis tool currently only outputs a single, default realization, which is described in \"Realization information\". Support for additional model configurations is planned, but not currently available."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 7,
    "content": "### Calibration\n\nIf available, this repository will download [calibrated parameters](https://communityhydrofabric.s3.us-east-1.amazonaws.com/index.html#hydrofabrics/community/gage_parameters/) from the [Community Hydrofabric](https://github.com/CIROH-UA/community_hf_patcher) AWS S3 bucket.\nHowever, many gages and catchments will not have such parameters available. In these cases, Data Preprocess will output realizations with default values.\n\nFor automatic calibration, please see [ngiab-cal](https://github.com/CIROH-UA/ngiab-cal), which is under active development."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 8,
    "content": "### Evaluation\n\nFor automatic evaluation using [TEEHR](https://github.com/RTIInternational/teehr), please run [NGIAB](https://github.com/CIROH-UA/NGIAB-CloudInfra) interactively using the `guide.sh` script."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 9,
    "content": "### Visualisation\n\nFor automatic interactive visualisation, please run [NGIAB](https://github.com/CIROH-UA/NGIAB-CloudInfra) interactively using the `guide.sh` script"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 10,
    "content": "# Requirements\n\nThis tool is **officially supported** on **macOS** and **Ubuntu** (tested on 22.04 & 24.04). To use it on Windows, please install [**WSL**](https://learn.microsoft.com/en-us/windows/wsl/install).\n\nIt is also **highly recommended** to use [Astral UV](https://docs.astral.sh/uv/) to install and run this tool. Installing the project via `pip` without the use of a virtual environment creates a **severe risk** of dependency conflicts."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 11,
    "content": "# Installation and running"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 12,
    "content": "### Running without install\n\nThis package supports pipx and uvx, which means you can run the tool without installing it. No virtual environment needed, just UV.\n\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 13,
    "content": "# Run these from anywhere!\nuvx --from ngiab-data-preprocess cli --help  # Running the CLI\nuvx ngiab-prep --help                        # Alias for the CLI\nuvx --from ngiab-data-preprocess map_app     # Running the map interface\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 14,
    "content": "### For uv installation\n\nClick here to expand\n\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 15,
    "content": "# Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 16,
    "content": "# It can be installed via pip if that fails"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 17,
    "content": "# pip install uv"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 18,
    "content": "# Create a virtual environment in the current directory\nuv venv"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 19,
    "content": "# Install the tool in the virtual environment\nuv pip install ngiab_data_preprocess"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 20,
    "content": "# To run the cli\nuv run cli --help"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 21,
    "content": "# To run the map\nuv run map_app\n```\n\nUV automatically detects any virtual environments in the current directory and will use them when you use `uv run`."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 22,
    "content": "### For legacy pip installation\n\nClick here to expand\n\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 23,
    "content": "# If you're installing this on jupyterhub / 2i2c you HAVE TO DEACTIVATE THE CONDA ENV\n(notebook) jovyan@jupyter-user:~$ conda deactivate\njovyan@jupyter-user:~$"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 24,
    "content": "# The interactive map won't work on 2i2c\n```\n\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 25,
    "content": "# This tool is likely to not work without a virtual environment\npython3 -m venv .venv\nsource .venv/bin/activate"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 26,
    "content": "# installing and running the tool\npip install 'ngiab_data_preprocess'\npython -m map_app"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 27,
    "content": "# CLI instructions at the bottom of the README\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 28,
    "content": "### Development installation\n\nClick to expand installation steps\n\nTo install and run the tool, follow these steps:\n\n1. Clone the repository:\n\n```\ngit clone https://github.com/CIROH-UA/NGIAB_data_preprocess\ncd NGIAB_data_preprocess\n```\n\n2. Create a virtual environment:\n\n```\nuv venv\n```\n\n3. Install the tool:\n\n```\nuv pip install -e .\n```\n\n4. Run the map app:\n\n```\nuv run map_app\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 29,
    "content": "# Map interface documentation"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 30,
    "content": "## Running the map interface app\n\nRunning the `map_app` tool will open the app in a new browser tab.\n\nInstall-free: `uvx --from ngiab-data-preprocess map_app`\n\nInstalled with uv: `uv run map_app`"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 31,
    "content": "## Using the map interface\n\n1. Select the catchment you're interested in on the map.\n2. Pick the time period you want to simulate.\n3. Click the following buttons in order:\n1. Create subset gpkg\n2. Create Forcing from Zarrs\n3. Create Realization\n\nOnce all the steps are finished, you can run NGIAB on the folder shown underneath the subset button.\n\n**Note:** When using the tool, the default output will be stored in the `~/ngiab_preprocess_output/<your-input-feature>/` folder. There is no overwrite protection on the folders."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 32,
    "content": "# CLI documentation"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 33,
    "content": "## Running the CLI\n\nInstall-free: `uvx ngiab-prep`\n\nInstalled with uv: `uv run cli`"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 34,
    "content": "## Arguments - `-h`, `--help`: Show the help message and exit. - `-i INPUT_FEATURE`, `--input_feature INPUT_FEATURE`: ID of feature to subset. Providing a prefix will automatically convert to catid, e.g., cat-5173 or gage-01646500 or wb-1234. - `--vpu VPU_ID` : The id of the vpu to subset e.g 01. 10 = 10L + 10U and 03 = 03N + 03S + 03W. `--help` will display all the options. - `-l`, `--latlon`: Use latitude and longitude instead of catid. Expects comma-separated values via the CLI, e.g., `python -m ngiab_data_cli -i 54.33,-69.4 -l -s`. - `-g`, `--gage`: Use gage ID instead of catid. Expects a single gage ID via the CLI, e.g., `python -m ngiab_data_cli -i 01646500 -g -s`. - `-s`, `--subset`: Subset the hydrofabric to the given feature. - `-f`, `--forcings`: Generate forcings for the given feature. - `-r`, `--realization`: Create a realization for the given feature. - `--start_date START_DATE`, `--start START_DATE`: Start date for forcings/realization (format YYYY-MM-DD). - `--end_date END_DATE`, `--end END_DATE`: End date for forcings/realization (format YYYY-MM-DD). - `-o OUTPUT_NAME`, `--output_name OUTPUT_NAME`: Name of the output folder. - `--source` : The datasource you want to use, either `nwm` for retrospective v3 or `aorc`. Default is `nwm`\n- `-D`, `--debug`: Enable debug logging. - `--run`: Automatically run [NGIAB's docker distribution](https://github.com/CIROH-UA/NGIAB-CloudInfra) against the output folder."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 35,
    "content": "- `--validate`: Run every missing step required to run NGIAB. - `-a`, `--all`: Run all operations. Equivalent to `-sfr` and `--run`."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 36,
    "content": "## Usage notes\n\n- If your input has a prefix of `gage-`, you do not need to pass `-g`.\n- The `-l`, `-g`, `-s`, `-f`, `-r` flags can be combined like normal CLI flags. For example, to subset, generate forcings, and create a realization, you can use `-sfr` or `-s -f -r`.\n- When using the `--all` flag, it automatically sets `subset`, `forcings`, `realization`, and `run` to `True`.\n- Using the `--run` flag automatically sets the `--validate` flag."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 37,
    "content": "## Examples\n\n1. Prepare everything for an NGIAB run at a given gage:\n\n```\nuvx ngiab-prep -i gage-10154200 -sfr --start 2022-01-01 --end 2022-02-28"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 38,
    "content": "#         add --run or replace -sfr with --all to run NGIAB, too"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 39,
    "content": "# to name the folder, add -o folder_name\n```\n\n2. Subset the hydrofabric using a catchment ID or VPU:\n\n```\nuvx ngiab-prep -i cat-7080 -s\nuvx ngiab-prep --vpu 01 -s\n```\n\n3. Generate forcings using a single catchment ID:\n\n```\nuvx ngiab-prep -i cat-5173 -f --start 2022-01-01 --end 2022-02-28\n```\n\n4. Create realization using a latitude/longitude pair and output to a named folder:\n\n```\nuvx ngiab-prep -i 33.22,-87.54 -l -r --start 2022-01-01 --end 2022-02-28 -o custom_output\n```\n\n5. Perform all operations using a latitude/longitude pair:\n\n```\nuvx ngiab-prep -i 33.22,-87.54 -l -s -f -r --start 2022-01-01 --end 2022-02-28\n```\n\n6. Subset the hydrofabric using a gage ID:\n\n```\nuvx ngiab-prep -i 10154200 -g -s"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 40,
    "content": "# or\nuvx ngiab-prep -i gage-10154200 -s\n```\n\n7. Generate forcings using a single gage ID:\n\n```\nuvx ngiab-prep -i 01646500 -g -f --start 2022-01-01 --end 2022-02-28\n```"
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 41,
    "content": "# Realization information\n\nThis tool currently offers one default realization."
  },
  {
    "idurl": 60,
    "idtype": "text",
    "order": 42,
    "content": "## NOAH + CFE\n\n[This realization](https://github.com/CIROH-UA/NGIAB_data_preprocess/blob/main/modules/data_sources/cfe-nowpm-realization-template.json) is intended to be roughly comparable to earlier versions of the National Water Model.\n\n- [NOAH-OWP-Modular](https://github.com/NOAA-OWP/NOAH-OWP-Modular): A refactoring of Noah-MP, a land-surface model. Used to model groundwater properties.\n- [Conceptual Functional Equivalent (CFE)](https://github.com/NOAA-OWP/CFE): A simplified conceptual approximation of versions 1.2, 2.0, and 2.1 of the National Water Model. Used to model precipitation and evaporation.\n- [SLoTH](https://github.com/NOAA-OWP/SLoTH): A module used to feed through unchanged values. In this default configuration, it simply forces certain soil moisture and ice fraction properties to zero."
  },
  {
    "idurl": 61,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB TEEHR Integration\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngiab-teehr/blob/main/README.md](https://github.com/CIROH-UA/ngiab-teehr/blob/main/README.md).\n\nA repository for coupling [TEEHR](https://rtiinternational.github.io/teehr/) with [Nextgen-In-A-Box (NGIAB)](https://github.com/jameshalgren/NGIAB-CloudInfra) simulation output.\n\nWarning: This code is experimental!\n\nThe `example_guide.sh` script demonstrates running a TEEHR evaluation (see `scripts/teehr_ngen.py`) on NGIAB output."
  },
  {
    "idurl": 61,
    "idtype": "table",
    "order": 2,
    "content": "|  |  |\n| --- | --- |\n| [![alt text](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 61,
    "idtype": "text",
    "order": 3,
    "content": "### To build and push the TEEHR image to the AWI CIROH registry\n\nCustomize the metrics calculated by TEEHR or any other code related to the workflow:\n\n1. Create a branch off of main\n2. Make your edits to `scripts/teehr_ngen.py` and/or `scripts/utils.py`\n3. Update the `Changelog` so that your changes can be associated with a tag\n4. Submit and PR and merge\n5. Then checkout main and pull the new changes, and push your tag:\n\n```\ngit checkout main\ngit pull\ngit tag -a v0.x.x -m \"version 0.x.x\"\ngit push origin v0.x.x\n```\n\nThis will trigger a `github action` to build and push the image with your tag, and the `latest` tag, to the AWI CIROH registry.\n\nTo build and push locally:\n\n```\ndocker build -t awiciroh/ngiab-teehr:<tag name> .\ndocker push awiciroh/ngiab-teehr:<tag name>\n\n```\n\nNow you can specify the image tag in the guide.sh script."
  },
  {
    "idurl": 62,
    "idtype": "table",
    "order": 1,
    "content": "|  |  |\n| --- | --- |\n| [![CIROH Logo](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67)](https://camo.githubusercontent.com/f1425ea9a6a4492162f5ff3a63a5d0827fc95442381ae96e095095c7afd57152/68747470733a2f2f6369726f682e75612e6564752f77702d636f6e74656e742f75706c6f6164732f323032322f30382f4349524f484c6f676f5f323030783230302e706e67) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 2,
    "content": "# NGIAB Data Visualizer\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngiab-client/blob/main/README.md](https://github.com/CIROH-UA/ngiab-client/blob/main/README.md).\n\n\n\n\nThis app was created using an experimental Tethys + React app scaffold. It uses React for the frontend of the app and Tethys as the backend.\n\n[![Data Visualizer Interface](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-1.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-1.png)\n\nThe Data Visualizer component provides:\n\n- **Geospatial visualization** of catchments and nexus points\n- **Time series analysis** of catchments, nexus points, and troute variables\n- **TEEHR output visualization** including metrics and interactive plots\n\nBuilt on the Tethys Platform [(Swain et al., 2015)](https://doi.org/10.1016/j.envsoft.2015.01.014), it enables web-based exploration of model outputs [(CIROH, 2025)](https://github.com/CIROH-UA/ngiab-client)."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 3,
    "content": "## Usage Guide"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 4,
    "content": "### Assited using `ViewOnTethys` Script\n\nLike TEEHR, the Data Visualizer can be activated upon execution of the main NGIAB guide script, `guide.sh`. A separate `viewOnTethys.sh` script is also available in the NGIAB-CloudInfra repository.\n\nOnce a run is complete, users can launch the Data Visualizer through their web browser when prompted by the guide script. Although TEEHR's outputs can be displayed within the Data Visualizer, this tool is primarily designed to provide a broad overview of model results. Users seeking TEEHR's more advanced analysis features can still access them outside the Data Visualizer.\n\nOne of the advantages of the `viewOnTethys.sh` script is that it allows the user to keep multiple outputs for the same hydrofabric. It prompts the user if they want to use the same output directory by renaming it and adding it to the collection of outputs or if they want to overwrite it.\n\n```\n  ‚ö† ~/ngiab_visualizer is not empty.\n  ‚Üí Keep (K) or Fresh start (F)? [K/F]: k\n‚Ñπ Reclaiming ownership of ~/ngiab_visualizer  (sudo may prompt)‚Ä¶\n  ‚ö† Directory exists: ~/ngiab_visualizer/gage-10154200\n  ‚Üí Overwrite (O) or Duplicate (D)? [O/D]: o\n  ‚úì Overwritten ‚ûú ~/ngiab_visualizer/gage-10154200\nChecking for ~/ngiab_visualizer/ngiab_visualizer.json...\n```\n\nYou should be able to see multiple outputs through the UI:"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 5,
    "content": "[![Figure 2: NGIAB Visualizer dropdown for multiple outputs ](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-2.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-2.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The map displays the ability of the visualizer to use multiple outputs'}"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 6,
    "content": "#### Visualizer Directory Organization\n\nThe Visualizer organizes data using a directory named `ngiab_visualizer`. This directory stores all the outputs generated by the user and includes a file called `ngiab_visualizer.json`. This file contains metadata that allows the Visualizer to locate and manage the outputs from different runs executed via the `./guide.sh` script. The metadata, specific to the Visualizer, is structured as a simple JSON file. It lists the outputs with details such as a label, the data path, and a unique identifier. The `./ViewOnTethys.sh` script handles the creation and management of this metadata.\n\n```\n{\n  \"model_runs\": [\\\n    {\\\n      \"label\": \"gage-10154200\",\\\n      \"path\": \"/var/lib/tethys_persist/ngiab_preprocess_output/gage-10154200\",\\\n      \"date\": \"2025-05-23:13:51:51\",\\\n      \"id\": \"61026834-4235-4d39-8a8e-f076a8854148\",\\\n      \"subset\": \"\",\\\n      \"tags\": []\\\n    },\\\n    {\\\n      \"label\": \"gage-20454200\",\\\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer/gage-20454200\",\\\n      \"date\": \"2025-05-23:17:00:34\",\\\n      \"id\": \"68f6cf78-188c-4e86-b797-6c40ea36e0e6\",\\\n      \"subset\": \"\",\\\n      \"tags\": []\\\n    },\\\n    {\\\n      \"label\": \"gage-35054600\",\\\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer/gage-35054600\",\\\n      \"date\": \"2025-05-23:17:01:10\",\\\n      \"id\": \"6d0cb736-2dac-4ea0-a3a3-ad26cd45ef36\",\\\n      \"subset\": \"\",\\\n      \"tags\": []\\\n    }\\\n  ]\n}\n\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 7,
    "content": "The path `/var/lib/tethys_persist/` belongs to the `$HOME` env variable of the container running the visualizer. When the user runs the `./ViewOnTethys.sh`, it mounts the directory from the host at `~/ngiab_visualizer` to `/var/lib/tethys_persist/ngiab_visualizer`.\n\nHowever, if the user wants more control, the user can copy their data directory to `~/ngiab_visualizer` on the host(not the container) while the container is **stop**,\n\n```\nchown -R $USER: ~/ngiab_visualizer\ncp -R your/data/path ~/ngiab_visualizer\n```\n\nFinally the user can open the `~/ngiab_visualizer/ngiab_visualizer.json` on the host(not the container), and add the specific run to the visualizer:\n\n```\n....\n    {\n      \"label\": \"gage-35054600\",\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer<MY_SPECIFIC_OUTPUT_DIRECTORY_NAME>\",\n      \"date\": \"2025-05-23:17:01:10\",\n      \"id\": \"6d0cb736-2dac-4ea0-a3a3-ad26cd45ef36\",\n      \"subset\": \"\",\n      \"tags\": []\n    }\n```\n\nThe user can then run `./ViewOnTethys.sh` script to spin again the container or if the user wants more control and just define the env variables and running the container"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 8,
    "content": "### Unassisted Usage\n\nFirst create the `MODELS_RUNS_DIRECTORY` directory at `\"$HOME/ngiab_visualizer\"`, and `DATASTREAM_DIRECTORY` directory at `\"$HOME/.datastream_ngiab\"`, and `VISUALIZER_CONF` file at `\"$MODELS_RUNS_DIRECTORY/ngiab_visualizer.json\"`\n\nCopy your `my-ngen-output` into the `MODELS_RUNS_DIRECTORY` directory, and then edit the `\"$MODELS_RUNS_DIRECTORY/ngiab_visualizer.json\"` fie and add the your output as below.\n\n```\n{\n    \"model_runs\": [\\\n        ...\\\n        {\\\n            \"label\": \"my-ngen-output\",\\\n            \"path\": \"/var/lib/tethys_persist/my-ngen-output\",\\\n            \"date\": \"2025-05-23:13:51:51\",\\\n            \"id\": \"61026834-4235-4d39-8a8e-f076a8854148\", #it can be any ID\\\n            \"subset\": \"\",\\\n            \"tags\": []\\\n        },\\\n        ...\\\n    ]\n}\n```\n\nDefine the env variables and running the container\n\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 9,
    "content": "# Set environment variables\nexport TETHYS_CONTAINER_NAME=\"tethys-ngen-portal\"        \\\n       TETHYS_REPO=\"awiciroh/tethys-ngiab\"               \\\n       TETHYS_TAG=\"latest\"                               \\\n       NGINX_PORT=80                                     \\\n       MODELS_RUNS_DIRECTORY=\"$HOME/ngiab_visualizer\"    \\\n       DATASTREAM_DIRECTORY=\"$HOME/.datastream_ngiab\"    \\\n       VISUALIZER_CONF=\"$MODELS_RUNS_DIRECTORY/ngiab_visualizer.json\" \\\n       TETHYS_PERSIST_PATH=\"/var/lib/tethys_persist\"     \\\n       SKIP_DB_SETUP=false                               \\\n       CSRF_TRUSTED_ORIGINS=\"[\\\"http://localhost:${NGINX_PORT}\\\",\\\"http://127.0.0.1:${NGINX_PORT}\\\"]\"\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 10,
    "content": "# Run container\n\n```\ndocker run --rm -d \\\n  -v \"$MODELS_RUNS_DIRECTORY:$TETHYS_PERSIST_PATH/ngiab_visualizer\" \\\n  -v \"$DATASTREAM_DIRECTORY:$TETHYS_PERSIST_PATH/.datastream_ngiab\" \\\n  -p \"$NGINX_PORT:$NGINX_PORT\" \\\n  --name \"$TETHYS_CONTAINER_NAME\" \\\n  -e MEDIA_ROOT=\"$TETHYS_PERSIST_PATH/media\" \\\n  -e MEDIA_URL=\"/media/\" \\\n  -e SKIP_DB_SETUP=\"$SKIP_DB_SETUP\" \\\n  -e DATASTREAM_CONF=\"$TETHYS_PERSIST_PATH/.datastream_ngiab\" \\\n  -e VISUALIZER_CONF=\"$TETHYS_PERSIST_PATH/ngiab_visualizer/ngiab_visualizer.json\" \\\n  -e NGINX_PORT=\"$NGINX_PORT\" \\\n  -e CSRF_TRUSTED_ORIGINS=\"$CSRF_TRUSTED_ORIGINS\" \\\n  \"${TETHYS_REPO}:${TETHYS_TAG}\"\n```\n\nVerify deployment:\n\n```\ndocker ps"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 11,
    "content": "# CONTAINER ID   IMAGE                          PORTS                 NAMES"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 12,
    "content": "# b1818a03de9b   awiciroh/tethys-ngiab:latest   0.0.0.0:80->80/tcp    tethys-ngen-portal\n```\n\nAccess at: [http://localhost:80](http://localhost/)"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 13,
    "content": "### Visualization Features\n\n**Nexus** points can be visualized when the user selects the output that wants to visualize. Time series can be retrieved by clicking on any of the **Nexus** points, or by changing the select dropdown assigned to the Nexus.\n\n[![Figure 3: NGIAB Visualizer time series visualization from Nexus points](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-3.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-3.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The map displays the ability of the visualizer to retrieve time series from Nexus points'}\n\n**Troute** variables time series can also be displayed using the **Troute** select dropdown.\n\n[![Figure 4: NGIAB Visualizer time series visualization from Troute variables](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-4.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-4.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The map displays the ability of the visualizer to retrieve time series from Troute variables'}\n\n**Catchments** time series can be retrieved by clicking on any of the **Catchments** polygons, or by changing the select dropdown assigned to the Catchments."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 14,
    "content": "[![Figure 5: NGIAB Visualizer time series visualization for Catchments](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-5.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-5.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The map displays the ability of the visualizer to retrieve time series from Catchments variables'}\n\n**TEEHR** evaluation can be visualized when the user hits a point that contains **TEEHR** evaluation output, the user can also look at a **Nexus** point on the dropdown assigned and enter the id of the **Nexus** points that contains **TEEHR** evaluation output."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 15,
    "content": "[![Figure 6: A map showing the geospatial visualization using the Data Visualizer within the Tethys framework for a selected outlet nexus point as well as displaying a time series plot between observed (labeled \"USGS\"; blue line) and simulated (labeled \"ngen\"; orange line)](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-6.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-6.png){alt='alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The left panel contains a \"Time Series Menu\" where the user can select a Nexus ID, variable (e.g., flow), and TEEHR data source. A map in the center displays a stream reach with a highlighted section representing the drainage basin and a blue point, indicating the selected nexus location. Below the map, a time series plot compares USGS (blue line) and Ngen (orange line) streamflow data from 2017 to 2023.'}\n\nSimilarly, a **TEEHR** evaluation metric can be visualized by going to the metrics tab"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 16,
    "content": "[Figure 7: NGIAB Visualizer performance metrics (KGE, NSE, and relative bias). The Visualizer can also show the performance of the NWM 3.0 compared to the observed time series.](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-7.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface. The map displays the ability of the visualizer to retrieve the TEEHR metrics on a table.\"Teehr Metrics\" presents performance metrics (e.g., Kling-Gupta Efficiency, Nash-Sutcliffe Efficiency, and Relative Bias) for the selected model versus reference data.'}"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 17,
    "content": "### DataStream Integration\n\nThe Visualizer also allows the user to download data as well from an [S3 bucket](https://datastream.ciroh.org/index.html) containing the output of the [NextGen DataStream](https://github.com/CIROH-UA/ngen-datastream). The `ViewOnTethys.sh` script will create a `~/.datastream_ngiab` directory in which it saves all the different outputs downloaded by the visualizer. It will also create a `~/.datastream_ngiab/datastream_ngiab.json` in which metadata will be saved to locate the downloaded output directories. It serves as a cache, so it allows the user to look first at the `~/.datastream_ngiab` before trying to download the data\n\n```\n‚Ñπ Reclaiming ownership of /home/aquagio/.datastream_ngiab  (sudo may prompt)‚Ä¶\n  ‚Ñπ No existing Datastream cache found ‚Äì a fresh download will be used.\n```\n\nThe `.datastream_ngiab.json` appends the different downloads with metadata that allows the user to know the file being downloaded. The `prefix` belongs to the path on the s3 bucket. The `label` is created with the following format: `ngen.<date>_<forecast_type>_<cycle>_<VPU>`"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 18,
    "content": "```\n{\n    \"datastream\": [\\\n        {\\\n            \"label\": \"ngen.20250522_medium_range_06_VPU_02\",\\\n            \"bucket\": \"ciroh-community-ngen-datastream\",\\\n            \"prefix\": \"v2.2/ngen.20250522/medium_range/06/VPU_02/ngen-run.tar.gz\",\\\n            \"path\": \"/var/lib/tethys_persist/.datastream_ngiab/ngen.20250522_medium_range_06_VPU_02\",\\\n            \"date\": \"2021-01-01:00:00:00\",\\\n            \"id\": \"15145d327f19426b890e4465160f963a\"\\\n        }\\\n    ]\n}\n```\n\n> **_NOTE:_** assuming only the first ensemble. If we are specific it will look like this: `ngen.<date>_<forecast_type>_<cycle>_<ensemble>_<VPU>`\n\nThis functionality allows the user to be able to quicklu search the data they want from the [S3 bucket](https://datastream.ciroh.org/index.html) containing the output of the [NextGen DataStream](https://github.com/CIROH-UA/ngen-datastream). They can explore and download as needed.\n\n[![Figure 8: NGIAB Visualizer Visualization of DataStream Data](https://github.com/CIROH-UA/ngiab-client/raw/main/static/imgs/fig6-8.png)](https://github.com/CIROH-UA/ngiab-client/blob/main/static/imgs/fig6-8.png){alt='A screenshot of the NGIAB and DataStream Visualizer web interface displaying the hydrofabric for DataStream output'}"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 19,
    "content": "## Development Installation\n\nYou need to install both the Tethys dependencies and the node dependencies.\n\nThe webpack dev server is configured to proxy the Tethys development server (see `webpack.config.js`). The app endpoint will be handled by the webpack development server and all other endpoints will be handled by the Tethys (Django) development server. As such, you will need to start both in separate terminals.\n\n0. First create a Virtual Environment with the tool of your choice and then run the following commands\n\n1. Install libmamba and make it your default solver (see: A Faster Solver for Conda: Libmamba):\n\n```\nconda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n\n```\n\n2. Install the Tethys Platform\n\nUsing `conda`\n\n```\nconda install -c conda-forge tethys-platform django=<DJANGO_VESION>\n\n```\n\nor using `pip`\n\n```\npip install tethys-platform django=<DJANGO_VERSION>\n\n```\n\n3. Create a `portal_config.yml` file :\n\nTo add custom configurations such as the database and other local settings you will need to generate a portal\\_config.yml file. To generate a new template portal\\_config.yml run:\n\n```\ntethys gen portal_config\n```\n\nYou can customize your settings in the portal\\_config.yml file after you generate it by manually editing the file or by using the settings command command. Refer to the Tethys Portal Configuration documentation for more information.\n\n4. Configure the Tethys Database"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 20,
    "content": "There are several options for setting up a DB server: local, docker, or remote. Tethys Platform uses a local SQLite database by default. For development environments you can use Tethys to create a local server:\n\n```\ntethys db configure\n```\n\n5. Install Node Version Manager and Node.js:\n\n5.1 Install Node Version Manager (nvm): [https://github.com/nvm-sh/nvm?tab=readme-ov-file#install--update-script](https://github.com/nvm-sh/nvm?tab=readme-ov-file#install--update-script)\n\n5.2 CLOSE ALL OF YOUR TERMINALS AND OPEN NEW ONES\n\n5.3 Use NVM to install Node.js 20:\n\n```\nnvm install 20\nnvm use 20\n```\n\n6. Install the PDM dependency manager:\n\n```\npip install --user pdm\n```\n\n> **_NOTE:_** if you have previously installed pdm in another environment, uninstall pdm first ( `pip uninstall pdm`), and then reinstall as shown above with the new environment active.\n\n7. Clone the app and install into the Tethys environment in development mode:\n\n```\ngit clone https://github.com/CIROH-UA/ngiab-client.git\ncd ngiab-client\npdm install\nnpm install --include=dev\ncd ../\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 21,
    "content": "## PDM Tips\n\nSee below for more PDM tips like how to manage dependencies, install dependencies, and run scripts."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 22,
    "content": "### Install only dev dependencies\n\n- Install all dev dependencies (test & lint)\n\n```\npdm install -G:all\n```\n\n- Install only test dependencies\n\n```\npdm install -G test\n```\n\n- Install only lint and formatter dependencies\n\n```\npdm install -G lint\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 23,
    "content": "### Managing dependencies\n\n- Add a new dependency:\n\n1. Add the package using `pdm`:\n\n```\npdm add <package-name>\n```\n\n2. Manually add the dependency to the `install.yml`.\n\n> **_IMPORTANT:_** Dependencies are not automatically added to the `install.yml` yet!\n\n- Add a new dev dependency:\n\n```\npdm add -dG test <package-name>\npdm add -dG lint <package-name>\n```\n\n> **_NOTE:_** Just use `pdm` to install and manage dev dependencies. The `install.yml` does not support dev dependencies, but they shouldn't be needed in it anyway, right?\n\n- Add a new optional dependeny:\n\n```\npdm add -G <group-name> <package-name>\n```\n\n> **_NOTE:_** You'll need to decide whether or not to add the optional dependencies to the `install.yml` b/c it does not support optional dependencies. You may consider using `pdm` to manage the optional dependencies.\n\n- Remove a dependency:\n\n1. Remove it from the `pyproject.yaml` and lock file:\n\n```\npdm remove --no-sync <package-name>\n```\n\n2. Manually remove it from the `install.yml`\n\n3. If you want to remove it from the environment, use `pip` or `conda` to remove the package."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 24,
    "content": "> **_IMPORTANT:_** TL;DR: Running `pdm remove` without the `--no-sync` will remove nearly all of the dependencies in your environment. While `pdm remove` is capable of removing the package from the environment, running `pdm remove` without the `--no-sync` option can break your Tethys environment. This is because `pdm` will attempt to get the environment to match the dependencies listed in your `pyproject.toml`, which usually does not include all of the dependencies of Tethys."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 25,
    "content": "### PDM Scripts\n\nThe project is configured with several PDM convenience scripts:\n\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 26,
    "content": "# Run linter\npdm run lint"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 27,
    "content": "# Run formatter\npdm run format"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 28,
    "content": "# Run tests\npdm run test"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 29,
    "content": "# Run all checks\npdm run all\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 30,
    "content": "## Formatting and Linting Manually\n\nThis package is configured to use yapf code formatting\n\n1. Install lint dependencies:\n\n```\npdm install -G lint\n```\n\n2. Run code formatting from the project directory:\n\n```\nyapf --in-place --recursive --verbose ."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 31,
    "content": "# Short version\nyapf -ir -vv .\n```\n\n3. Run linter from the project directory:\n\n```\nflake8 .\n```\n\n> **_NOTE:_** The configuration for yapf and flake8 is in the `pyproject.toml`."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 32,
    "content": "## Testing Manually\n\nThis package is configured to use pytest for testing\n\n1. Install test dependencies:\n\n```\npdm install -G test\n```\n\n2. Run tests from the project directory:\n\n```\npytest\n```\n\n> **_NOTE:_** The configuration for pytest and coverage is in the `pyproject.toml`."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 33,
    "content": "## Build Node Modules\n\nWebpack is configured to bundle and build the React app into the `tethysapp/<app_package>/public/frontend` directory. Before building a Python distribution for release, you should build using this command:\n\n```\nnpm run build\n\n```"
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 34,
    "content": "## Test Node Modules\n\nUse the following commands to lint and test the React portion of the app.\n\n```\nnpm run lint\nnpm run test\n\n```\n\nThe linting capability is powered by [eslint](https://eslint.org/) and a number of plugins for React. The testing capabilities include [jest](https://jestjs.io/), [jsdom](https://github.com/jsdom/jsdom#readme), [testing-framework](https://testing-library.com/), [user-event](https://testing-library.com/docs/user-event/intro/), and a few other JavaScript testing utilties to make it easy to test the frontend of the React-Tethys app."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 35,
    "content": "## Acknowledgements\n\nThe React + Django implementation is based on the excellent work done by @Jitensid that can be found on GitHub here: [Jitensid/django-webpack-dev-server](https://github.com/Jitensid/django-webpack-dev-server)."
  },
  {
    "idurl": 62,
    "idtype": "text",
    "order": 36,
    "content": "## Contribute\n\nPlease feel free to contribute!"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB Calibration\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngiab\\_cal/blob/main/README.md](https://github.com/CIROH-UA/ngiab_cal/blob/main/README.md).\n\nA Python CLI tool to simplify hydrologic model calibration for NextGen In A Box (NGIAB) workflows."
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 2,
    "content": "## Table of Contents\n\n- What is this?\n- Installation\n- Requirements\n- Basic Usage\n- Advanced Options\n- Calibration Process\n- Calibration Configuration File\n- Example: Calibrating CAMELS Basins\n- How It Works\n- How is ngen-cal running?\n- Development\n- License\n- Acknowledgments"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 3,
    "content": "## What is this?\n\nngiab-cal is a utility that works with the [NGIAB folder structure](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html#nextgen-run-directory-structure-ngen-run). It automates the creation of a calibration directory with all necessary configuration files to run a modified version of [ngen-cal](https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal).\n\nThe tool simplifies these key tasks:\n\n- Creating calibration configurations\n- Running the calibration process using Docker\n- Copying calibrated parameters back to your model configuration"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 4,
    "content": "## Installation\n\nSeveral installation options are available:"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 5,
    "content": "### Using `uvx` (recommended: for its speed,and efficient environment management)\n\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 6,
    "content": "# Run directly without installation\nuvx ngiab-cal --help"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 7,
    "content": "# Or install as a tool\nuv tool install ngiab-cal\nngiab-cal --help\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 8,
    "content": "### Using `pipx`\n\n```\npipx install ngiab-cal\nngiab-cal --help\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 9,
    "content": "### Traditional pip installation\n\n```\npip install ngiab-cal\npython -m ngiab_cal --help\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 10,
    "content": "## Requirements\n\n- Python 3.10+\n- Docker (for running calibrations)\n- Internet connection (for downloading USGS data)"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 11,
    "content": "## Basic Usage\n\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 12,
    "content": "# Create calibration configuration\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 13,
    "content": "# Create and run calibration (200 iterations)\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID --run -i 200"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 14,
    "content": "# Force recreation of calibration configuration\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID -f\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 15,
    "content": "## Advanced Options\n\n```\nusage: ngiab-cal [-h] [-g GAGE] [-f] [--run] [-i ITERATIONS] [--debug] [-w WARMUP] [--calibration_ratio CALIBRATION_RATIO]\n                  data_folder\n\nCreate a calibration config for ngen-cal\n\npositional arguments:\n  data_folder           Path to the folder you wish to calibrate\n\noptions:\n  -h, --help            show this help message and exit\n  -g GAGE, --gage GAGE  Gage ID to use for calibration\n  -f, --force           Overwrite existing configuration\n  --run                 Try to automatically run the calibration, this may be unstable\n  -i ITERATIONS, --iterations ITERATIONS\n                        Default:100 number of iterations to calibrate for\n\n  --debug               enable debug logging\n  -w WARMUP, --warmup WARMUP\n                        Default:365\n                        Number of days at the beginning of the simulation\n                         to exclude from calibration objective metric calculation\n  --calibration_ratio CALIBRATION_RATIO, --cr CALIBRATION_RATIO\n                        Default:0.5\n                        How to split time after warmup into calibration and validation.\n                        1 == 100% calibration, 0 == 100% validation, 0.8 == 80% calibration 20% validation\n\n```"
  },
  {
    "idurl": 63,
    "idtype": "table",
    "order": 16,
    "content": "|   year 1   |   year 2   |   year 3   |   year 4   |   year 5   |\n|<- warmup ->|<-     calibration     ->|                         |\n|<-             warmup               ->|<-      validation     ->|"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 17,
    "content": "## Calibration Process\n\nThe tool applies a standard hydrological modeling workflow, which involves warmup, calibration, and validation periods. The --warmup period is crucial for allowing the model to reach a stable state before its performance is evaluated against observed data. Following the warmup, the remaining period is typically divided into calibration (where model parameters are adjusted to match observations) and validation (where the model's performance with the calibrated parameters is tested on an independent dataset). The tool facilitates this split, as detailed in the diagram and options below.\n\n```\nDefault calibration settings on a 5 year period\n\n\n\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 18,
    "content": "## Example: Calibrating CAMELS Basins\n\nHere's a script to calibrate all CAMELS basins:\n\n```\n#!/bin/bash"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 19,
    "content": "# Download a list of CAMELS gage ids\nwget https://raw.githubusercontent.com/peckhams/nextgen_basin_repo/5e1317256a9365ae3a24a250358314e1e9ffc339/CAMELS/Data/camels_name.txt ./camels_name.txt\noutput_folder=$(cat ~/.ngiab/preprocessor)\nwhile read line\ndo\n    gage=$(echo \"$line\" | cut -d ';' -f 1)\n    echo $gage\n    # subset the hydrofabric, calculate mean-average area forcings, generate model config files\n    uvx --from ngiab_data_preprocess cli -i gage-\"$gage\" -sfr --start 2007-10-01 --end 2013-09-30\n    # calibrate gage for 200 iterations\n    uvx ngiab-cal \"$output_folder\"/gage-\"$gage\" -g \"$gage\" --run -i 200\ndone < <(tail -n +2 ./camels_name.txt)\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 20,
    "content": "## Calibration Configuration File\n\nngiab-cal generates a configuration file at `calibration/ngen_cal_conf.yaml` that controls the calibration process. This file is created from a template with the following key sections:"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 21,
    "content": "### General Configuration\n\n```\ngeneral:\n  strategy:\n    type: estimation\n    algorithm: dds         # Uses Dynamically Dimensioned Search algorithm\n  name: calib              # Don't modify this\n  log: true               # Enable logging\n  workdir: /ngen/ngen/data/calibration  # Don't modify this working directory in the Docker container\n  yaml_file: /ngen/ngen/data/calibration/ngen_cal_conf.yaml # Don't modify this either\n  iterations: 100         # Number of calibration iterations (customizable with -i flag)\n  restart: 0              # Start from beginning (0) or resume from iteration\n```"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 22,
    "content": "### Model Parameters\n\nThe file defines parameters for both CFE (Conceptual Functional Equivalent) and Noah-OWP-Modular (an extended, refactored version of the Noah-MP land surface model) hydrologic models:\n\n```\nCFE:\n  - name: b               # CFE parameter name\n    min: 2.0              # Minimum allowed value\n    max: 15.0             # Maximum allowed value\n    init: 4.05            # Initial value\n  - name: satpsi\n    min: 0.03\n    max: 0.955\n    init: 0.355\n  # Additional parameters...\n```\n\nFor each parameter, the configuration specifies:\n\n- `name`: Parameter identifier\n- `min`: Minimum allowed value during calibration\n- `max`: Maximum allowed value during calibration\n- `init`: Initial value"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 23,
    "content": "### Evaluation Configuration\n\n```\neval_params:\n  objective: kge           # Kling-Gupta Efficiency as objective function\n  evaluation_start: \"...\"  # Start time for calibration period\n  evaluation_stop: \"...\"   # End time for calibration period\n  valid_start_time: \"...\"  # Start time including warmup\n  valid_end_time: \"...\"    # End time of simulation\n  # Additional time parameters...\n  basinID: 01646500        # USGS gage ID\n  site_name: \"USGS 01646500: \"  # Label for plots\n```\n\nThese time periods follow the calibration process diagram shown earlier, with separate periods for warmup, calibration, and validation."
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 24,
    "content": "## How It Works\n\n1. **Input Validation** \\- Checks that all required files are present\n2. **USGS Data Download** \\- Retrieves observed streamflow for calibration\n3. **Configuration Creation** \\- Generates necessary files for ngen-cal\n4. **Docker Execution** \\- Runs the calibration in a containerized environment\n5. **Parameter Extraction** \\- Copies calibrated parameters back to your configuration"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 25,
    "content": "## How is ngen-cal running?\n\nThe tool uses a custom [branch of CIROH-UA/ngen-cal](https://github.com/CIROH-UA/ngen-cal/tree/ngiab_cal) called `ngiab_cal`. This branch contains:\n\n- A modified version of ngen-cal\n- A Dockerfile that builds on top of the Next Gen In-A-Box container\n- Installation of ngen-cal inside the container\n\nWhen you provide the `--run` argument, it downloads a pre-built Docker image ( `joshcu/ngiab-cal:demo`) to run the calibration."
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 26,
    "content": "## Limitations\n\nngiab-cal has several important limitations to be aware of:\n\n1. **Limited Calibration Algorithms**: Currently only configured to use the Dynamically Dimensioned Search (DDS) algorithm. Other algorithms available in the main ngen-cal branch are not supported.\n\n2. **Limited Model Support**: Only calibrates parameters for CFE (Conceptual Functional Equivalent) and NoahOWP-Modular hydrologic models. Other models in the NextGen framework are not currently supported.\n\n3. **Single-Gage Calibration**: Designed for calibrating single-basin models using one USGS streamgage. Multi-gage or multi-objective calibration is not supported.\n\n4. **Custom ngen-cal Branch**: Uses a modified version of ngen-cal from the `ngiab_cal` branch, which differs from the main branch. Features from newer releases of ngen-cal may not be available.\n\n5. **Compatibility Concerns**: The customized ngen-cal implementation may not be compatible with future changes to either ngen-cal or the NGIAB framework."
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 27,
    "content": "## Development\n\nContributions welcome! Comprehensive development instructions coming soon"
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 28,
    "content": "## Visualisation\n\nDuring calibration, plots will be created inside `./calibration/Output/Calibration_Run/ngen_*******_worker/Plot_Iteration/`\nAfter calibration, the tool will copy the validated parameters back into the root config folder which allows you to run ngiab with the [guide script](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/guide.sh) and use the teehr / tethys visualiser.\nMore streamlined workflow coming soon."
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 29,
    "content": "## License\n\n\"Software code created by U.S. Government employees is not subject to copyright\nin the United States (17 U.S.C. ¬ß105). The United States/Department of Commerce\nreserve all rights to seek and obtain copyright protection in countries other\nthan the United States for Software authored in its entirety by the Department\nof Commerce. To this end, the Department of Commerce hereby grants to Recipient\na royalty-free, nonexclusive license to use, copy, and create derivative works\nof the Software outside of the United States.\""
  },
  {
    "idurl": 63,
    "idtype": "text",
    "order": 30,
    "content": "## Acknowledgments\n\n- [CIROH](https://docs.ciroh.org/) for NextGen In A Box\n- [NGEN-CAL](https://github.com/NOAA-OWP/ngen-cal) developers"
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 1,
    "content": "# Community Hydrofabric\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/community\\_hf\\_patcher/blob/main/README.md](https://github.com/CIROH-UA/community_hf_patcher/blob/main/README.md).\n\nThis repository serves as the basis of the **CIROH Community Hydrofabric**, a temporary fork of [Lynker Spatial](https://github.com/CIROH-UA/community_hf_patcher/blob/main/lynker-spatial.com)'s **[NextGen Hydrofabric v2.2](https://www.lynker-spatial.com/data?path=hydrofabric%2Fv2.2%2Fconus%2F)**. This fork contains adjustments that improve the hydrofabric's compatibility with CIROH projects, including the [NextGen In A Box](https://ngiab.ciroh.org/) ecosystem. These changes are slated for inclusion in the NextGen Hydrofabric v3, which will supplant this fork when it is released.\n\nIf you'd like to submit changes for long-term inclusion in the NextGen Hydrofabric, then you're most likely looking for Lynker Spatial's [**community.fabric**](https://github.com/lynker-spatial/community.fabric) repository.\n\nThis repository consists of scripts and Docker configurations that apply this fork's patches. Upon running, the patched hydrofabric is published to the [**Community Hydrofabric bucket**](https://communityhydrofabric.s3.us-east-1.amazonaws.com/index.html#hydrofabrics/community/) on AWS S3."
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 2,
    "content": "The [source hydrofabric](https://www.lynker-spatial.com/data?path=hydrofabric%2Fv2.2%2Fconus%2F) is provided by Lynker Spatial [under the ODbL license](https://lynker-spatial.s3-us-west-2.amazonaws.com/copyright.html)."
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 3,
    "content": "## Current Patches\n\n- Corrects the gage to flowpath mapping of around 4500 gages.\n  - A full list [is available here](https://communityhydrofabric.s3.us-east-1.amazonaws.com/hydrofabrics/community/gage_replacements.csv) in the [Community Hydrofabric bucket](https://communityhydrofabric.s3.us-east-1.amazonaws.com/index.html#hydrofabrics/community/).\n  - Detailed information regarding this patch is available in its [pull request](https://github.com/CIROH-UA/community_hf_patcher/pull/1).\n- Reformats hydrolocation table to be gpkg compliant.\n  - This allows the hydrofabric to appear as geometry in GIS applications and tools.\n- Adds database indices to commonly searched values to speed up retrieval.\n  - Example: `select * from divides where id=\"wb-1234\"`"
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 4,
    "content": "## Usage"
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 5,
    "content": "### Prerequisites\n\n- Docker\n- AWS CLI (if you plan to upload files to S3)"
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 6,
    "content": "### Running\n\nThe full build process is handled by the below shell script:\n\n```\n./generate_hydrofabric.sh\n```\n\nIf you'd like to upload files to an AWS S3 bucket, then uncomment the AWS commands in the shell scripts."
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 7,
    "content": "## Repository Structure\n\n- **`generate_hydrofabric.sh`**: The main entrypoint of the patcher. Responsible for building and running the Docker container, extracting processed hydrofabric files, and optionally uploading them to an S3 bucket.\n- **`generate_vpus.sh`**: A secondary entrypoint for the patcher. It functions similarly to `generate_hydrofabric.sh`, but instead subsets the hydrofabric into individual VPUs.\n- **`Dockerfile`**: A multi-stage Dockerfile that defines the steps for downloading, processing, and compressing hydrofabric data.\n- **`scripts/`**: Contains Python and shell scripts for specific data processing tasks, such as updating gages, converting hydrolocations, and subsetting VPUs.\n- **`scripts/formatting`**: Scripts purely related to how the information is stored in the hydrofabric.\n- **`scripts/hydro`**: Scripts that modify the hydrologic data. Any changes that would alter the output of a simulation will be here."
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 8,
    "content": "## Workflow Overview\n\nDocker is being used in a _slightly_ unconventional way here to take advantage of the automatic hashing of files and caching of steps to only re-run sections that have been modified. The ADD command on the source hydrofabric is extremely slow, so this process will need to be reworked if the fork is updated frequently.\n\n1. **Build and Run the Docker Container**:\nEach of the shell scripts call on the Dockerfile to build and run the container. The Dockerfile defines multiple stages for downloading, processing, and compressing hydrofabric data. Each stage performs a specific task, such as:\n   - Downloading raw hydrofabric files.\n   - Adding indices for faster querying.\n   - Updating gages and converting hydrolocations.\n   - Subsetting VPUs and compressing the output.\n2. **Extract files**:\nOnce all stages are complete, the script extracts the processed files from the container.\n\n`generate_hydrofabric.sh` will export the following:\n\n   - `conus_nextgen.gpkg`: The complete patched hydrofabric.\n   - `conus_nextgen.tar.gz`: Identical to the above, but compressed as a tarball.\n   - `gage_replacements.csv`: A catalog of which gages were altered by the patcher."
  },
  {
    "idurl": 64,
    "idtype": "text",
    "order": 9,
    "content": "`generate_vpus.sh` will export the following:\n   - `VPU`: A folder containing separate hydrofabrics for each VPU.\n   - `VPU/compressed`: A folder containing compressed tarballs of the VPU hydrofabrics.\n3. **Upload to S3 (Optional)**:\nIf you have valid credentials to the Community Hydrofabric S3 bucket, uncomment the lines in `generate_hydrofabric.sh` and `generate_vpus.sh` to upload the output automatically."
  },
  {
    "idurl": 76,
    "idtype": "text",
    "order": 1,
    "content": "# Getting Started\n\ninfo\n\nSee the [Intro to NGIAB](https://docs.ciroh.org/docs/products/ngiab/intro/) pages for a closer look at the topics summarized here.\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/01\\_GETTING\\_STARTED.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/01_GETTING_STARTED.md).\n\nNextGen In A Box is a simple, straightforward way to begin integrating the NextGen framework into your research, creating a pathway to connect your work with other models and easily implement it in operational contexts."
  },
  {
    "idurl": 76,
    "idtype": "text",
    "order": 2,
    "content": "### What is the NextGen framework?\n\nThe Next Generation Water Resources Modeling Framework, most frequently referred to as NextGen, is a hydrologic modeling framework that forms the core for upcoming modern versions of the National Water Model. NextGen is highly modular and model-agnostic, which allows it to easily interoperate with hydrological models of any kind via the [Basic Model Interface](https://csdms.colorado.edu/wiki/BMI) (BMI) standard.\n\nUnfortunately, while NextGen is extremely powerful, it is also extremely laborious to configure and deploy. NGIAB solves this problem via containerization.\n\n> For a more detailed summary, click here: [https://docs.ciroh.org/docs/products/ngiab/intro/what-is](https://docs.ciroh.org/docs/products/ngiab/intro/what-is)"
  },
  {
    "idurl": 76,
    "idtype": "text",
    "order": 3,
    "content": "### What is containerization?\n\nContainerization is a type of virtualization technology, which means that it creates simulated operating systems on top of your standard operating system. Compared to traditional virtual machines, which act as standalone units, containers save on storage and memory by sharing key components of the operating system between containers.\n\nThese containers also come with two other benefits: they offer a consistent environment through which software can be run, and they can be easily be reproduced from \"image\" files, which save the container's state in its entirety. NGIAB takes advantage of these states by containerizing the NextGen framework, entirely mitigating its difficult setup process."
  },
  {
    "idurl": 76,
    "idtype": "text",
    "order": 4,
    "content": "## Using NGIAB\n\nIf you're completely new to the NGIAB, the **[NGIAB 101 training module](https://docs.ciroh.org/training-NGIAB-101/)** is the best place to get started. It contains everything you need to get acquainted with both NGIAB itself and its surrounding ecosystem of helpful tools.\n\nOtherwise, the [next section](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/02_INSTALL.md) of this documentation offers a quick refresher on how to start running NGIAB."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 1,
    "content": "# Installing NGIAB\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/02\\_INSTALL.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/02_INSTALL.md).\n\nNGIAB is designed to run on Unix systems, which means that it's a bit easier to run on Mac and Linux computers.\n\nWindows users can still make full use of NGIAB using Windows Subsystem for Linux (WSL); however, this may impact performance."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 2,
    "content": "## Prerequisites\n\nTo get started with NGIAB-CloudInfra, you'll need to have Docker installed (and WSL, if appropriate)."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 3,
    "content": "### Windows\n\n1. **Install WSL:** Head over to Microsoft's official documentation and follow their comprehensive guide on installing WSL: [https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install)\n2. **Install Docker Desktop:** Begin by downloading and installing Docker Desktop from the official website: [https://docs.docker.com/desktop/install/windows-install/#install-docker-desktop-on-windows](https://docs.docker.com/desktop/install/windows-install/#install-docker-desktop-on-windows)\n3. **Start Docker Desktop:** After installation, launch the Docker Desktop application.\n4. **Open WSL as Admin:** Right-click on the WSL icon and select \"Run as Administrator\".\n5. **Verify Installation:** In the WSL window, type the command docker ps -a to check if Docker is running correctly. This command should display a list of Docker containers.\n\n> **Warning**: If you've installed WSL before as a part of Docker, be sure to create a second WSL distribution that isn't tied to Docker.\n> NextGen In A Box shell commands can't be run from within Docker's dedicated WSL environment."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 4,
    "content": "> Note that the absolute best Windows performance can be achieved by installing Docker Engine within a WSL environment and working strictly from there, using the same steps as a typical Linux installation.\n>\n> However, since the overhead from WSL will apply either way, it's often not worth the hassle. A purely Linux-based environment is strongly recommended for performance-sensitive and operational applications."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 5,
    "content": "### Mac\n\n1. **Install Docker Desktop:** Download and install Docker Desktop for Mac from: [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)\n2. **Start Docker Desktop:** Launch the Docker Desktop application once the installation is complete.\n3. **Open Terminal:** Open the Terminal application on your Mac.\n4. **Verify Installation:** Similar to Windows, use the command docker ps -a in the Terminal to verify Docker is functioning as expected.\n\n> Note that the Docker VMM offers the best performance on Macs. For more information, see Docker's documentation on [Virtual Machine Managers](https://docs.docker.com/desktop/features/vmm/)."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 6,
    "content": "### Linux\n\n1. **Install Docker:** The installation process for Linux varies depending on your distribution. Refer to the official documentation for detailed instructions: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)\n2. **Start Docker and Verify:** Follow the same steps as described for Mac to start Docker and verify its installation using the docker ps -a command in the terminal.\n\n> Note that Linux-based Docker performance will be significantly improved when installing Docker Engine rather than Docker Desktop."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 7,
    "content": "## Installing and Testing\n\nAt a bare minimum, installing NGIAB is as simple as downloading this repository.\nHowever, this guide also includes steps to download sample data and start up your first NGIAB run, which will help you get started right away."
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 8,
    "content": "### Step 1: Create Project Directory\n\n- **Windows users: WSL (Right click and run as Admin):** For ease of access, you may want to store NGIAB's files in your Windows directories. To move there, run the following in your WSL CLI:\n\n```\ncd /mnt/c/Users/<Folder>\n```\n\n- From there, navigate to the directory where you'd like to store NGIAB and its associated data.\n\n```\nmkdir -p NextGen\ncd NextGen\n```"
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 9,
    "content": "### Step 2: Download Sample Data\n\n> While this step isn't strictly necessary, it'll be useful for verifying that NGIAB is working properly on your system.\n\n- Within your project directory, create the `ngen-data` folder to hold the sample data.\n\n```\nmkdir -p ngen-data\ncd ngen-data\n```\n\n- Use wget to download the compressed data file. Then, extract it.\n\n```\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\ntar -xf AWI_16_10154200_009.tar.gz\n```\n\n- Then, return to the root of the project directory.\n\n```\ncd ..\n```"
  },
  {
    "idurl": 77,
    "idtype": "text",
    "order": 10,
    "content": "### Step 3: Clone and Run NGIAB\n\n> **For WSL users:** Before pulling NGIAB, ensure that Git is configured to pull with LF line breaks instead of CRLF line breaks. Failing to do so will prevent NGIAB's shell scripts from correctly running.\n>\n> Information on triaging this issue is available in the [NGIAB 101 training module](https://docs.ciroh.org/training-NGIAB-101/installation.html).\n\n- Clone the NGIAB-CloudInfra repository.\n\n```\ngit clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\ncd NGIAB-CloudInfra\n```\n\n- At this point, everything you need to install NGIAB has been installed!\n- To test your installation, try running the interactive guide script, which will help you navigate your first model run:\n\n```\n./guide.sh\n```\n\n> For a broader introduction to using the NGIAB ecosystem, including how to preprocess your own data, please see the [NGIAB 101 training module](https://docs.ciroh.org/training-NGIAB-101/)."
  },
  {
    "idurl": 78,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB Specifications\n\n[**üìÑÔ∏èContainers and Guide Scripts** \\\\\nGuidance on NGIAB's associated containers and guide scripts.](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/specifications/containers)[**üìÑÔ∏èModel Run Directories** \\\\\nGuidance on NGIAB's model run directories.](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/specifications/run-directories)[**üìÑÔ∏èRealization Files** \\\\\nGuidance on NextGen realization files.](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/specifications/realizations)[**üìÑÔ∏èIncluded Models** \\\\\nGuidance on models included within NGIAB.](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/specifications/models)"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 1,
    "content": "# Building NGIAB Docker\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/04\\_BUILDING.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/04_BUILDING.md).\n\n> To be completed soon."
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 2,
    "content": "## Building the container locally"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 3,
    "content": "## Running a local version of the container"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 4,
    "content": "## Modifying the container"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 5,
    "content": "### Changing `ngen`/ `t-route` sources"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 6,
    "content": "### Adding C/C++/Fortran BMI modules"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 7,
    "content": "### Adding Python BMI modules"
  },
  {
    "idurl": 79,
    "idtype": "text",
    "order": 8,
    "content": "## Deploying new releases\n\nAll new releases should be deployed via the CI/CD scripts."
  },
  {
    "idurl": 80,
    "idtype": "text",
    "order": 1,
    "content": "# Contributing to NGIAB\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/05\\_CONTRIBUTE.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/05_CONTRIBUTE.md).\n\nThank you for considering contributing to our project! We welcome contributions from everyone, and we want to make the process as easy and transparent as possible."
  },
  {
    "idurl": 80,
    "idtype": "text",
    "order": 2,
    "content": "## Issue tracking\n\nPlease check our [issue tracker](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues) for existing issues that you can work on. If you find a new issue or have a feature request, please open a new issue so we can discuss it and prioritize it accordingly.\n\n> Note that in addition to its own reports, the NGIAB-CloudInfra issue tracker currently hosts tickets from CIROH CyberInfrastructure."
  },
  {
    "idurl": 80,
    "idtype": "text",
    "order": 3,
    "content": "## How to contribute\n\n1. If an Issue does not already exist, you can use the [bug report template](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=) to create one.\n\n2. Depending on your permissions, create either a fork or a branch of the repository to make changes in.\n\n3. Make your changes and commit them with clear and concise commit messages.\n\n4. Push your changes to the fork/branch.\n\n5. Open a pull request from your fork/branch to the repository with a clear and descriptive title and detailed description of your changes.\n\n6. Once your pull request is submitted, a PR validation CI pipeline will trigger that will validate the changes.\n\n7. Please make sure that the PR pipeline is successful.\n\n8. The maintainers of the repository will merge the PR upon verifying both the validation pipeline results and the contents of the commit."
  },
  {
    "idurl": 80,
    "idtype": "text",
    "order": 4,
    "content": "## Testing changes locally\n\nPlease see [\"Building the NGIAB Docker Container\"](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/04_BUILDING.md) for information on testing local changes."
  },
  {
    "idurl": 81,
    "idtype": "text",
    "order": 1,
    "content": "# Contact\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/06\\_CONTACT.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/06_CONTACT.md).\n\nNextGen In A Box is primarily developed and maintained by the [Collaborative Institute for Research to Operations in Hydrology](https://ciroh.org/) (CIROH).\nIf you're having trouble or would like to send us feedback, please don't hesitate to do so."
  },
  {
    "idurl": 81,
    "idtype": "text",
    "order": 2,
    "content": "## Reporting issues\n\nIf you have a bug or feature request, this repository's [issue tracker](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues) is the best place to submit them.\nFor guidance on submitting an issue, please see the [contribution guide](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/05_CONTRIBUTE.md)."
  },
  {
    "idurl": 81,
    "idtype": "text",
    "order": 3,
    "content": "## Getting help\n\nCIROH's [monthly office hours](https://docs.ciroh.org/docs/products/ngiab/office-hours) are a great way to engage with experts, ask questions, and stay updated on the latest developments.\nTo join, email us at [ciroh-it-admin@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/mailto:ciroh-it-admin@ua.edu) to receive the Teams Meeting link and calendar invitation.\n\nIf you need more immediate help, we're also available via email or Slack ‚Äî see below."
  },
  {
    "idurl": 81,
    "idtype": "text",
    "order": 4,
    "content": "## General inquiries\n\nNeed to get in touch for some other reason? Please reach out!\n\n- Email: [ciroh-it-admin@ua.edu](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/mailto:ciroh-it-admin@ua.edu)\n- Slack: [#ciroh-ua-it-support channel](https://cirohworkspace.slack.com/archives/C057BLQB867)\n  - _Note: requires access to CIROH Slack workspace_"
  },
  {
    "idurl": 81,
    "idtype": "table",
    "order": 5,
    "content": "|  |  |\n| --- | --- |\n| [![CIROH Logo](https://github.com/CIROH-UA/NGIAB-CloudInfra/raw/main/docs/img/ciroh-bgsafe.png)](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/img/ciroh-bgsafe.png) | Funding for this project was provided by the National Oceanic & Atmospheric Administration (NOAA), awarded to the Cooperative Institute for Research to Operations in Hydrology (CIROH) through the NOAA Cooperative Agreement with The University of Alabama (NA22NWS4320003). |"
  },
  {
    "idurl": 81,
    "idtype": "text",
    "order": 6,
    "content": "## Funding acknowledgement"
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 1,
    "content": "# Containers and Guide Scripts\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03\\_01\\_CONTAINERS.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_01_CONTAINERS.md).\n\nNGIAB includes a series of guide scripts that invoke both NGIAB itself and several associated utilities from the NGIAB ecosystem. This page will provide information on what these containers are, how to invoke them through the guide scripts, and how they can be called upon manually.\n\n> Remember: for most users, the guide scripts are the best way to invoke NGIAB-related containers. Manually running the containers is mostly useful for automation, custom scripting, and other development purposes."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 2,
    "content": "## NGIAB Docker distribution ( [`awiciroh/ciroh-ngen-image`](https://hub.docker.com/r/awiciroh/ciroh-ngen-image/tags))\n\nThis image is **the core NGIAB distribution** for non-HPC environments. It contains everything you need to run the NextGen framework, along with a core series of common NextGen-compatible models. For the specifics of which models are included by default, see [3.4. Included Models](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_04_MODELS.md).\n\nThe NGIAB image builds from this repository's `docker/` folder. The following command is used to build it locally:\n\n```\ncd docker\ndocker build -f Dockerfile -t awiciroh/ciroh-ngen-image:latest . --no-cache\n```"
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 3,
    "content": "### Input and output\n\nTo run NGIAB, you'll need a valid [NGIAB model run directory](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_03_RUN_DIRECTORIES.md). These directories define the datasets, forcings, and model configuration that NGIAB should pass to the NextGen framework.\n\nIf you're still getting started with NGIAB, consider using the [Data Preprocess](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html) tool to prepare these.\n\nThe model outputs will be saved to the `outputs/` subfolder of the model run directory, alongside additional `metadata/` and `forcings/` subfolders."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 4,
    "content": "### Running from a guide script\n\nRunning `guide.sh` will prompt you to provide a path to your model run directory. After that, it'll automatically begin running NGIAB.\n\nWhen NGIAB starts up, you will be asked whether you'd like to run in \"Serial\" or \"Parallel\" mode. This determines whether the simulation will run on a single thread or across multiple processes. **Pick \"Parallel\" if you're unsure**, as most modern computers are designed to take full advantage of multithreaded processing.\n\n> The \"Run Bash Shell\" and \"Interactive-Shell\" options provide CLI access to the container, allowing its contents to be explored if needed. In practice, this is rarely useful outside of development and debugging."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 5,
    "content": "### Running the container manually\n\nFor most purposes, the `latest` tag will always be the most appropriate option, offering builds for both AMD64 and ARM64 architectures. However, if you find that Docker is pulling the wrong architecture for your system, then `latest-amd64` and `latest-arm64` are available as aliases.\n\nThe following Docker command will launch and run an instance of NGIAB, where `[RUN_DIR]` is replaced with the absolute path of your model run directory:\n\n```\ndocker run --rm -it -v \"[RUN_DIR]:/ngen/ngen/data\" \"awiciroh/ciroh-ngen-image:latest\" /ngen/ngen/data/ [auto]\n```\n\nHere's a breakdown of what this command does:\n\n- `--rm` instructs Docker to tear down and delete the container upon exiting. This is important for saving storage.\n- `-it` is a pair of standard flags that facilitate CLI access to the container.\n- `-v \"[RUN_DIR]:/ngen/ngen/data\"` mounts your run directory's contents to `mgen/ngen/data/` within the container.\n- `\"awiciroh/ciroh-ngen-image:latest\"` identifies the image. (All remaining arguments after this one are passed to the container entrypoint script.)\n- `ngen/ngen/data/` tells the container entrypoint script where the mounted data is.\n- `auto` is an optional argument. If it is included, the container will automatically perform a parallel run of NextGen. Otherwise, an interactive prompt will offer a choice between serial and parallel options."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 6,
    "content": "Note that all execution is facilitated by the container entrypoint script, `HelloNGEN.sh`, which can be found in this repository's `Docker` folder. As such, even if you're running the container manually, you won't need to worry about the finer details of starting up a NextGen run."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 7,
    "content": "## NGIAB TEEHR Integration ( [`awiciroh/ngiab-teehr`](https://hub.docker.com/r/awiciroh/ngiab-teehr/tags))\n\nThis image runs the NGIAB TEEHR integration, which automatically performs comparisons with NWM and USGS data, calculates preliminary metrics, and prepares your model output for in-depth evaluation with the [TEEHR library](https://rtiinternational.github.io/teehr/).\n\nThe image is built from the [ngiab-teehr](https://github.com/CIROH-UA/ngiab-teehr) repository."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 8,
    "content": "### Input and output\n\nThe TEEHR integration takes an already-executed NGIAB model run directory as input.The evaluated results will be saved to the `TEEHR/` subfolder of the model run directory.\n\nNote that the TEEHR integration will **fully overwrite the run directory's contents**. If you'd like to retain your run directory for any reason, saving a backup is strongly recommended."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 9,
    "content": "### Running from a guide script\n\nAfter `guide.sh` completes an NGIAB run, it will optionally ask to run the TEEHR integration. If selected, this will launch the `runTeehr.sh` script. `runTeehr.sh` can also be run independently if desired.\n\nOnce run, `runTeehr.sh` will prompt for a data path and an image tag. If the script is being run immediately after an NGIAB execution, both the most recent path and recommended tag will most likely be correct. From there, execution will begin automatically."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 10,
    "content": "### Running the container manually\n\nThe TEEHR integration offers both `latest` and `x86` tags, which support ARM64 and AMD64 systems, respectively. Be sure to choose the appropriate tag for your system's architecture.\n\nThe following Docker command will launch and run the TEEHR integration, where `[RUN_DIR]` is replaced with the absolute path of your model run directory:\n\n```\ndocker run --rm -v \"[RUN_DIR]:/app/data\" \"awiciroh/ngiab-teehr:[tag]\"\n```\n\nHere's a breakdown of what this command does:\n\n- `--rm` instructs Docker to tear down and delete the container upon exiting. This is important for saving storage.\n- `-v \"[RUN_DIR]:/ngen/ngen/data\"` mounts your data folder's contents to `app/data/` within the container.\n- `\"awiciroh/ngiab-teehr:[tag]\"` identifies the image. Be sure to select the correct tag for your system."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 11,
    "content": "## NGIAB Data Visualizer ( [`awiciroh/tethys-ngiab`](https://hub.docker.com/r/awiciroh/tethys-ngiab/tags))\n\nThis image runs the NGIAB Data Visualizer. This tool offers a spatial web-based exploration of your model outputs powered by [Tethys Platform](https://www.tethysplatform.org/).\n\nThe image is built from the [ngiab-client](https://github.com/CIROH-UA/ngiab-client) repository."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 12,
    "content": "### Input and output\n\nThe Data Visualizer takes an already-executed NGIAB model run directory as input. It will not alter the folder's contents.\n\nThe Data Visualizer's outputs are stored in the `ngiab_visualizer/` subfolder of your user home folder. As such, the Data Visualizer will retain model outputs for display between sessions."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 13,
    "content": "### Running from a guide script\n\nAfter `guide.sh` completes an NGIAB run, it will optionally ask to run the Data Visualizer. If selected, this will launch the `viewOnTethys.sh` script. `viewOnTethys.sh` can also be run independently if desired.\n\nOnce run, `viewOnTethys.sh` will prompt for a data path and an image tag. If the script is being run immediately after an NGIAB execution, both the most recent path and recommended tag will most likely be correct. It will then request a port; this should only need to be changed if port 80 is occupied or in certain SSH configurations. From there, follow the on-screen instructions.\n\nFor more information on using the visualizer, please see the [ngiab-client](https://github.com/CIROH-UA/ngiab-client) repository."
  },
  {
    "idurl": 83,
    "idtype": "text",
    "order": 14,
    "content": "### Running the container manually\n\nDue to the complexity of launching the Data Visualizer, launching it without using a guide script is not currently recommended. If necessary, please reference `viewOnTethys.sh` for more details on this process."
  },
  {
    "idurl": 84,
    "idtype": "text",
    "order": 1,
    "content": "# Model Run Directories\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03\\_02\\_RUN\\_DIRECTORIES.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_02_RUN_DIRECTORIES.md).\n\nRunning NextGen requires building a standard run directory complete with only the necessary files. This can be done automatically with the [Data Preprocess](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/docs/products/ngiab/components/ngiab-preprocessor) tool.\n\nBelow is an explanation of NGIAB's format for model run directories."
  },
  {
    "idurl": 84,
    "idtype": "text",
    "order": 2,
    "content": "### Root directory `ngen-run/`\n\nAn NGIAB run directory `ngen-run` contains the following subfolders:\n\n- User-provided (required)\n  - `config`: model configuration files and hydrofabric configuration files.\n- Generated by NGIAB\n  - `forcings`: catchment-level forcing timeseries files. Forcing files contain variables like wind speed, temperature, precipitation, and solar radiation.\n  - `metadata`: Programmatically generated folder used within ngen. Do not edit this folder.\n  - `outputs`: This is where ngen will place the output files.\n  - `restart`: For restart files. (optional)\n  - `lakeout`: for t-route. (optional)\n- Generated by TEEHR integration\n  - `teehr`: Contains TEEHR-formatted output data.\n\n```\nngen-run/\n‚îÇ\n‚îú‚îÄ‚îÄ config/\n‚îÇ\n‚îú‚îÄ‚îÄ forcings/\n‚îÇ\n‚îú‚îÄ‚îÄ lakeout/\n|\n‚îú‚îÄ‚îÄ metadata/\n‚îÇ\n‚îú‚îÄ‚îÄ outputs/\n‚îÇ\n‚îú‚îÄ‚îÄ restart/\n‚îÇ\n‚îú‚îÄ‚îÄ teehr/\n\n```"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 3,
    "content": "|   |"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 4,
    "content": "|   |"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 5,
    "content": "|   |"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 6,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 7,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 84,
    "idtype": "table",
    "order": 8,
    "content": "|   ‚îÇ   |\n|   |"
  },
  {
    "idurl": 84,
    "idtype": "text",
    "order": 9,
    "content": "### Configuration directory `ngen-run/config/`\n\nThis folder contains the NextGen realization file, which serves as the primary model configuration for the ngen framework. This file specifies which models to run (such as NoahOWP/CFE, LSTM, etc), run parameters like date and time, and hydrofabric specifications (like location, gage, catchment).\n\nBased on the models defined in the realization file, [BMI](https://bmi.csdms.io/en/stable/index.html) configuration files may be required. For those models that require per-catchment configuration files, a folder will hold these files for each model in `ngen-run/config/cat-config`. See the directory structure convention below.\n\n```\nngen-run/\n|\n‚îú‚îÄ‚îÄ config/\n|   ‚îÇ\n|   ‚îú‚îÄ‚îÄ nextgen_09.gpkg\n\n\n|   ‚îú‚îÄ‚îÄ realization.json\n\n\n|   ‚îú‚îÄ‚îÄ ngen.yaml\n\n\n|   ‚îú‚îÄ‚îÄ cat-config/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄPET/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄCFE/\n|   ‚îÇ   |\n\n   ‚îú‚îÄ‚îÄNOAH-OWP-M/\n\n```\n\nNextGen requires a single geopackage file. This file is the [hydrofabric (Johnson, 2022)](https://mikejohnson51.github.io/hyAggregate/) (spatial data). An example geopackage can be found on [Lynker-Spatial's website](https://www.lynker-spatial.com/data?path=hydrofabric%2Fv2.2%2F). Tools to subset a geopackage into a smaller domain can be found at [Lynker's hfsubset](https://github.com/LynkerIntel/hfsubset)."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 1,
    "content": "# Realization Files\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03\\_03\\_REALIZATIONS.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_03_REALIZATIONS.md).\n\nRealization files define the arrangement of models that the NextGen framework will use to execute model runs.\nThese realizations are written in [JSON format](https://www.json.org/json-en.html).\n\n> This section is heavily referenced from [the NextGen framework's documentation on the topic](https://noaa-owp.github.io/ngen/md_doc_2_r_e_a_l_i_z_a_t_i_o_n___c_o_n_f_i_g_u_r_a_t_i_o_n.html)."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 2,
    "content": "## Top-level keys\n\nThe realization configuration must contain three first-level object keys: `global`, `time`, and `catchments`.\nNGIAB model run configurations will often also include the `routing` and `metadata` keys.\n\nThe optional `output_root` key should not be used with NGIAB, as it will interfere with retrieving model outputs from the container."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 3,
    "content": "## `global` key-value object\n\nThe `global` key-value object defines the global formulation and forcings for the model run.\n\n```\n\"global\": {\n  \"formulations\": [\\\n    {\\\n        \"name\": \"bmi_c\",\\\n        \"params\": {\\\n            \"maxsmc\": 0.439,\\\n            \"wltsmc\": 0.066,\\\n            \"satdk\": 0.00000338\\\n        /* --- continued --- */\\\n        }\\\n    }\\\n  ],\n  \"forcing\": {\n      \"file_pattern\": \".*{{id}}.*.csv\",\n      \"path\": \"./data/forcing/\"\n  }\n},\n```"
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 4,
    "content": "### `formulations`\n\nContains a list of formulation key-value objects that define the default required formulations. Each formulation object has the following keys:\n\n- `name`: Defines the name value for the type of BMI module being referenced.\n\n  - Valid values include `bmi_c++`, `bmi_c`, `bmi_fortran`, `bmi_python`, and `bmi_multi`.\n- `params`: A key-value object. Its contents will vary depending on the target model.\n\nThe parameters of a formulation differ depending on its contents. For model-specific parameters, see \" [Included Models](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_04_MODELS.md)\".\n\n- Required\n  - `model_type_name`: A unique string identifying the underlying model type.\n  - `main_output_variable`: The name of the framework variable that will store the formulation's `get_response()` function."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 5,
    "content": "- It is not responsible for catchment output files, which are controlled by the `output_variable` parameter.\n- Required for single-module formulations\n  - `init_config`: The path for the BMI initialization for the model. The substring `{{id}}` will be substituted for the catchment ID, allowing for per-catchment configurations.\n  - `uses_forcing_file`: A boolean indicating whether the underlying BMI model is written to read input forcing data from a forcing file.\n- Conditionally required\n  - `forcing_file`: A string path to the forcing data file for the model. The substring `{{id}}` will be substituted for the catchment ID, allowing for per-catchment configurations.\n\n    - Only required if `uses_forcing_file` is enabled.\n  - `registration_function`: Name of the pointer registration function in the external module. Defaults to `register_bmi` if unspecified.\n\n    - Only meaningful for `bmi_c` formulations.\n    - Only required if `register_bmi` is not the pointer registration function.\n  - `library_file`: Path to the library file for the BMI model."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 6,
    "content": "- Non-meaningful for `bmi_python` formulations, which depend on the current Python environment.\n    - Required for all other formulations targeting NGIAB, as NGIAB relies strictly on external BMI libraries.\n    - For paths to library files included in NGIAB, see \" [Included Models](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_04_MODELS.md)\".\n  - `python_type`: The name of the Python class representing the BMI model, including the package name.\n\n    - Only required for `bmi_python` formulations, and non-meaningful for other types.\n    - For types of included Python models, see \" [Included Models](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_04_MODELS.md)\".\n- Optional\n  - `variables_names_map`: Specifies a mapping of model inputs/outputs to aliases that act as framework variables.\n\n    - Helpful for instructing the framework on how to provide certain inputs to a model, such as forcing data or outputs from other submodules in `bmi_multi` formulations.\n  - `model_params`: Specifies static or dynamic parameters that will be passed to the model as model variables."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 7,
    "content": "- Note that only the hydrofabric is currently supported as a source of dynamic parameters.\n      - For other forms of data passing, see `variables_names_map`.\n    - Dynamic parameters should be specified as key-value objects with the following contents:\n      - `source`: The source of the parameter. NextGen currently only supports `\"hydrofabric\"` as a source.\n      - `from`: The type of data to pass from the source, such as `\"area_sqkm\"`.\n    - Non-meaningful for `bmi_multi` formulations.\n  - `output_variables`: A list of strings indicating the set of output variables to include in the realization's `get_output_line_for_timestep()` function.\n\n    - Defaults to the output of the model's `get_output_var_names()` function if unspecified.\n    - In `bmi_multi` formulations, this should not be specified for submodules.\n  - `output_header_fields`: A list of strings used as a header for the realization's printed output. In practice, this should be formatted versions of the variable names from `output_variables`.\n\n    - Defaults to the value of `output_variables` if unspecified.\n    - In `bmi_multi` formulations, this should not be specified for submodules.\n  - `allow_exceed_end_time`: Specifies whether a model is allowed to execute `Update` calls beyond its end time (or the latest forcing data entry). Defaults to `false` if unspecified.\n  - `fixed_time_step`: Specifies whether a model has a fixed time-step size. Defaults to `true` if unspecified.\n- For `bmi_multi` formulations"
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 8,
    "content": "- `modules`: A list of submodules. Each submodule should be specified as a formulation key-value object.\n\n> This section provides only a brief reference for writing formulations in NGIAB. For more specific technical details, please reference the [NextGen framework documentation](https://noaa-owp.github.io/ngen/md_doc_2_b_m_i___m_o_d_e_l_s.html) on external BMI models."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 9,
    "content": "### `forcing`\n\nThe contents of this key-value object will depend on the format of the provided forcings.\n\n- **NetCDF files**\n  - `path`: Points to the relative path of the forcings file.\n  - `provider`: Should be set to `\"NetCDF\"`.\n- **CSV files**\n  - `file_pattern`: The file pattern for the forcing files. Should include the substring `{{id}}`, which will be substituted for the catchment ID.\n  - `path`: Points to the relative path of the forcings directory."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 10,
    "content": "## `time` key-value object\n\nThe `time` key-value object simply contains three keys:\n\n- `start_time`: Defines the UTC start time of the simulation. Must be in the form `yyyy-mm-dd hh:mm:ss`.\n- `end_time`: Defines the UTC end time of the simulation. Must be in the form `yyyy-mm-dd hh:mm:ss`.\n- `output_interval`: Defines the time interval on which model outputs are generated. Written in seconds.\n\n```\n\"time\": {\n    \"start_time\": \"2015-12-01 00:00:00\",\n    \"end_time\": \"2015-12-30 23:00:00\",\n    \"output_interval\": 3600\n},\n```"
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 11,
    "content": "## `catchments` key-value object\n\nThe `catchments` key-value object must contain a list of all catchment object keys that will have defined formulations."
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 12,
    "content": "## `routing` key-value object\n\nNGIAB model runs should always use the following routing object to ensure that `troute.yaml` is accessible within the container.\n\n```\n\"routing\": {\n    \"t_route_config_file_with_path\": \"/ngen/ngen/data/calibration/troute.yaml\",\n    \"t_route_connection_path\": \"\"\n},\n```"
  },
  {
    "idurl": 85,
    "idtype": "text",
    "order": 13,
    "content": "## `metadata` key-value object\n\nThis object is generated by certain tools, such as [Data Preprocess](https://github.com/CIROH-UA/ngiab_data_preprocess), to provide contextual information regarding the model run directory.\nWhile NGIAB doesn't actively use this metadata, it may be helpful in determining how a model run directory was created."
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 1,
    "content": "# Included Models\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03\\_04\\_MODELS.md](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/03_04_MODELS.md)."
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 2,
    "content": "## Simple Logical Tautology Handler (SLOTH)\n\n> _GitHub: [NOAA-OWP/SLoTH](https://github.com/NOAA-OWP/SLoTH)_\n\nSLoTH offers the simplest possible behavior for a model: each input will be returned directly as an output.\nWhile largely useless on its own, it offers necessary utilities for supporting complex formulations of multiple BMI modules.\nFor example, it can be used to feed constant forcing values into a model or echo output values between timesteps.\n\nLibrary file path: `/dmod/shared_libs/libslothmodel.so.1.0.0`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 3,
    "content": "## Conceptual Functional Equivalent (CFE)\n\n> _GitHub: [NOAA-OWP/CFE](https://github.com/NOAA-OWP/CFE)_\n\nThis simplified conceptual model by Fred Ogden is designed to be functionally equivalent to earlier versions of the National Water Model.\nIt offers a streamlined solution to modeling runoff generation, vadose zone dynamics, and groundwater behavior.\n\nLibrary file path: `/dmod/shared_libs/libcfebmi.so.1.0.0`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 4,
    "content": "## Potential Evapotranspiration (PET)\n\n> _GitHub: [NOAA-OWP/evapotranspiration](https://github.com/NOAA-OWP/evapotranspiration)_\n\nThis module bundles several functions for estimating potential evapotranspiration, or the upper bound for the amount of water\nthat will be passively evaporated from soil given sufficient supply.\n\nLibrary file path: `/dmod/shared_libs/libpetbmi.so.1.0.0`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 5,
    "content": "## NOAH-OWP-Modular\n\n> _GitHub: [NOAA-OWP/NOAH-OWP-Modular](https://github.com/NOAA-OWP/NOAH-OWP-Modular)_\n\nNOAH-OWP-Modular is a generalized refactoring of Noah-MP, a land surface model.\n\nLibrary file path: `/dmod/shared_libs/libsurfacebmi.so`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 6,
    "content": "## TOPMODEL\n\n> _GitHub: [NOAA-OWP/TOPMODEL](https://github.com/NOAA-OWP/TOPMODEL)_\n\nTOPMODEL is a watershed model focused on interactions between groundwater and surface water.\n\nLibrary file path: `/dmod/shared_libs/libtopmodelbmi.so.1.0.0`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 7,
    "content": "## Long Short-Term Memory (LSTM)\n\n> _GitHub: [CIROH-UA/lstm](https://github.com/CIROH-UA/lstm)_\n>\n> _Fork developed by [Jonathan Frame](https://github.com/jmframe)_\n\nLSTM networks are a type of recurrent neural network used in deep learning.\nThis LSTM module is specifically tailored for generalize streamflow prediction within CONUS.\n\nPython class: `lstm.bmi_LSTM`"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 8,
    "content": "## Soil Moisture Profiles\n\n> _GitHub: [NOAA-OWP/SoilMoistureProfiles](https://github.com/NOAA-OWP/SoilMoistureProfiles)_\n\nThis module packages several schemes to model soil moisture, including ones specifically tailored to CFE and TOPMODEL.\n\n_Temporarily unavailable due to an [upstream issue](https://github.com/CIROH-UA/ngen/issues/14)._"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 9,
    "content": "## Soil Freeze-Thaw Model\n\n> _GitHub: [NOAA-OWP/SoilFreezeThaw](https://github.com/NOAA-OWP/SoilFreezeThaw)_\n\nThis model simulates heat transfer in soil, enabling modeling of freeze/thaw cycles in water.\nIts underlying methodology is comparable to NOAH-MP (see NOAH-OWP-Modular above).\n\n_Temporarily unavailable due to an [upstream issue](https://github.com/CIROH-UA/ngen/issues/14)._"
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 10,
    "content": "# T-Route\n\n> _GitHub: [CIROH-UA/t-route](https://github.com/CIROH-UA/t-route)_ _Fork maintained by [Josh Cunningham](https://github.com/joshcu)_\n\nT-Route is a routing model used for solving streamflow networks.\nCompared to the other models packaged with NGIAB, T-Route is uniquely central because it sits at the core of all NextGen model runs.\nAs such, instead of invoking it via a formulation, it should be configured in `troute.yaml`.\n\nNGIAB includes a customized fork of T-Route maintained by CIROH.\nIt retains all functionality from the canonical version of T-Route while enhancing it with major performance optimizations."
  },
  {
    "idurl": 86,
    "idtype": "text",
    "order": 11,
    "content": "# Adding additional models\n\nUnfortunately, NGIAB does not currently offer native support for adding additional models.\nInvestigation on how best to provide this functionality is underway.\n\nIn the meantime, you will need to build a new version of the NGIAB-CloudInfra container that incorporates your desired model.\nFor more information, please see [\"Building the NGIAB Docker container\"](https://github.com/CIROH-UA/NGIAB-CloudInfra/blob/main/docs/04_BUILDING.md)."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Infrastructure Access\nsource: https://docs.ciroh.org/docs/services/access\nscraped_date: 2025-01-31\n---\n\nCIROH provides access to public cloud services, HPC, and on-premises infrastructure to support the research projects of CIROH's members and partners.\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 2,
    "content": "## Selecting a platform\n\nClick the buttons below to open/close their panels.\n\n**What is your primary requirement?**\n\nHigh Performance Computing\n\n**Which HPC platform are you planning to use?**\n\nPantarhei HPC\n\nPantarhei offers CPU, GPU, and FPGA nodes.\n\n[Get started with Pantarhei](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/)\n\nWukong HPC\n\nWukong offers CPU and GPU nodes.\n\n[Get started with Wukong](https://docs.ciroh.org/docs/services/on-prem/Wukong/)\n\n* * *\n\nCloud Computing\n\n**What kind of computing environment are you looking for?**\n\nGeneral Purpose\n\nCIROH provides researchers with access to enterprise-level AWS cloud services.\n\n[CIROH AWS Account info](https://docs.ciroh.org/docs/services/cloudservices/aws/)\n\nGoogle-Specific Services\n\nCIROH Google VM Link\nCIROH provides researchers with access to enterprise-level Google Cloud services.\n\n[CIROH Google Account info](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n\nInteractive Computing Environment\n\nIn collaboration with 2i2c, CIROH offers a dedicated JupyterHub environment on Google Cloud specifically designed for hydrological research. Both CPU and GPU options are available.\n\n[CIROH-2i2c JupyterHub info](https://docs.ciroh.org/docs/services/cloudservices/2i2c/)\n\n* * *\n\nData Storage and Archival\n\nHydroshare is a collaborative, general-purpose repository for data, models, and other research products.\n\n[HydroShare info](https://docs.ciroh.org/docs/services/cloudservices/HydroShare/)"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 3,
    "content": "Additionally, CIROH provides researchers with access to data buckets for use with AWS and Google Cloud services.\n\n[CIROH AWS Account info](https://docs.ciroh.org/docs/services/cloudservices/aws/) [CIROH Google Account info](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n\n* * *\n\nData-Intensive Computing\n\nCIROH's AWS instance and Google VMs both offer solid options for data-intensive processes.\n\n[CIROH AWS Account info](https://docs.ciroh.org/docs/services/cloudservices/aws/) [CIROH Google Account info](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 4,
    "content": "## Accessing Public Cloud Services\n\nCIROH has partnered with [**AWS**](https://docs.ciroh.org/docs/services/cloudservices/aws/) and [**Google Cloud**](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/) to provide access to their cloud computing services.\n\nnote\n\nIf using CIROH-2i2c services, please see the \"Accessing CIROH JupyterHub\" section below for additional steps."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 5,
    "content": "### Requesting Project Access\n\nPIs or Workshops Lead leading CIROH projects or workshops may use this form to request cloud computing resources on AWS or Google Cloud. Access is available to all consortium members and partners.\n\n1. Submit a GitHub template request detailing your project requirements and specifications.\n2. Our team will review your request and assist you in obtaining the necessary access.\n\n[Cloud Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=case_studies_call.md&title=)\n\nnote\n\nPlease refer to [this link](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues?q=is%3Aissue%20is%3Aclosed%20label%3Agoogle%2C%222i2c%20JupyterHub%22%2Caws%20) for references to submitted forms.\n\nCIROH Consortium members or partners are responsible for:\n\n- Management of CIROH subaccounts assigned to them.\n- Project-specific software and environment configuration.\n- Handling account creation and/or access for project contacts."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 6,
    "content": "### Cost of Use\n\n- Use of CIROH JupyterHub is free for all consortium projects.\n- Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.\n- CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.\n\n[Exceeding Budget Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=exceeding_budget_request.md&title=)\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 7,
    "content": "## Accessing CIROH-2i2c JupyterHub\n\nIn partnership with [**2i2c**](https://2i2c.org/), CIROH provides [**JupyterHub**](https://docs.ciroh.org/docs/services/cloudservices/2i2c) with both CPU and GPU capabilities."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 8,
    "content": "### Requesting Project Access\n\nPIs or Workshop Leads leading CIROH projects or workshops may use this form to request cloud computing resources (CIROH-2i2c JupyterHub). Access is available to all consortium members and partners.\n\n1. Submit a GitHub template request detailing your project requirements and specifications.\n2. Our team will review your request and assist you in obtaining the necessary access.\n\n[Cloud Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=case_studies_call.md&title=)"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 9,
    "content": "### Requesting Individual Access\n\nSubmit one of the following forms to get access to CIROH JupyterHub environments:\n\n[CIROH-2i2c JupyterHub CPU Access Request Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u) [CIROH-2i2c JupyterHub GPU Access Request Form](https://forms.office.com/r/mkrVJzyg9u)\n\nnote\n\nYou will need to submit your GitHub username for this request.\nIf you do not currently have a GitHub account, follow the instructions at [GitHub](https://docs.github.com/en/get-started/start-your-journey/creating-an-account-on-github)."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 10,
    "content": "### Requesting Custom Images\n\nTo request custom images:\n\n1. Create an `environment.yml` file by exporting your conda environment.\n2. Fill out the CIROH-2i2c JupyterHub Software Install form.\n\n[CIROH-2i2c JupyterHub Software Install Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u)\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 11,
    "content": "## Accessing On-Premises Infrastructure\n\nCIROH operates an on-premises infrastructure that includes high-performance computing (HPC) resources and specialized software via the [**Pantarhei**](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/) and [**Wukong**](https://docs.ciroh.org/docs/services/on-prem/Wukong/) systems."
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 12,
    "content": "### Requesting Project Access\n\nPrinciple Investigators (PIs) leading CIROH projects may use this form to request CIROH on-premise resources for their teams, including Pantarhei and Wukong. Access is available to all consortium members and partners.\n\n1. Submit a GitHub template request detailing your project requirements and specifications.\n2. Our team will review your request and assist you in obtaining the necessary access.\n\n> **Note**: The On-Premises Infrastructure Request Form must be submitted by the PI of the project.\n\n[On-premises Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=)"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 13,
    "content": "### Requesting Individual Access\n\n> **Note**: Before requesting individual access, the On-Premises Infrastructure Request Form above **must** be completed by your PI.\n\nNon-UA users should complete the VPN Access Request section of the form below before proceeding.\n\nFrom there, please complete the On-Premise Access Request section of the form below to request individual access to Pantarhei or Wukong.\n\n[On-Premise Access Request Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUMzdTOUpKVU5UWFNCU0ZQUlowS0cxV0xFRy4u)\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 14,
    "content": "## Accessing JetStream2 through CIROH\n\nIn collaboration with [NSF Access](https://docs.ciroh.org/docs/services/external-resources/nsf-access/), CIROH offers access to an allocation on the [JetStream2](https://docs.ciroh.org/docs/services/external-resources/nsf-access/jetstream) computing platform.\n\n**Step 1:** The PI for your project must submit the Infrastructure Request Form below to request team-wide access to JetStream2.\n\n[Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=)\n\n**Step 2:** If you don't already have an NSF Access account, register for one using the link below.\n\n[NSF Access New User Registration](https://operations.access-ci.org/identity/new-user)\n\n**Step 3:** Using your NSF Access ID, submit the JetStream2 Access Request form for individual user accounts on JetStream2.\n\n[JetStream2 Access Request Form](https://forms.office.com/r/ERyKyHbdaC)\n\ninfo\n\nIf you are unable to access the JetStream2 forms, please contact the CIROH team at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) for assistance.\n\nOnce you're granted access, you're ready to begin using JetStream2! Visit the [logging in to JetStream2 page](https://docs.jetstream-cloud.org/getting-started/login/) to get started.\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 15,
    "content": "## Accessing NWM BigQuery API\n\nTo access CIROH's [**NWM BigQuery API**](https://docs.ciroh.org/docs/products/data-management/bigquery-api/), please submit the form below.\n\n[NWM BigQuery API Access Request Form](https://forms.office.com/r/FeNpjZstkr)\n\n* * *"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 16,
    "content": "## Requesting Infrastructure Support for Conferences and Programs\n\nCIROH Project Leads can request computing resources for workshops using this process. Available infrastructure includes NSF Access VMs, CIROH-2i2c JupyterHub, AWS, and Google Cloud - accessible to all consortium members and partners.\n\n1. Complete our GitHub template with your workshop's technical requirements.\n2. Our team will process your request and ensure participants have necessary access before your workshop begins.\n\n[Workshop IT Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&projects=&template=workshop_IT_request.md)"
  },
  {
    "idurl": 87,
    "idtype": "text",
    "order": 17,
    "content": "- [Selecting a platform](https://docs.ciroh.org/docs/services/access/#selecting-a-platform)\n- [Accessing Public Cloud Services](https://docs.ciroh.org/docs/services/access/#accessing-public-cloud-services)\n  - [Requesting Project Access](https://docs.ciroh.org/docs/services/access/#requesting-project-access)\n  - [Cost of Use](https://docs.ciroh.org/docs/services/access/#cost-of-use)\n- [Accessing CIROH-2i2c JupyterHub](https://docs.ciroh.org/docs/services/access/#accessing-ciroh-2i2c-jupyterhub)\n  - [Requesting Project Access](https://docs.ciroh.org/docs/services/access/#requesting-project-access-1)\n  - [Requesting Individual Access](https://docs.ciroh.org/docs/services/access/#requesting-individual-access)\n  - [Requesting Custom Images](https://docs.ciroh.org/docs/services/access/#requesting-custom-images)\n- [Accessing On-Premises Infrastructure](https://docs.ciroh.org/docs/services/access/#accessing-on-premises-infrastructure)\n  - [Requesting Project Access](https://docs.ciroh.org/docs/services/access/#requesting-project-access-2)\n  - [Requesting Individual Access](https://docs.ciroh.org/docs/services/access/#requesting-individual-access-1)\n- [Accessing JetStream2 through CIROH](https://docs.ciroh.org/docs/services/access/#accessing-jetstream2-through-ciroh)\n- [Accessing NWM BigQuery API](https://docs.ciroh.org/docs/services/access/#accessing-nwm-bigquery-api)\n- [Requesting Infrastructure Support for Conferences and Programs](https://docs.ciroh.org/docs/services/access/#requesting-infrastructure-support-for-conferences-and-programs)"
  },
  {
    "idurl": 88,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Cloud Services\nsource: https://docs.ciroh.org/docs/services/cloudservices/\nscraped_date: 2025-01-31\n---\n\n**In tandem with the power of the public cloud**, our team of researchers, hydrologists, and engineers at CIROH is committed to advancing our understanding of hydrologic processes, improving operational hydrologic forecasting techniques and workflows, collaborating on community water modeling, converting forecasts into practical solutions, and utilizing water predictions to help guide decision-making processes.\n\n**By leveraging the scalability and flexibility of public cloud platforms like AWS and Google Cloud**, CIROH Cloud empowers our team to conduct groundbreaking research in hydrology. This translates into a robust and efficient computing environment that accelerates discovery and innovation.\n\n![Cloud Services](https://docs.ciroh.org/img/graphics/public_cloud_services.jpeg)\n\n* * *"
  },
  {
    "idurl": 88,
    "idtype": "text",
    "order": 2,
    "content": "[**üóÉÔ∏èCIROH AWS Account** \\\\\n2 items](https://docs.ciroh.org/docs/services/cloudservices/aws/)[**üìÑÔ∏èCIROH Google Account** \\\\\nGoogle Research Cloud](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)[**üóÉÔ∏èCIROH JupyterHub** \\\\\n1 item](https://docs.ciroh.org/docs/services/cloudservices/2i2c/)[**üìÑÔ∏èHydroShare and CIROH JupyterHub Integration** \\\\\nHydroShare and CIROH JupyterHub Integration](https://docs.ciroh.org/docs/services/cloudservices/HydroShare/)[**üìÑÔ∏èCUAHSI JupyterHub** \\\\\ncuahsi jupyterhub](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/)[**üìÑÔ∏èCloud Services Blogs** \\\\\nWant to keep up with the latest updates in the cloud? If so, consider checking out the block pages for CIROH's cloud services partners.](https://docs.ciroh.org/docs/services/cloudservices/cloudserviceblogs)"
  },
  {
    "idurl": 89,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: On-Premises Services\nsource: https://docs.ciroh.org/docs/services/on-prem\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 89,
    "idtype": "text",
    "order": 2,
    "content": "## What is On-Premises services?\n\nAt CIROH, On-Premises services establish a comprehensive platform that facilitates the exchange of research data and access to computational resources and enables collaborative partnerships with academic peers locally and globally. A team of engineers and developers at the University of Alabama operates these services. Our overarching objective is to construct a dynamic \"network of services\" tailored to enable efficient organization, analysis, and dissemination of research data. Within On-Premises Research Computing, we structure our services around the following fundamental domains:"
  },
  {
    "idurl": 89,
    "idtype": "text",
    "order": 3,
    "content": "- **Hydroinformatics:** This domain integrates hydrology with information technology, focusing on the development and application of computational tools and techniques for data management, analysis, modeling, and decision support in hydrological studies.\n- **Data Science and Big Data:** Utilizing data-driven approaches and big data analytics to process and analyze large volumes of hydrological data from various sources, including remote sensing, sensors, and numerical models, to extract meaningful insights and patterns.\n- **Numerical Modeling and Simulation:** Developing and implementing computational models and simulation techniques to simulate hydrological processes and phenomena, such as rainfall-runoff modeling, groundwater flow, and water quality modeling, to support scientific research and water resources management.\n- **Geographic Information Systems (GIS):** Applying GIS technology to hydrological research by integrating spatial data with hydrological models, analyzing spatial patterns, and visualizing hydrological processes to understand spatial relationships and make informed decisions in water management.\n- **Machine Learning and Artificial Intelligence:** Employing machine learning algorithms and AI techniques to enhance hydrological modeling, prediction, and decision-making by learning from data patterns, optimizing model parameters, and improving the accuracy of hydrological forecasts and simulations."
  },
  {
    "idurl": 89,
    "idtype": "text",
    "order": 4,
    "content": "These domains reflect the interdisciplinary nature of hydrology and highlight the critical role of On-Premises High-performance computing (HPC) in advancing our understanding of water systems and addressing complex hydrological challenges.\n\nIn essence, CIROH's Research Computing services are a cornerstone for fostering interdisciplinary collaboration, enabling data-driven research, and advancing scientific discovery in hydrology and related domains."
  },
  {
    "idurl": 89,
    "idtype": "text",
    "order": 5,
    "content": "## Available clusters\n\n* * *\n\n[**üóÉÔ∏èPantarhei** \\\\\n4 items](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/)[**üóÉÔ∏èWukong** \\\\\n3 items](https://docs.ciroh.org/docs/services/on-prem/Wukong/)\n\n- [What is On-Premises services?](https://docs.ciroh.org/docs/services/on-prem/#what-is-on-premises-services)\n- [Available clusters](https://docs.ciroh.org/docs/services/on-prem/#available-clusters)"
  },
  {
    "idurl": 90,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: External Resources\nsource: https://docs.ciroh.org/docs/services/external-resources\nscraped_date: 2025-01-31\n---\n\nCIROH recognizes the importance of leveraging external resources to enhance our research capabilities. This section provides information on external computing resources available to CIROH consortium members. Explore options beyond our on-premise and cloud-based services to expand your research horizons.\n\n[Learn about NSF Access](https://access-ci.org/)\n\n**ACCESS** is a program established and funded by the National Science Foundation to help researchers and educators, with or without supporting grants, to utilize the nation's advanced computing systems and services ‚Äì at no cost."
  },
  {
    "idurl": 90,
    "idtype": "text",
    "order": 2,
    "content": "# ACCESS Resources"
  },
  {
    "idurl": 90,
    "idtype": "text",
    "order": 3,
    "content": "## ACCESS offers computing and storage resources free of charge to researchers and educators.\n\n[About ACCESS Resources](https://allocations.access-ci.org/resources)"
  },
  {
    "idurl": 91,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Subdomain Request\nsource: https://docs.ciroh.org/docs/services/subdomain\nscraped_date: 2025-01-31\n---\n\nTo request a ciroh.org subdomain, please fill out the following form.\n\n[CIROH Subdomain Request Form](https://github.com/CIROH-UA/ciroh-ua.github.io/issues/new?assignees=&labels=&projects=&template=subdomain-request.md&title=)\n\n* * *\n\nnote\n\nYour request will be handled by _DevOps staff at CIROH_ and if permitted will be given access to the **research cloud** or **on-premise** infrastructure."
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH AWS Account\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws\nscraped_date: 2025-01-31\n---\n\nCIROH Cloud leverages the power of AWS to empower researchers and unlock groundbreaking advancements in hydrology. CIROH provides access to enterprise-level AWS cloud platform to researchers.\n\n![AWS](https://blog.adobe.com/en/publish/2021/08/31/media_1649ebc3fbbce0df508081913819d491fc3f7c7a9.png?width=750&format=png&optimize=medium)\n\n[AWS Cloud Services Homepage](https://aws.amazon.com/)"
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 2,
    "content": "## Unleashing Research Potential with AWS Cloud Services\n\nHere's how AWS empowers your research:\n\n- **Enhanced Data Accessibility and Analysis:** AWS provides scalable storage and computing resources, allowing researchers to readily access, analyze, and manipulate vast datasets efficiently.\n\n- **Specialized Solutions at Your Fingertips:** The AWS Marketplace offers a wealth of pre-built solutions and tools specifically designed for hydrological research. This eliminates the need for time-consuming development and allows researchers to focus on scientific discovery.\n\n- **World-Class IT Infrastructure for Research Excellence:** AWS offers a robust and secure cloud infrastructure that delivers the best possible IT foundation for your research projects. This translates to increased efficiency, reduced costs, and faster time-to-results.\n\n- **Accelerated Research Timelines:** By leveraging the on-demand scalability and elasticity of AWS, researchers can dynamically scale their computing resources to meet the specific needs of their projects. This translates to faster analysis and completion of research endeavors.\n\n* * *"
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 3,
    "content": "## Requesting CIROH AWS Accounts\n\nCIROH Cloud Hosting services include:\n\n- Creation of AWS subaccounts for CIROH consortium members and partners.\n- Project PI contact identity creation and access (AWS IAM)\n\nCIROH Consortium members or partners are responsible for:\n\n- Management of CIROH subaccounts assigned to them.\n- Project-specific software and environment configuration.\n- Handling account creation and/or access for project contacts\nTo get started, please head to the \"Accessing Public Cloud Services\" section of the Infrastructure Access page.\n\n[Infrastructure Access](https://docs.ciroh.org/docs/services/access#accessing-public-cloud-services)\n\nnote\n\nPlease refer to [this link](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues?q=is%3Aissue+is%3Aclosed+label%3Aaws) for references to submitted forms.\n\n* * *"
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 4,
    "content": "## Cost of Use\n\n- Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.\n- CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.\n\n[Exceeding Budget Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=exceeding_budget_request.md&title=)\n\n* * *"
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 5,
    "content": "## AWS News Blog\n\nStay up-to-date on the latest AWS news and announcements by visiting the official AWS News Blog:\n\n[AWS News Blog](https://aws.amazon.com/blogs/aws/)\n\n* * *"
  },
  {
    "idurl": 92,
    "idtype": "text",
    "order": 6,
    "content": "## Help and Support\n\n- Email UA's CIROH Cloud Team: [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu)\n- Message the CIROH Cloud Slack Channel: #ciroh-ua-it-admin\n- Message the CIROH AWS Support Slack Channel: #aws-ciroh-support\n\n* * *\n\n[**üìÑÔ∏èCIROH AWS Office Hours** \\\\\nCIROH AWS Office Hours](https://docs.ciroh.org/docs/services/cloudservices/aws/officehours)[**üóÉÔ∏èDocumentation and Tutorial** \\\\\n3 items](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/)\n\n- [Unleashing Research Potential with AWS Cloud Services](https://docs.ciroh.org/docs/services/cloudservices/aws/#unleashing-research-potential-with-aws-cloud-services)\n- [Requesting CIROH AWS Accounts](https://docs.ciroh.org/docs/services/cloudservices/aws/#requesting-ciroh-aws-accounts)\n- [Cost of Use](https://docs.ciroh.org/docs/services/cloudservices/aws/#cost-of-use)\n- [AWS News Blog](https://docs.ciroh.org/docs/services/cloudservices/aws/#aws-news-blog)\n- [Help and Support](https://docs.ciroh.org/docs/services/cloudservices/aws/#help-and-support)"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Google Account\nsource: https://docs.ciroh.org/docs/services/cloudservices/google-cloud\nscraped_date: 2025-01-31\n---\n\nCIROH Cloud leverages the power of Google Cloud to empower researchers and unlock groundbreaking advancements in hydrology. CIROH provides access to enterprise-level Google cloud platform to researchers.\n\n![Google Cloud](https://lh3.googleusercontent.com/VEnnK2SyklusfxZ3dIYjlQH3xSwK2BFSJ69TFQ9g8HjM6m3CouRlTia5FW3z3GS0x83WC9TylZCaA9Jf_2kmr7mXxI9_HYLZTFy_bg)\n\n[Google Cloud Homepage](https://cloud.google.com/)"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 2,
    "content": "## Unleashing Research Potential with Google Cloud Services\n\nHere's some services and tools offered by Google Cloud:\n\n- **Compute Engine:** It is a highly scalable computing and hosting service that provides on-demand, high-performance computing resources. It lets you create and run virtual machines on Google infrastructure.\n\n- **Cloud Storage:** Google Cloud provides fast, low-cost, highly durable archive and backup storage, allowing researchers to readily access, analyze, and manipulate vast datasets efficiently.\n\n- **BigQuery:** Using its serverless architecture, researchers can use SQL queries to analyze huge datasets. It lets you manage all data types across clouds with fine-grained access controls.\n\n- **Google Earth Engine:** It is a cloud-based geospatial analysis platform to analyze earth observation data. Google Cloud offers Earth Engine for remote sensing research, predicting desease outbreaks, natural resource management, and more.\n\n* * *"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 3,
    "content": "## Requesting CIROH Google Cloud Accounts\n\nCIROH Cloud Hosting services include:\n\n- Creation of Google Cloud subaccounts for CIROH consortium members and partners.\n- Project PI contact identity creation and access (Google IAM)\n\nTo get started, please head to the \"Accessing Public Cloud Services\" section of the Infrastructure Access page.\n\n[Infrastructure Access](https://docs.ciroh.org/docs/services/access#accessing-public-cloud-services)\n\nnote\n\nPlease refer to [this link](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues?q=is:issue+is:closed+label:google) for references to submitted forms.\n\n* * *"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 4,
    "content": "## Cost of Use\n\n- Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.\n- CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.\n\n[Exceeding Budget Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=exceeding_budget_request.md&title=)\n\n* * *"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 5,
    "content": "## Google Cloud Blog\n\nStay up-to-date on the latest Google Cloud news and announcements by visiting the official Google Cloud Blog:\n\n[Google Cloud Blog](https://cloud.google.com/blog)\n\n* * *"
  },
  {
    "idurl": 93,
    "idtype": "text",
    "order": 6,
    "content": "## Help and Support\n\n- Email UA's CIROH Cloud Team: [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu)\n- Message the CIROH Cloud Slack Channel: #ciroh-ua-it-admin\n\n- [Unleashing Research Potential with Google Cloud Services](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/#unleashing-research-potential-with-google-cloud-services)\n- [Requesting CIROH Google Cloud Accounts](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/#requesting-ciroh-google-cloud-accounts)\n- [Cost of Use](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/#cost-of-use)\n- [Google Cloud Blog](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/#google-cloud-blog)\n- [Help and Support](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/#help-and-support)"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH JupyterHub\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 2,
    "content": "## Powered by 2i2c JupyterHub on Google Cloud\n\nCIROH, in collaboration with 2i2c, offers a dedicated JupyterHub environment on Google Cloud specifically designed for hydrological researchers. 2i2c is a cloud service provider specializing in open-source infrastructure for research and development.\n\n![2i2c Image](https://docs.ciroh.org/img/2i2c.png)"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 3,
    "content": "## Video Tutorial\n\nWatch this video to find out how to access CIROH-2i2c Jupyterhub and how to launch and use CIROH-2i2c Jupyterhub with HydroShare:\n\nCIROH Jupyterhub - YouTube\n\n[Photo image of CUAHSI](https://www.youtube.com/channel/UCybBLl24p1L68lW_gK-UdYQ?embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F)\n\nCUAHSI\n\n2.08K subscribers\n\n[CIROH Jupyterhub](https://www.youtube.com/watch?v=DnbxhLdb6TM)\n\nCUAHSI\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n0:00\n\n0:00 / 3:24\n‚Ä¢Live\n\n‚Ä¢"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 4,
    "content": "## Benefits of CIROH-2i2c JupyterHub:\n\n- **Managed JupyterHub as a Service:** CIROH Cloud takes care of the entire JupyterHub infrastructure, allowing researchers to focus on their scientific endeavors.\n\n- **Open Source Powerhouse:** Built on open-source tools, 2i2c JupyterHub offers flexibility, scalability, and a collaborative environment that fosters research advancement.\n\n- **Leveraging Google Cloud:** 2i2c utilizes Google Cloud's robust infrastructure to deliver a powerful and reliable platform for your computational needs.\n\n* * *"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 5,
    "content": "## Requesting Access to CIROH-2i2c JupyterHub\n\nCIROH JupyterHub provides both CPU and GPU capabilities. To get started, please head to the Infrastructure Access page.\n\n[Infrastructure Access](https://docs.ciroh.org/docs/services/access#accessing-ciroh-2i2c-jupyterhub)\n\nnote\n\nIf you are participating in an event that uses the workshop environment, ask your workshop coordinator for information on logging in.\n\n* * *"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 6,
    "content": "## Requesting Software Installation on CIROH-2i2c JupyterHub\n\nBefore making a request, please refer to the [Dockerfile](https://github.com/2i2c-org/awi-ciroh-image/blob/main/Dockerfile) for the list of software currently deployed on CIROH JupyterHub.\n\nIf your software in not listed in this file, please submit the form below to request new software installation on 2i2c JupyterHub.\n\n[JupyterHub (2i2c) Software Install Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u)\n\n* * *"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 7,
    "content": "## CIROH-2i2c JupyterHub Environments\n\nClick on the buttons below to access the CIROH-2i2c JupyterHub environments.\n\n> _Note that the workshop environment is only active for the duration of the conferences and programs it supports._\n\n[CIROH Production JupyterHub](https://ciroh.awi.2i2c.cloud/hub/login) [CIROH Staging JupyterHub](https://staging.ciroh.awi.2i2c.cloud/hub/login) [CIROH Workshop JupyterHub](https://workshop.ciroh.awi.2i2c.cloud/hub/login)\n\n* * *\n\n**Please remember to stop the server when you're not actively using it.**\n\nYouTube\n\n* * *\n\n![2i2c Image](https://docs.ciroh.org/img/2i2c-1.png)"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 8,
    "content": "### Server Options\n\n- Small - 5GB RAM, 2 CPUs\n\n- Medium - 11GB RAM, 4 CPUs\n\n- Large - 24GB RAM, 8 CPUs\n\n- Huge - 52GB RAM, 16 CPUs\n\nnote\n\nThe per user storage quota is currently set to 250 GB."
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 9,
    "content": "### Software currently deployed on CIROH JupyterHub\n\nPlease refer to the [Dockerfile](https://github.com/2i2c-org/awi-ciroh-image/blob/main/Dockerfile) for the list of software currently deployed on CIROH JupyterHub."
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 10,
    "content": "### Cost of Use\n\nCIROH 2i2c JupyterHub is free to use for consortium members. Its cost is covered by CIROH Infrastructure project funds.\n\n* * *"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 11,
    "content": "## 2i2c Blog\n\nStay up-to-date on the latest 2i2c news and announcements by visiting the official 2i2c Blog:\n\n[2i2c Blog](https://2i2c.org/blog/)\n\n* * *"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 12,
    "content": "## Help and Support\n\n- Email UA's CIROH Cloud Team: [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu)\n- Message the CIROH Cloud Slack Channel: #ciroh-ua-it-admin"
  },
  {
    "idurl": 94,
    "idtype": "text",
    "order": 13,
    "content": "- [Powered by 2i2c JupyterHub on Google Cloud](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#powered-by-2i2c-jupyterhub-on-google-cloud)\n- [Video Tutorial](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#video-tutorial)\n- [Benefits of CIROH-2i2c JupyterHub:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#benefits-of-ciroh-2i2c-jupyterhub)\n- [Requesting Access to CIROH-2i2c JupyterHub](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#requesting-access-to-ciroh-2i2c-jupyterhub)\n- [Requesting Software Installation on CIROH-2i2c JupyterHub](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#requesting-software-installation-on-ciroh-2i2c-jupyterhub)\n- [CIROH-2i2c JupyterHub Environments](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#ciroh-2i2c-jupyterhub-environments)\n  - [Server Options](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#server-options)\n  - [Software currently deployed on CIROH JupyterHub](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#software-currently-deployed-on-ciroh-jupyterhub)\n  - [Cost of Use](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#cost-of-use)\n- [2i2c Blog](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#2i2c-blog)\n- [Help and Support](https://docs.ciroh.org/docs/services/cloudservices/2i2c/#help-and-support)"
  },
  {
    "idurl": 95,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: HydroShare and CIROH JupyterHub Integration\nsource: https://docs.ciroh.org/docs/services/cloudservices/HydroShare\nscraped_date: 2025-01-31\n---\n\nHydroShare is a repository, website, and hydrologic information system for sharing hydrologic data and models aimed at giving users the cyberinfrastructure needed to innovate and collaborate in research to solve water problems.\n\n[HydroShare Homepage](https://www.hydroshare.org/)"
  },
  {
    "idurl": 95,
    "idtype": "text",
    "order": 2,
    "content": "## HydroShare and CIROH JupyterHub Integration\n\nUsers now have the capability to directly launch and execute computational notebooks from HydroShare resources into the CIROH Jupyterhub environments. Here's how to get started:\n\n1. First, confirm that you have access to the CIROH Jupyterhub. If not, follow [these steps](https://docs.ciroh.org/docs/services/access#accessing-ciroh-2i2c-jupyterhub)\n\n2. CIROH Jupyterhub is an approved app, and appears on [https://www.hydroshare.org/apps/](https://www.hydroshare.org/apps/). Navigate to this page to access it directly, or select it from the \"Open with\" list on any resource that you have access to containing a Jupyter notebook.\n\n3. In CIROH JupyterHub, click on the \"Login to continue\" button. Select one of the server options that is appropriate for the analysis you need to run (small, medium, large, or huge), as well as the \"New Pangeo Notebook\" base image. Then, click the \"Start\" button to launch your server.\n\n![HydroShare 2i2c Image](https://docs.ciroh.org/img/hydroshare.png)\n\n![HydroShare 2i2c Image](https://docs.ciroh.org/img/hydroshare-1.png)\n\n* * *\n\n4. You will now be inside the CIROH JupyterHub. All of the files from your HydroShare resource will appear in the file browser on the left, including any notebooks that were in your resource. Double click on a notebook to open it and then run it.\n\n![HydroShare 2i2c Image](https://docs.ciroh.org/img/hydroshare-2.png)"
  },
  {
    "idurl": 95,
    "idtype": "text",
    "order": 3,
    "content": "## Video Tutorial\n\nWatch this video to find out how to access CIROH Jupyterhub and how to launch and use CIROH Jupyterhub with HydroShare:\n\nCIROH Jupyterhub - YouTube\n\n[Photo image of CUAHSI](https://www.youtube.com/channel/UCybBLl24p1L68lW_gK-UdYQ?embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F)\n\nCUAHSI\n\n2.08K subscribers\n\n[CIROH Jupyterhub](https://www.youtube.com/watch?v=DnbxhLdb6TM)\n\nCUAHSI\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)\n\nMore videos"
  },
  {
    "idurl": 95,
    "idtype": "text",
    "order": 4,
    "content": "## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?v=DnbxhLdb6TM&embeds_referring_euri=https%3A%2F%2Fdocs.ciroh.org%2F)\n\n0:00\n\n0:00 / 3:24\n‚Ä¢Live\n\n‚Ä¢\n\ninfo\n\nTo learn more about CIROH Jupyterhub and gain access go to: [CIROH JupyterHub Documentation](https://docs.ciroh.org/docs/services/cloudservices/2i2c/)\n\n    To join a group and gain access to the CIROH HydroShare community go to: [HydroShare Communities](https://www.hydroshare.org/communities/)\n\n- [HydroShare and CIROH JupyterHub Integration](https://docs.ciroh.org/docs/services/cloudservices/HydroShare/#hydroshare-and-ciroh-jupyterhub-integration)\n- [Video Tutorial](https://docs.ciroh.org/docs/services/cloudservices/HydroShare/#video-tutorial)"
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CUAHSI JupyterHub\nsource: https://docs.ciroh.org/docs/services/cloudservices/cuahsi\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 2,
    "content": "## An Introduction\n\nThe CUAHSI JupyterHub is a free cloud computing environment that enables researchers to execute scientific code and explore, modify, and interact with data inside a remote execution environment using Python and/or R programming languages. It is integrated with HydroShare and the Hydrologic Information System data repositories, making it easy to leverage community datasets, collaborate, and disseminate research workflows.\n\n> **NOTE:** Below content is taken from [https://github.com/CUAHSI/jupyterhub/blob/main/docs/getting-started.md](https://github.com/CUAHSI/jupyterhub/blob/main/docs/getting-started.md)"
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 3,
    "content": "# Getting Started\n\n* * *"
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 4,
    "content": "## Access\n\nTo access the CUAHSI JupyterHub platform, you must be a member of the CUAHSI JupyterHub Group. Group membership limits system interruptions and ensures that resources are effectively curated and managed. When first accessing the application, you will be directed to the CUAHSI JupyterHub Group landing page. Request to join the group, and after admission has been granted you will be able to access the computational environment. To expedite the approval process, please ensure that your HydroShare user profile is complete and up-to-date. Contact [help@cuahsi.org](mailto:help@cuahsi.org) if you have any questions regarding this process."
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 5,
    "content": "## Launching JupyterHub\n\nThere are multiple ways to access the CUAHSI JupyterHub platform which are listed below. All of these methods require that you register your HydroShare account with the CUAHSI JupyterHub Group (see :Access\" above)."
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 6,
    "content": "### HydroShare Web Application\n\nThe simplest way to get started with the CUAHSI JupyterHub is by launching it directly from the HydroShare Apps library ( [hydroshare.org/apps](https://hydroshare.org/apps)) by clicking on the CUAHSI JupyterHub icon. This redirect you to the CUAHSI JupyterHub server where you will be asked to login using your HydroShare credentials. Once authenticated, you will be asked to choose a [Profile Environment](https://github.com/CUAHSI/jupyterhub/blob/main/docs/getting-started.md) which will be used to launch an isolated cloud computing environment for you to work in.\n\n![HydroShare Apps Library](https://docs.ciroh.org/assets/images/hsapps-library-53eb4199e4b44a12a80378335a2f1523.png)\n\nIn this space, you can create files and execute code from within your web browser. Any data you upload, download, and create is associated with your HydroShare account and will persist between sessions, meaning that it will be there next time you log in. Prior to gaining access, you will be asked join the CUAHSI JupyterHub HydroShare group (see the Access and Authentication section for details)."
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 7,
    "content": "### HydroShare Open-With Menu\n\nAnother common way of accessing the CUAHSI JupyterHub environment is by using the HydroShare `Open with` functionality. This button can be found in the top right corner of any HydroShare resource landing page. After selecting \"CUAHSI JupyterHub\", a computing environment will be prepared and the content of the current HydroShare resource will be placed inside of it. This is a convenient method for executing code, data, and workflows that have been published in the HydroShare repository.\n\n![HydroShare Open With](https://docs.ciroh.org/assets/images/hsapps-open-with-d8443c3e7f5e78719e5529f68e0b110f.png)"
  },
  {
    "idurl": 96,
    "idtype": "text",
    "order": 8,
    "content": "### Direct URL\n\nOnce you are familiar with the this environment, it's often useful to access it directly rather than navigating through HydroShare. This can be done by simply navigating to [https://jupyterhub.cuahsi.org](https://jupyterhub.cuahsi.org/).\n\n- [An Introduction](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#an-introduction)\n- [Access](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#access)\n- [Launching JupyterHub](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#launching-jupyterhub)\n  - [HydroShare Web Application](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#hydroshare-web-application)\n  - [HydroShare Open-With Menu](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#hydroshare-open-with-menu)\n  - [Direct URL](https://docs.ciroh.org/docs/services/cloudservices/cuahsi/#direct-url)"
  },
  {
    "idurl": 97,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Cloud Services Blogs\nsource: https://docs.ciroh.org/docs/services/cloudservices/cloudserviceblogs\nscraped_date: 2025-01-31\n---\n\nWant to keep up with the latest updates in the cloud? If so, consider checking out the block pages for CIROH's cloud services partners.\n\nHere, you'll find:\n\n- Product launches and updates\n- Customer success stories\n- Industry insights\n- Technical deep dives\n- And much more!\n\n* * *"
  },
  {
    "idurl": 97,
    "idtype": "text",
    "order": 2,
    "content": "## AWS News Blog\n\n![AWS](https://blog.adobe.com/en/publish/2021/08/31/media_1649ebc3fbbce0df508081913819d491fc3f7c7a9.png?width=750&format=png&optimize=medium)\n\nStay up-to-date on the latest AWS news and announcements by visiting the official AWS News Blog:\n\n[AWS News Blog](https://aws.amazon.com/blogs/aws/)\n\n* * *"
  },
  {
    "idurl": 97,
    "idtype": "text",
    "order": 3,
    "content": "## Google Cloud Blog\n\n![AWS](https://lh3.googleusercontent.com/VEnnK2SyklusfxZ3dIYjlQH3xSwK2BFSJ69TFQ9g8HjM6m3CouRlTia5FW3z3GS0x83WC9TylZCaA9Jf_2kmr7mXxI9_HYLZTFy_bg)\n\nStay up-to-date on the latest Google Cloud news and updates by visiting the official Google Cloud Blog:\n\n[Google Cloud Blog](https://cloud.google.com/blog)\n\n* * *"
  },
  {
    "idurl": 97,
    "idtype": "text",
    "order": 4,
    "content": "## 2i2c Blog\n\n![AWS](https://2i2c.org/media/logo.svg)\n\nStay up-to-date on the latest 2i2c news and announcements by visiting the official 2i2c Blog:\n\n[2i2c Blog](https://2i2c.org/blog/)\n\n- [AWS News Blog](https://docs.ciroh.org/docs/services/cloudservices/cloudserviceblogs/#aws-news-blog)\n- [Google Cloud Blog](https://docs.ciroh.org/docs/services/cloudservices/cloudserviceblogs/#google-cloud-blog)\n- [2i2c Blog](https://docs.ciroh.org/docs/services/cloudservices/cloudserviceblogs/#2i2c-blog)"
  },
  {
    "idurl": 98,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Pantarhei\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei\nscraped_date: 2025-01-31\n---\n\nPantarhei serves as an analytical and computational resource accessible to the research community of the Cooperative Institute for Research to Operations in Hydrology (CIROH). This cluster is managed by the CIROH IT Computing group at the University of Alabama.\n\nUse of this resource is governed by the University of Alabama Acceptable Use Policy for Computer and Network Resources, Information Classification Policy, and Information Protection Procedure. Please review these policies on-line:\n\n1. [Computer and Network Resources](https://oit.ua.edu/about/policies/it-use-guideline-computer-and-network-use/)\n2. [Information Classification Policy](https://ua-public.policystat.com/policy/14809337/latest/)\n3. [Information Protection Procedure](https://oit.ua.edu/internaldocs/20210205-UA%20Information%20Protection%20Procedures.pdf)\n\n* * *"
  },
  {
    "idurl": 98,
    "idtype": "text",
    "order": 2,
    "content": "### Navigational Resources\n\n[**üìÑÔ∏èSystem Architecture** \\\\\nSystem Architecture of Pantarhei](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/sysinfo)[**üìÑÔ∏èObtaining an Account** \\\\\nObtain an account on Pantarhei](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/obtain)[**üìÑÔ∏èAccessing the System** \\\\\nAccess of On-Premises Cluster Pantarhei](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access)[**üóÉÔ∏èRunning Jobs** \\\\\n1 item](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/RunningJobs/)\n\n- [Navigational Resources](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/#navigational-resources)"
  },
  {
    "idurl": 99,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Wukong\nsource: https://docs.ciroh.org/docs/services/on-prem/Wukong\nscraped_date: 2025-01-31\n---\n\nWukong serves as an analytical and computational resource accessible to the research\ncommunity of the Cooperative Institute for Research to Operations in Hydrology (CIROH).\nThis cluster is managed by the CIROH IT Computing group at the University of Alabama.\n\nUse of this resource is governed by the University of Alabama Acceptable Use Policy for\nComputer and Network Resources, Information Classification Policy, and Information\nProtection Procedure. Please review these policies on-line:\n\n1. [Computer and Network Resources](https://oit.ua.edu/about/policies/it-use-guideline-computer-and-network-use/)\n2. [Information Classification Policy](https://ua-public.policystat.com/policy/14809337/latest/)\n3. [Information Protection Procedure](https://oit.ua.edu/internaldocs/20210205-UA%20Information%20Protection%20Procedures.pdf)\n\n* * *"
  },
  {
    "idurl": 99,
    "idtype": "text",
    "order": 2,
    "content": "### Navigational Resources\n\n[**üìÑÔ∏èSystem Architecture** \\\\\nSystem Architecture of Wukong](https://docs.ciroh.org/docs/services/on-prem/Wukong/sysinfo)[**üìÑÔ∏èObtaining an Account** \\\\\nObtain an account on Wukong](https://docs.ciroh.org/docs/services/on-prem/Wukong/obtain)[**üìÑÔ∏èAccessing the System** \\\\\nAccess of On-Premises Cluster Wukong](https://docs.ciroh.org/docs/services/on-prem/Wukong/access)\n\n- [Navigational Resources](https://docs.ciroh.org/docs/services/on-prem/Wukong/#navigational-resources)"
  },
  {
    "idurl": 100,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NSF Access\nsource: https://docs.ciroh.org/docs/services/external-resources/nsf-access\nscraped_date: 2025-01-31\n---\n\n**ACCESS** is an advanced computing and data resource program supported by the U.S. National Science Foundation (NSF). Please refer to [https://allocations.access-ci.org/](https://allocations.access-ci.org/) for more information on how to get access to NSF ACCESS resources.\n\nnote\n\nAllocations are absolutely free of cost and you do not need NSF, or funding from any agency to receive one.\n\nHere are some useful links to get started:\n\n- [About Access Allocations](https://allocations.access-ci.org/get-involved)\n- [Get your first project](https://allocations.access-ci.org/get-your-first-project)\n- [Supported Resources](https://allocations.access-ci.org/resources)"
  },
  {
    "idurl": 100,
    "idtype": "text",
    "order": 2,
    "content": "# Recommended Resources for CIROH projects:\n\n![JetStream2 logo](https://docs.jetstream-cloud.org/images/JS2-Logo-Transparent.png)[JetStream2](https://docs.ciroh.org/docs/services/external-resources/nsf-access/jetstream/)\n\n![Anvil logo and researchers](https://www.rcac.purdue.edu/files/anvil/Aba-exec.jpg)[Anvil](https://docs.ciroh.org/docs/services/external-resources/nsf-access/anvil/)\n\n![NSF NCAR Derecho logo](https://ncar-hpc-docs-arc-iframe.readthedocs.io/compute-systems/derecho/media/Derecho_Logo_Landscape_NCARBlue.svg#only-light)[NSF NCAR Derecho](https://docs.ciroh.org/docs/services/external-resources/nsf-access/derecho/)"
  },
  {
    "idurl": 101,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AWS Office Hours\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws/officehours\nscraped_date: 2025-01-31\n---\n\nScheduled the monthly CIROH AWS Office Hour sessions, an opportunity for discussing AWS-related inquiries with direct response from AWS experts. These sessions cover various topics, AWS services, projects, and other topics of interest. AWS technical staff is available to address the community's AWS questions and have discussions around best practices. We encourage CIROH members to participate to learn more about how to effectively leverage AWS tools and resources for their projects and share knowledge and experience with fellow CIROH members.\n\ninfo\n\nEmail: [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) to subscribe to monthly CIROH Office Hour Sessions."
  },
  {
    "idurl": 102,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AWS Documentation\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws/documentation\nscraped_date: 2025-01-31\n---\n\n[**üìÑÔ∏èAWS Best Practices** \\\\\nAs the main account administrator for CIROH subaccount, here are some best practices to follow within your subaccount:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/)[**üìÑÔ∏èTag Resources on AWS** \\\\\nAWS tags for cost tracking](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/tagging/)[**üìÑÔ∏èAWS Data Science Tools** \\\\\nAWS Data Science Tools](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/data-science-tools/)"
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: 2i2c JupyterHub Documentation\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 2,
    "content": "### 2i2c JupyterHub details:\n\ninfo\n\n[2i2c JupyterHub](https://2i2c.org/)"
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 3,
    "content": "### 2i2c JupyterHub Documentation and Tutorials:\n\ninfo\n\n[2i2c Docs](https://docs.2i2c.org/)"
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 4,
    "content": "### 2i2c JupyterHub Infrastructure Documentation:\n\ninfo\n\n[2i2c Infrastructure Docs](https://infrastructure.2i2c.org/)"
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 5,
    "content": "### awi-ciroh-image repository on CIROH GitHub:\n\ninfo\n\n[Github URL for AWI-CIROH 2i2c Hub Image](https://github.com/2i2c-org/awi-ciroh-image)\n\nnote\n\nFor any other questions, please email us at **[ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu)** with detailed information.\n\n* * * [**üìÑÔ∏èJupyterHub User Directory** \\\\\n2i2c JupyterHub File System](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/directory/)[**üìÑÔ∏èManage files in GCP bucket** \\\\\n2i2c JupyterHub Google Cloud Buckets](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/)[**üìÑÔ∏èPush and Pull to GitHub** \\\\\n2i2c JupyterHub is a cloud-based JupyterHub environment specifically designed for hydrological researchers. It is powered by 2i2c JupyterHub, a cloud-based JupyterHub environment specifically on Google Cloud](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/github-push/)[**üìÑÔ∏èRequest custom images** \\\\\n2i2c JupyterHub is a cloud-based JupyterHub environment specifically designed for hydrological researchers. It is powered by 2i2c JupyterHub, a cloud-based JupyterHub environment specifically on Google Cloud](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/custom-images/)[**üìÑÔ∏èPersistent Conda Environment** \\\\\n2i2c JupyterHub is a cloud-based JupyterHub environment specifically designed for hydrological researchers."
  },
  {
    "idurl": 103,
    "idtype": "text",
    "order": 6,
    "content": "It is powered by 2i2c JupyterHub, a cloud-based JupyterHub environment specifically on Google Cloud](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda/)[**üìÑÔ∏èPrevent Server Timeout** \\\\\nCurrently, user servers on JupyterHub stop after about an hour of inactivity. While this helps save costs, it can be problematic for long-running jobs if there is no active interaction with the notebook.](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/server-timeout/)[**üìÑÔ∏èPython Package Version Conflicts** \\\\\na tutorial guide to help debug python package conflicts](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/python-package-conflicts/)\n\n- [2i2c JupyterHub details:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/#2i2c-jupyterhub-details)\n- [2i2c JupyterHub Documentation and Tutorials:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/#2i2c-jupyterhub-documentation-and-tutorials)\n- [2i2c JupyterHub Infrastructure Documentation:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/#2i2c-jupyterhub-infrastructure-documentation)\n- [awi-ciroh-image repository on CIROH GitHub:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/#awi-ciroh-image-repository-on-ciroh-github)"
  },
  {
    "idurl": 104,
    "idtype": "table",
    "order": 1,
    "content": "| Pantarhei Login Node |\n| --- |\n| Number of nodes | Processors per node | Cores per node | Sockets per node | Memory per node | Local storage per node |\n| 1 | 2x Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz | 16 | 2 | 96 Gb | 76 TB |"
  },
  {
    "idurl": 104,
    "idtype": "text",
    "order": 2,
    "content": "---\ntitle: System Architecture\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei/sysinfo\nscraped_date: 2025-01-31\n---\n\n- Login Node\n- Compute Nodes\n- FPGA Nodes\n- GPU Nodes"
  },
  {
    "idurl": 104,
    "idtype": "text",
    "order": 3,
    "content": "### Network\n\nAll nodes are interconnected by a Mellanox InfiniBand switch with FDR 56 Gb/s networks."
  },
  {
    "idurl": 105,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Obtaining an Account\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei/obtain\nscraped_date: 2025-01-31\n---\n\nPantarhei cluster is accessible to the CIROH members and partners of the Cooperative Institute for Research to Operations in Hydrology (CIROH)."
  },
  {
    "idurl": 105,
    "idtype": "text",
    "order": 2,
    "content": "### How to get access to Pantarhei?\n\n1. Submit the On-Premise Infrastructure Request Form below to get access to Pantarhei:\n\nWe encourage PI of the project to start here: (select On-Premises Infrastructure Request Form and fill out details)\n\n[Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=)\n\n* * *\n\n2. Submit the On-premise Access Request form for individual user accounts on Pantarhei:\n\n[On-Premise Access Request Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUMzdTOUpKVU5UWFNCU0ZQUlowS0cxV0xFRy4u)\n\n* * *\n\nnote\n\nFor UA users, please submit On-Premises Access Request Form. For outside UA users, please start with VPN Access Request Form followed by On-Premises Access Request Form\n\n* * *\n\n**Note**: If you are unable to access the On-Premise forms, please contact the CIROH team at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) for assistance.\n\n- [How to get access to Pantarhei?](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/obtain/#how-to-get-access-to-pantarhei)"
  },
  {
    "idurl": 106,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Accessing the System\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access\nscraped_date: 2025-01-31\n---\n\nPantarhei curretly only supports the Secure Shell (SSH) mechanisms for logging in. The Secure Shell mechanism uses SSH keys. If you need help creating or uploading your SSH keys, please see the Managing SSH Public Keys page for that information."
  },
  {
    "idurl": 106,
    "idtype": "text",
    "order": 2,
    "content": "### General overview\n\nTo connect to Pantarhei using SSH, you must follow two high-level steps:\n\n- [Connect to the University of Alabama (UA) Network](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access/#connect-to-the-network)\n- [Connect to the Secure Shell (SSH)](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access/#connect-to-the-ssh)\n\nObtain Pantarhei Access\n\nIn the case that access to the Pantarhei system is unavailable to you, please follow the instructions on [Obtaining an Account](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/obtain)."
  },
  {
    "idurl": 106,
    "idtype": "text",
    "order": 3,
    "content": "### Connect to the Network\n\nUniversity of Alabama (UA) requires users to use the Virtual private network (VPN) to connect to the UA campus network in order to connect to the Pantarhei cluster.\n\ntip\n\nFor more information on setting up a VPN, please visit the [Office of Information Technology (OIT) website](https://oit.ua.edu/services/internet-networking/vpn/)."
  },
  {
    "idurl": 106,
    "idtype": "text",
    "order": 4,
    "content": "### Connect to the SSH\n\n- MacOS and Linux\n- Windows\n\nOnce you are connected to the VPN, follow these steps to access Pantarhei:\n\n1. **Open a Terminal:** Find `Terminal` in your local machine and open it.\n\ntip\n\nIn MacOS, use Spotlight search ( **Command** \\+ **Spacebar**) and type `Terminal` to open a new terminal window.\n\n2. **Connect via SSH:** In the terminal,\n   - Use the SSH command to connect to Pantarhei.\n\n     ```codeBlockLines_e6Vv\n     ssh <USERNAME>@pantarhei.ua.edu\n\n     ```\n\n     note\n\n     Replace `<USERNAME>` with your actual Pantarhei username.\n\n   - Enter your Pantarhei password\n\nWe hope this guide helps you efficiently utilize the Pantarhei HPC system for your research needs. Happy computing!\n\n- [General overview](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access/#general-overview)\n- [Connect to the Network](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access/#connect-to-the-network)\n- [Connect to the SSH](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/access/#connect-to-the-ssh)"
  },
  {
    "idurl": 107,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Running Jobs\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei/RunningJobs\nscraped_date: 2025-01-31\n---\n\nProficient users acquainted with the Linux command line interface have the option to utilize standard job submission utilities for the purpose of managing and executing tasks on the computational nodes within the Pantarhei system.\n\n* * *"
  },
  {
    "idurl": 107,
    "idtype": "text",
    "order": 2,
    "content": "### Navigational Resources\n\n[**üìÑÔ∏èAccessing the Compute Nodes** \\\\\nAccessing compute notes in Pantarhei](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/RunningJobs/computenode)\n\n- [Navigational Resources](https://docs.ciroh.org/docs/services/on-prem/Pantarhei/RunningJobs/#navigational-resources)"
  },
  {
    "idurl": 108,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: System Architecture\nsource: https://docs.ciroh.org/docs/services/on-prem/Wukong/sysinfo\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 108,
    "idtype": "table",
    "order": 2,
    "content": "| Compute Node Specifications |\n| --- |\n| Model | Intel(R) Xeon(R) Platinum 8470 |\n| Number of nodes | 1 |\n| Sockets per node | 2 |\n| Cores per socket | 52 |\n| Cores per node | 208 |\n| Hardware threads per core | 2 |\n| Hardware threads per node | 416 |\n| Clock rate | 2.00GHz (3.80GHz max boost) |\n| RAM | 1024 GB DDR5-4800 |\n| Cache | L1d cache: 4.9 MiB (104 instances)                   <br>L1i cache: 3.3 MiB (104 instances)<br>L2 cache: 208 MiB (104 instances)<br>L3 cache: 210 MiB (2 instances) |\n| Local storage per node | 56 TB |\n| Number GPUs per node | 8 |\n| GPU model | NVIDIA A100 SXM4 |\n| Memory per GPU | 80 GB |"
  },
  {
    "idurl": 108,
    "idtype": "text",
    "order": 3,
    "content": "### Comupute Node\n\n\n\n\ninfo\n\nPresently, the Wukong operates as a stand-alone, self-contained server, implying that the compute node is the login node."
  },
  {
    "idurl": 108,
    "idtype": "text",
    "order": 4,
    "content": "### Network\n\nThe Wukong's all GPUs are fully interconnected with NVIDIA NVLink technology.\n\n- [Comupute Node](https://docs.ciroh.org/docs/services/on-prem/Wukong/sysinfo/#comupute-node)\n- [Network](https://docs.ciroh.org/docs/services/on-prem/Wukong/sysinfo/#network)"
  },
  {
    "idurl": 109,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Obtaining an Account\nsource: https://docs.ciroh.org/docs/services/on-prem/Wukong/obtain\nscraped_date: 2025-01-31\n---\n\nWukong cluster is accessible to the research community of the Cooperative Institute for Research to Operations in Hydrology (CIROH)."
  },
  {
    "idurl": 109,
    "idtype": "text",
    "order": 2,
    "content": "### General overview\n\nTo obtain an account, users will need to follow these step:\n\n1. Submit [On-premises Infrastructure Request Form](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=) and describe full project information and resouce requirements.\n\nwarning\n\nThis GitHub issue must be submitted by Principal Investigator (PI) of the project.\n\nHint\n\nIn the GitHub issue PI should mention following details:\n\n1. PI's **Full Name**\n2. PI's **Affiliated Institute**\n3. PI's **Affiliated Email Address**\n4. List of students who will need access, also please provide following details of each student\n   - **Full Name** of student\n   - **GitHub User Name** of student\n   - **Affiliated Email Address** of student\n\n2. Submit On-premise Access Request form in [CIROH On-Premise Access Form](https://forms.office.com/r/hED4zGVACM).\n\ntip\n\nThe administration of the Wukong cluster falls under the purview of the CIROH IT Computing group at the University of Alabama (UA). Consequently, individuals lacking UA credentials ( _MyBama ID_, _VPN User Name_, and _CWID_) are obliged to complete the **`VPN Access Request`** form, followed by the **`On-premise Access Request`** form within [CIROH On-Premise Access Form](https://forms.office.com/r/hED4zGVACM).\n\nAccount Creation\n\nCreation of accounts on the Wukong system necessitates submission of individual forms by each respective user."
  },
  {
    "idurl": 109,
    "idtype": "text",
    "order": 3,
    "content": "For instance, a Principal Investigator (PI) submits a GitHub issue pertaining to a project, and three students require access to the Wukong system for project-related work, three separate account creation forms must be duly submitted during this process.\n\n- [General overview](https://docs.ciroh.org/docs/services/on-prem/Wukong/obtain/#general-overview)"
  },
  {
    "idurl": 110,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Accessing the System\nsource: https://docs.ciroh.org/docs/services/on-prem/Wukong/access\nscraped_date: 2025-01-31\n---\n\nWukong curretly only supports the Secure Shell (SSH) mechanisms for logging in. The Secure Shell mechanism uses SSH keys. If you need help creating or uploading your SSH keys, please see the Managing SSH Public Keys page for that information."
  },
  {
    "idurl": 110,
    "idtype": "text",
    "order": 2,
    "content": "### General overview\n\nTo connect to Wukong using SSH, you must follow two high-level steps:\n\n- [Connect to the University of Alabama (UA) Network](https://docs.ciroh.org/docs/services/on-prem/Wukong/access/#connect-to-the-network)\n- [Connect to the Secure Shell (SSH)](https://docs.ciroh.org/docs/services/on-prem/Wukong/access/#connect-to-the-ssh)\n\nObtain Wukong Access\n\nIn the case that access to the Wukong system is unavailable to you, please follow the instructions on [Obtaining an Account](https://docs.ciroh.org/docs/services/on-prem/Wukong/obtain)."
  },
  {
    "idurl": 110,
    "idtype": "text",
    "order": 3,
    "content": "### Connect to the Network\n\nUniversity of Alabama (UA) requires users to use the Virtual private network (VPN) to connect to the UA campus network in order to connect to the Wukong cluster.\n\ntip\n\nFor more information on setting up a VPN, please visit the [Office of Information Technology (OIT) website](https://oit.ua.edu/services/internet-networking/vpn/)."
  },
  {
    "idurl": 110,
    "idtype": "text",
    "order": 4,
    "content": "### Connect to the SSH\n\n- MacOS and Linux\n- Windows\n\nOnce you are connected to the VPN, follow these steps to access Wukong:\n\n1. **Open a Terminal:** Find `Terminal` in your local machine and open it.\n\ntip\n\nIn MacOS, use Spotlight search ( **Command** \\+ **Spacebar**) and type `Terminal` to open a new terminal window.\n\n2. **Connect via SSH:** In the terminal,\n   - Use the SSH command to connect to Wukong.\n\n     ```codeBlockLines_e6Vv\n     ssh <USERNAME>@Wukong.ua.edu\n\n     ```\n\n     note\n\n     Replace `<USERNAME>` with your actual Wukong username.\n\n   - Enter your Wukong password\n\nWe hope this guide helps you efficiently utilize the Wukong HPC system for your research needs. Happy computing!\n\n- [General overview](https://docs.ciroh.org/docs/services/on-prem/Wukong/access/#general-overview)\n- [Connect to the Network](https://docs.ciroh.org/docs/services/on-prem/Wukong/access/#connect-to-the-network)\n- [Connect to the SSH](https://docs.ciroh.org/docs/services/on-prem/Wukong/access/#connect-to-the-ssh)"
  },
  {
    "idurl": 111,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: JetStream2\nsource: https://docs.ciroh.org/docs/services/external-resources/nsf-access/jetstream\nscraped_date: 2025-01-31\n---\n\nJetStream2 is accessible to the CIROH members and partners of the Cooperative Institute for Research to Operations in Hydrology (CIROH).\n\nJetstream2 is a powerful hybrid-cloud platform designed for researchers and educators. It offers a range of flexible, on-demand, and programmable infrastructure tools, from interactive virtual machines (VMs) to advanced infrastructure and orchestration services. The primary resource consists of AMD Milan 7713 CPUs with 128 cores per node and 512 GB RAM per node, all connected by a high-speed 100 gbps wthernet.\n\n![JetStream2 logo](https://docs.jetstream-cloud.org/images/JS2-Logo-Transparent.png)_Image Source: [https://docs.jetstream-cloud.org/](https://docs.jetstream-cloud.org/)_"
  },
  {
    "idurl": 111,
    "idtype": "text",
    "order": 2,
    "content": "### Use cases:\n\nJetstream2 is ideal for researchers with diverse needs:\n\n- **On-demand virtual machines**: It is ideal for research that requires on demand virtual machine services. It is also best for researchers needing to create their own customized virtual machine environment for specific software needs.\n- **Always-on research infrastructure**: It can host research-supporting infrastructure services that require continuous operation.\n- **Educational support**: It can be used to provide virtual machines for student use in research or coursework.\n\nFor information about available instance sizes, visit the [JetStream2 VM Sizes](https://docs.jetstream-cloud.org/general/vmsizes/) page.\n\n* * *"
  },
  {
    "idurl": 111,
    "idtype": "text",
    "order": 3,
    "content": "### Gaining Access to JetStream2\n\nCIROH offers its researchers access to an allocation on JetStream2. To get started, visit the Infrastructure Access page below.\n\n[Infrastructure Access](https://docs.ciroh.org/docs/services/access#accessing-jetstream2-through-ciroh)\n\n* * *\n\ninfo\n\nFor a more detailed information on JetStream2, visit the official NSF ACCESS Jetstream2 website [here.](https://docs.jetstream-cloud.org/)\n\n- [Use cases:](https://docs.ciroh.org/docs/services/external-resources/nsf-access/jetstream/#use-cases)\n- [Gaining Access to JetStream2](https://docs.ciroh.org/docs/services/external-resources/nsf-access/jetstream/#gaining-access-to-jetstream2)"
  },
  {
    "idurl": 112,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Anvil\nsource: https://docs.ciroh.org/docs/services/external-resources/nsf-access/anvil\nscraped_date: 2025-01-31\n---\n\nAnvil is a powerful supercomputer, offering computing power for demanding research problems. Purdue's Anvil cluster consists of 1,000 nodes with two 64-core AMD EPYC \"Milan\" processors each and delivers over one billion CPU core hours each year. With a peak performance of 5.1 petaflops and a speed of 100 Gbps interconnect, Anvil ensures rapid data transfer and processing for efficient research workflows. Standard Anvil nodes have 256GB of DDR4-3200 memory each, ideal for most research tasks.\n\n![Anvil statistics](https://www.rcac.purdue.edu/files/anvil/Anvil_cummulative_8-2024_stats_only%20%281%29.png)_Image Source: [https://www.rcac.purdue.edu/anvil](https://www.rcac.purdue.edu/anvil)_"
  },
  {
    "idurl": 112,
    "idtype": "text",
    "order": 2,
    "content": "### Use cases:\n\n- **General-Purpose CPU Power**: Anvil's powerful CPUs (with 128 cores per node) are ideal for computationally intensive tasks suitable for modeling and simulation across scientific and engineering fields.\n- **Memory-Intensive Workloads**: The dedicated large memory nodes (with 1TB of DDR4-3200 memory per node) works bestfor research that demands significant memory resources.\n- **Composable Subsystem**: It is a private cloud built on Kubernetes and consists of bothe CPU and GPU nodes and S3 data storage. It is suitable for applications such as model inference service (via NVIDIA Triton), Specialized LLMs, dataset hosting, science gateways and web application hosting, and classroom and training applications via interactive access interfaces.\n\n[Anvil Documentation](https://www.rcac.purdue.edu/anvil#docs)\n\ninfo\n\nFor a more detailed information on Anvil, visit the official NSF ACCESS website [here.](https://allocations.access-ci.org/resources)\n\n- [Use cases:](https://docs.ciroh.org/docs/services/external-resources/nsf-access/anvil/#use-cases)"
  },
  {
    "idurl": 113,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NSF NCAR Derecho\nsource: https://docs.ciroh.org/docs/services/external-resources/nsf-access/derecho\nscraped_date: 2025-01-31\n---\n\nNCAR's Derecho supercomputer is a high-performance computing system with 19.87 petaflops of processing power. It comprises 2,488 nodes, each equipped with two 64-core AMD EPYC 7763 Milan processors, totaling 323,712 processor cores. Each node has 256 GB of DDR4 memory and is interconnected by an HPE Slingshot v11 high-speed network in a dragonfly configuration. The system supports Earth system science research for U.S. institutions.\n\n![NSF NCAR Derecho logo](https://ncar-hpc-docs-arc-iframe.readthedocs.io/compute-systems/derecho/media/Derecho_Logo_Landscape_NCARBlue.svg#only-light)_Image Source: [hhttps://arc.ucar.edu/docs](https://arc.ucar.edu/docs)_"
  },
  {
    "idurl": 113,
    "idtype": "text",
    "order": 2,
    "content": "### Use Cases:\n\n- **Earth System Science Research:** Designed specifically for tasks related to climate modeling, weather prediction, and environmental studies.\n- **Large-Scale Simulations:** Optimal for computationally demanding research in Earth sciences that requires extensive core-hours.\n- **Educational Use:** Available for classroom and instructional purposes, supporting students, postdocs, and new faculty without external funding.\n\nTo learn more, visit the [NCAR HPC Documentation page](https://arc.ucar.edu/docs).\n\n[NSF NCAR HPC Documentation](https://arc.ucar.edu/docs)\n\ninfo\n\nFor additional information on NSF NCAR Derecho and its allocation process, refer to the [NSF ACCESS Resources](https://allocations.access-ci.org/resources) page.\n\n- [Use Cases:](https://docs.ciroh.org/docs/services/external-resources/nsf-access/derecho/#use-cases)"
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AWS Best Practices\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice\nscraped_date: 2025-01-31\n---\n\nAs the main account administrator for CIROH subaccount, here are some best practices to follow within your subaccount:"
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 2,
    "content": "## Security:\n\n- **MFA:** Require Multi-Factor Authentication (MFA) for all subaccount users and admins to enhance account security.\n\n- **IAM roles for resources:** Instead of individual access keys, utilize IAM roles for accessing resources within subaccounts. This simplifies access management and eliminates the need for storing long-lived credentials.\n\n- **Regularly review and update permissions:** Regularly review and update user and role permissions within subaccounts to ensure they remain aligned with their current needs.\n\n- **Utilize git-secrets**: git-secrets is a client tool that prohibits unwanted commits containing secret data such as API keys, passwords, and tokens. You can integrate it into your CI/CD pipelines to prevent sensitive information from being added to your GitHub repositories. For more information, refer to the [AWS documentation](https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/scan-git-repositories-for-sensitive-information-and-security-issues-by-using-git-secrets.html) and the [git-secrets GitHub repository](https://github.com/awslabs/git-secrets).\n\n- **Use AWS Secrets Manager**: Use AWS Secrets Manager, or other secrets management solution, so you don't have to hardcode keys in plaintext. The application or client can then retrieve secrets when needed. For more information, see [What is AWS Secrets Manager?](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html)"
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 3,
    "content": "## Access Key Management :\n\n- Never store your access key in plain text, in a code repository, or in code.\n- Never check in the access key in the public repository.\n- Disable or delete access key when no longer needed.\n- Enable least-privilege permissions.\n- Rotate access keys regularly, preferably every 90 days."
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 4,
    "content": "## Resource Management:\n\n- **Tagging:** Implement a consistent tagging strategy for resources in all linked accounts. This allows for better cost allocation, resource identification, and easier filtering when managing resources across multiple accounts. Follow [How to tag resources on AWS](https://docs.ciroh.org/docs/services/cloudservices/aws/tagging).\n\n- **Cost allocation**: Allowed limit for **new** subaccount is $500/project per month. Monitor the usage throughout the month and if it reaches above $500/project, notify admin of the subaccount to take necessary actions. For projects expecting more than $500 per month usage, please email [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) in advance to get the approval from higher management. **Effective Sept 2024**, we transitioned to a new budgeting model (for existing\nusers) that provides your CIROH AWS subaccount with a $10,000 budget for\nevery 6-month period and monthly max limit of $3000. This change will give\nyou more flexibility to plan and execute your research workloads without the\nconstraints of a monthly cap.\n\n- **Resource quotas:** Set resource quotas for subaccounts to limit their spending and resource usage. This helps prevent accidental overspending and ensures efficient resource allocation.\n\n- **Monitor resource usage:** Encourage subaccount admins to monitor their resource usage regularly to identify potential cost optimization opportunities."
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 5,
    "content": "- **Data Locality:** Always consider the location of your data when selecting a region for deploying resources. Deploying resources in the same region as your data minimizes data transfer costs and latency, leading to improved performance and cost-efficiency.\n\n- **Region Selection:** Carefully evaluate the available AWS regions and select the one that best aligns with your data residency requirements, compliance needs, and desired performance characteristics.\n\nEBS:\n\n- **EBS Volume Management: Avoiding Unnecessary Costs:** Terminate EBS Volumes with Terminated Instances: When terminating an EC2 instance, ensure that you also delete any associated EBS volumes that are no longer needed. EBS volumes incur charges even if they are not attached to a running instance.\n\n- **Regularly Review EBS Volume Usage:** Periodically review your EBS volumes using the EC2 Management Console or AWS CLI. Identify any unattached volumes that are no longer required and delete them to avoid ongoing charges.\n\nEFS:\n\n- **Data Lifecycle Management:** Evaluate your data access patterns. For infrequently accessed files, consider migrating data from Amazon EFS to Amazon S3 to leverage its cost-efficient storage classes, such as S3 Standard-IA or S3 Glacier.\n\n- **Tiered Storage Strategy:** Implement a tiered storage strategy where frequently accessed data resides on EFS for high performance, while infrequently accessed or archival data is moved to S3 for cost-effective long-term storage."
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 6,
    "content": "## Governance and Compliance:\n\n- **Standardized configurations:** Establish and enforce standardized configurations for resources across linked accounts. This ensures consistency and simplifies management.\n\n- **Compliance policies:** Implement compliance policies for subaccounts to ensure they adhere to relevant regulations and internal standards.\n\n- **Logging and auditing:** Enable logging and auditing for all activities within linked accounts to track resource usage, identify potential security threats, and maintain compliance.\n\n- **Regular security audits:** Conduct regular security audits of linked accounts to identify and address any vulnerabilities."
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 7,
    "content": "## Additional Recommendations:\n\n- **Centralized documentation:** Use CIROH DocuHub ( [docs.ciroh.org](https://docs.ciroh.org/)) as a central location for documenting procedures, best practices, and resource usage guidelines for linked accounts.\n\n- **Training and awareness:** Offer training and awareness programs to subaccount admins on secure practices, compliance requirements, and resource management best practices through CIROH AWS Office hours.\n\n- **Regular communication:** Maintain regular communication with subaccount admins to address their concerns, answer questions, and share updates regarding policies and procedures via Slack Channel and also available through CIROH AWS Office hours."
  },
  {
    "idurl": 114,
    "idtype": "text",
    "order": 8,
    "content": "## Application Deployment:\n\n- Use terraforms or any Infrastructure as Code if possible for your application deployment.\n\n- [Security:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#security)\n- [Access Key Management :](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#access-key-management-)\n- [Resource Management:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#resource-management)\n- [Governance and Compliance:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#governance-and-compliance)\n- [Additional Recommendations:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#additional-recommendations)\n- [Application Deployment:](https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/aws-best-practice/#application-deployment)"
  },
  {
    "idurl": 115,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Tag Resources on AWS\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/tagging\nscraped_date: 2025-01-31\n---\n\nTags in AWS are essential for organizing resources based on their purpose, owner, or environment, and can also aid in cost tracking when unique key-value pairs are assigned.\n\n1. Using AWS Console:\n\nNavigate to the desired resource, such as an EC2 instance, and follow these steps:\n\n- Select the instance from the list view.\n- Go to the Tags tab and click on the Manage tags button.\n- Add a new tag with a unique Key and Value.\n\n![AWS 'manage tags' window](https://docs.ciroh.org/img/EC2-CreateTag.png)\n\n- Save the changes.\n\n2. Using AWS CLI:\n\nUse the following command-line example to create a tag for an EC2 instance:\n\n```codeBlockLines_e6Vv\naws ec2 create-tags \\\n  --resources i-1234567890abcdef0 \\\n  --tags Key=webserver,Value=dev\n\n```\n\nFor each project, tag all its resources with:\n\nProject=project\\_name (e.g., ciroh-hydroshare, ciroh-fim)\nDouble-check the tag name with the AWS main account admin to make sure it fits well with our naming scheme."
  },
  {
    "idurl": 116,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AWS Data Science Tools\nsource: https://docs.ciroh.org/docs/services/cloudservices/aws/documentation/data-science-tools\nscraped_date: 2025-01-31\n---\n\nA collection of Amazon Web Services (AWS) scripts supporting Water Data Science. This repository can serve as a resoruces for those looking to connect and leverage the power of AWS products, specifically AWS S3 storage.\n\ninfo\n\n[AWS Data Science Tools](https://github.com/whitelightning450/AWS_Scripts)"
  },
  {
    "idurl": 117,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: JupyterHub User Directory\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/directory\nscraped_date: 2025-01-31\n---\n\nThis is a guide for understanding the File System in CIROH JupyterHub. You can find detailed explanation on [2i2c docs site](https://docs.2i2c.org/user/topics/data/filesystem/)."
  },
  {
    "idurl": 117,
    "idtype": "text",
    "order": 2,
    "content": "### 1\\. `/home/jovyan`\n\nThis is your home directory and is same for all JupyterHub users. **Only you can access files in your home directory.** Any files you place in your home directory persists between sessions. It is recommended to use only for notebooks and code since it is not suitable for large datasets."
  },
  {
    "idurl": 117,
    "idtype": "text",
    "order": 3,
    "content": "### 2\\. `/home/jovyan/shared`\n\nThis is the shared **readonly** directory. All users can access and read from the shared directory. Only the hub admins can add and delete data from this directory."
  },
  {
    "idurl": 117,
    "idtype": "text",
    "order": 4,
    "content": "### 3\\. `/tmp`\n\nThis is a non persistient directory. This means any files you add under /tmp direcotry will be deleted once you log out. This directory can be used to store data temporary data.\n\n- [1\\. `/home/jovyan`](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/directory/#1-homejovyan)\n- [2\\. `/home/jovyan/shared`](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/directory/#2-homejovyanshared)\n- [3\\. `/tmp`](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/directory/#3-tmp)"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Manage files in GCP bucket\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage\nscraped_date: 2025-01-31\n---\n\nThis guide is for managing objects in GCP buckets available on 2i2c CIROH JupyterHub. For more detailed explanation, you can visit [2i2c docs site](https://docs.2i2c.org/user/topics/data/object-storage/manage-object-storage-gcp/)."
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 2,
    "content": "### 1\\. Overview\n\nCIROH JupyterHub uses object Google Cloud Storage to store data in buckets (containers for objects). Currently, there are two buckets available to use on CIROH JupyterHub.\n\n- **Scratch Bucket**: It is intended for storing temporary files since any files in scratch bucket get deleted after seven days. Open a terminal in CIROH JupyterHub and run this command to display your scratch bucket name:\n\n```codeBlockLines_e6Vv\necho $SCRATCH_BUCKET\ngs://awi-ciroh-scratch/<username>\n\n```\n\n**Note:** In the above command output, the name of the bucket is 'awi-ciroh-scratch' and `<username>` is the folder in the bucket.\n\n- **Persistent Bucket**: It is recommended to use for storing files that you will be using for a longer period of time. Open a terminal in CIROH JupyterHub and run this command to display your persistent bucket name:\n\n```codeBlockLines_e6Vv\necho $PERSISTENT_BUCKET\ngs://awi-ciroh-persistent/<username>\n\n```"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 3,
    "content": "### 2\\. Copying file to a bucket\n\nYou can copy files on your CIROH JupyterHub to an available bucket using the following command.\n\n```codeBlockLines_e6Vv\ngcloud storage cp <filepath> $PERSISTENT_BUCKET/<filepath>\n\n```"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 4,
    "content": "### 3\\. Copying file from a bucket to CIROH JupyterHub\n\nYou can copy files from an accessible bucket to your CIROH JupyterHub using the following command.\n\n```codeBlockLines_e6Vv\ngcloud storage cp $PERSISTENT_BUCKET/<filepath> <destination-filepath>\n\n```"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 5,
    "content": "### 4\\. Listing files in a bucket\n\nYou can list all files/folder in a bucket using the following command.\n\n```codeBlockLines_e6Vv\ngcloud storage ls $PERSISTENT_BUCKET\n\n```\n\n**Note:** The above command will list all files/folders in the folder `<username>`. It won't list files in the sub-folders of folder `<username>`. To list all files including the files in the sub-folders of the root folder `<username>`, use the following command.\n\n```codeBlockLines_e6Vv\ngcloud storage ls --recursive $PERSISTENT_BUCKET\n\n```"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 6,
    "content": "### 5\\. Deleting file from a bucket\n\nYou can delete a file in a bucket with the following command:\n\n```codeBlockLines_e6Vv\ngcloud storage rm $PERSISTENT_BUCKET/<filepath>\n\n```"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 7,
    "content": "### 6\\. User permssions on buckets\n\nAll users have read/write permissions on both the scratch and persistent buckets.\n\nnote\n\nAnyone can access each other's files in buckets on the hub. Please be careful not to delete other user's files. Using the enviornment variables ($SCRATCH\\_BUCKET & $PERSISTENT\\_BUCKET) to access buckets in commands would prevent accidententally deleting any other user's files. Your actions impact the entire organization's storage. If unsure, consult with the team lead or ciroh IT support."
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 8,
    "content": "### 7\\. Accessing buckets in Python\n\nYou can find information on how to access buckets in Python code, [here.](https://docs.2i2c.org/user/topics/data/object-storage/working-with-object-storage/)"
  },
  {
    "idurl": 118,
    "idtype": "text",
    "order": 9,
    "content": "## Where to go for help:\n\n- Email [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) UA CIROH Cloud Team\n- CIROH Cloud Slack Channel - #ciroh-ua-it-admin\n- CIROH Infrastructure Support Slack Channel - #ciroh-infrastructure-support\n\n- [1\\. Overview](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#1-overview)\n- [2\\. Copying file to a bucket](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#2-copying-file-to-a-bucket)\n- [3\\. Copying file from a bucket to CIROH JupyterHub](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#3-copying-file-from-a-bucket-to-ciroh-jupyterhub)\n- [4\\. Listing files in a bucket](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#4-listing-files-in-a-bucket)\n- [5\\. Deleting file from a bucket](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#5-deleting-file-from-a-bucket)\n- [6\\. User permssions on buckets](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#6-user-permssions-on-buckets)\n- [7\\. Accessing buckets in Python](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#7-accessing-buckets-in-python)\n- [Where to go for help:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/gcp-object-storage/#where-to-go-for-help)"
  },
  {
    "idurl": 119,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Push and Pull to GitHub\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/github-push\nscraped_date: 2025-01-31\n---\n\nFollow these steps to allow 2i2c JupyterHub to push to GitHub repositories:\n\n- Install [CIROHJupyterHubGitHubPush](https://github.com/apps/cirohjupyterhubgithubpush) on your device by clicking on Install button.\n\n- Choose the repositories where you want to allow pushing from 2i2c.\n\n- Open a terminal on JupyterHub and run:\n\n```codeBlockLines_e6Vv\ngh-scoped-creds --client-id Iv23lixIgj0sAZqz98bH\n\n```"
  },
  {
    "idurl": 119,
    "idtype": "text",
    "order": 2,
    "content": "### You can also follow along with this video tutorial that walks you through the same process visually.\n\nYouTube\n\n- [You can also follow along with this video tutorial that walks you through the same process visually.](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/github-push/#you-can-also-follow-along-with-this-video-tutorial-that-walks-you-through-the-same-process-visually)"
  },
  {
    "idurl": 120,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Request custom images\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/custom-images\nscraped_date: 2025-01-31\n---\n\nIf you have a request for creating custom images, then please follow these instructions."
  },
  {
    "idurl": 120,
    "idtype": "text",
    "order": 2,
    "content": "### 1\\. Create an environment.yml file:\n\n- Open your terminal or command prompt. Make sure you have conda installed and activated in the environment that contains the packages you want to use for creating custom images. Learn more [here](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).\n\n- Run the following command, replacing **ENVNAME** with the actual name of your environment.\n\n```codeBlockLines_e6Vv\nconda env export -n ENVNAME > environment.yml\n\n```"
  },
  {
    "idurl": 120,
    "idtype": "text",
    "order": 3,
    "content": "### 2\\. Submit a Request Form:\n\n- Click on the link below to access the Jupyterhub (2i2c) Software Install form.\n- Select Install Software on CIROH 2i2c JupyterHub as a reason for request.\n- Fill out remaining sections of the form and submit it.\n\n[JupyterHub (2i2c) Software Install Form](https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u)"
  },
  {
    "idurl": 120,
    "idtype": "text",
    "order": 4,
    "content": "### 3\\. Share your environment.yml file with CIROH-IT support\n\n- After submitting the request form, attach the environment.yml file you created in step 1 to an email and send it to [ciroh-it-support@ua.edu](mailto:ciroh-it-support@ua.edu)\n\n- [1\\. Create an environment.yml file:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/custom-images/#1-create-an-environmentyml-file)\n- [2\\. Submit a Request Form:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/custom-images/#2-submit-a-request-form)\n- [3\\. Share your environment.yml file with CIROH-IT support](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/custom-images/#3-share-your-environmentyml-file-with-ciroh-it-support)"
  },
  {
    "idurl": 121,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Persistent Conda Environment\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda\nscraped_date: 2025-01-31\n---\n\nTo ensure your Conda environments persist across server restarts on the CIROH 2i2c server, create them in your home directory. Follow these steps:"
  },
  {
    "idurl": 121,
    "idtype": "text",
    "order": 2,
    "content": "### 1\\. Create a directory for Conda environments:\n\nYou can set up a directory within your home folder to store all your Conda environments. This prevents them from being removed when the server is restarted. For example:\n\n```codeBlockLines_e6Vv\nmkdir -p ~/conda_envs\n\n```"
  },
  {
    "idurl": 121,
    "idtype": "text",
    "order": 3,
    "content": "### 2\\. Create a new environment in that directory:\n\nUse the _--prefix_ option with conda create to specify the location where you want to create your environment. For example, to create an environment called _my\\_env_ in _~/conda\\_envs_:\n\n```codeBlockLines_e6Vv\nconda create --prefix ~/conda_envs/my_env python=3.9\n\n```"
  },
  {
    "idurl": 121,
    "idtype": "text",
    "order": 4,
    "content": "### 3\\. Activate the environment:\n\nYou can activate the environment as usual, using the path to where you created it:\n\n```codeBlockLines_e6Vv\nconda activate ~/conda_envs/my_env\n\n```"
  },
  {
    "idurl": 121,
    "idtype": "text",
    "order": 5,
    "content": "### 4\\. Automatically activate the environment on restart:\n\nIf you want this environment to be activated every time you log in or the server restarts, you can add the following to your _.bashrc_ or _.bash\\_profile_ file:\n\n```codeBlockLines_e6Vv\nconda activate ~/conda_envs/my_env\n\n```\n\nBy creating your environments in your home folder (e.g., _~/conda\\_envs/_), they will persist across server restarts, ensuring that you don't have to recreate them every time.\n\n- [1\\. Create a directory for Conda environments:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda/#1-create-a-directory-for-conda-environments)\n- [2\\. Create a new environment in that directory:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda/#2-create-a-new-environment-in-that-directory)\n- [3\\. Activate the environment:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda/#3-activate-the-environment)\n- [4\\. Automatically activate the environment on restart:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/conda/#4-automatically-activate-the-environment-on-restart)"
  },
  {
    "idurl": 122,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Prevent Server Timeout\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/server-timeout\nscraped_date: 2025-01-31\n---\n\nJupyterHub servers typically stop after about an hour of inactivity to help manage computational reosurces. This can be present challenges for users who need to run long-running jobs. You have two options to keep your JupyterHub server active for longer periods:"
  },
  {
    "idurl": 122,
    "idtype": "text",
    "order": 2,
    "content": "### 1\\. Jupyter Keepalive Extension\n\nThe Jupyter Keepalive extension provides an easy way to control your server's active time.\n\n- To install the extension, open a terminal and run:\n\n```codeBlockLines_e6Vv\npip install jupyter-keepalive\n\n```\n\n- Use the JupyterLab Command Palette (Command+Shift+C on Mac or Control+Shift+C on Linux/Windows) to select the \"Keep server alive while idle\" option.\n- Once your task is complete, it's crucial that you then use the Command Palette to select the \"Stop keeping server alive\" option. This will ensure that the server is no longer being kept active unnecessarily.\n\n![image of jupyterlab command palette](https://docs.ciroh.org/img/server-keepalive.png)"
  },
  {
    "idurl": 122,
    "idtype": "text",
    "order": 3,
    "content": "### 2\\. Using `time.sleep()` (Alternate Method)\n\nAs an alternative, you can create a separate notebook with the following code and run the cell before starting your job.\n\n```codeBlockLines_e6Vv\nimport time\ntime.sleep(24 * 60 * 60) # Sleeps for 24 hours\n\n```\n\n- [1\\. Jupyter Keepalive Extension](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/server-timeout/#1-jupyter-keepalive-extension)\n- [2\\. Using `time.sleep()` (Alternate Method)](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/server-timeout/#2-using-timesleep-alternate-method)"
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Python Package Version Conflicts\nsource: https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/python-package-conflicts\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 2,
    "content": "### Overview:\n\nWhen different versions of Python packages are installed in your user home directory than the version that was already installed in the JupyterHub image, these local installations take precedence, potentially causing version mismatches and unexpected behavior."
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 3,
    "content": "### Troubleshooting guide:"
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 4,
    "content": "#### Step 1: Identify Package Conflicts\n\n```codeBlockLines_e6Vv\nls ~/.local/lib/pythonX.Y/site-packages/\n\n```\n\n**Note**: Replace X.Y with your Python version (e.g., 3.10). Find your version with `python --version`."
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 5,
    "content": "#### Step 2: Remove conflicting packages from user home directory\n\nClear all locally installed Python packages:\n\n```codeBlockLines_e6Vv\nrm -rf ~/.local/lib/pythonX.Y\n\n```\n\n**Note**: This will remove **ALL** Python packages installed in your home directory and ensure that only system or environment packages are used."
  },
  {
    "idurl": 123,
    "idtype": "text",
    "order": 6,
    "content": "#### Step 3: Verify Your Environment\n\n- Verify that you are using the correct JupyterHub image by checking the JUPYTER\\_IMAGE environment variable:\n\n```codeBlockLines_e6Vv\necho $JUPYTER_IMAGE\n\n```\n\n- Reinstall needed packages properly\n\n```codeBlockLines_e6Vv\npip install <package-name>\n\n```\n\n- [Overview:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/python-package-conflicts/#overview)\n- [Troubleshooting guide:](https://docs.ciroh.org/docs/services/cloudservices/2i2c/documentation/python-package-conflicts/#troubleshooting-guide)"
  },
  {
    "idurl": 124,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Accessing the Compute Nodes\nsource: https://docs.ciroh.org/docs/services/on-prem/Pantarhei/RunningJobs/computenode\nscraped_date: 2025-01-31\n---\n\nPantarhei employs the [Slurm Workload Manager](https://slurm.schedmd.com/documentation.html) for the purpose of job scheduling and management. Utilizing Slurm, a user initiates a request for resources and submits a job to a designated queue. Subsequently, the system undertakes the task of extracting jobs from the queues, assigning the requisite compute nodes, and executing the submitted tasks. Although users typically access the Slurm job scheduler by SSH-ing to a Pantarhei login node, it is imperative to emphasize that the recommended practice entails utilizing Slurm to submit work as a job, as opposed to executing computationally intensive tasks directly on a login node. Given that all users share the login nodes, running anything beyond minimal test jobs can adversely affect the collective ability of users to effectively utilize Pantarhei resources."
  },
  {
    "idurl": 124,
    "idtype": "text",
    "order": 2,
    "content": "Pantarhei's framework is tailored to accommodate the moderate-scale computational and data requirements of the majority of CIROH users. Users with allocations possess the capability to submit tasks to a diverse array of queues, each featuring distinct job size and walltime constraints. Dedicated sets of queues are allocated for CPU, GPU, and FPGA nodes, with typically shorter walltime and smaller job size limits translating to expedited turnaround times. Several additional considerations regarding Pantarhei queues merit attention:\n\n1. Pantarhei facilitates shared jobs, whereby multiple tasks can be executed on a single node. This approach enhances job throughput, maximizes overall system utilization, and fosters increased user accessibility to Pantarhei resources.\n2. Pantarhei accommodates long-running jobs, with run times extendable up to seven days for tasks utilizing up to 6 full nodes.\n3. The maximum permissible job size on Pantarhei is 240 cores. For tasks exceeding this threshold, users are advised to initiate a consulting ticket to engage in further discussion with Pantarhei support personnel."
  },
  {
    "idurl": 125,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Data and Code Sharing Policy\nsource: https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy\nscraped_date: 2025-01-31\n---\n\nThe Cooperative Institute for Research to Operations in Hydrology (CIROH) is committed to an open data policy that will maximize the impact and broad use of data and research products produced by CIROH projects and will also ensure that Federal data sharing requirements are met. This policy document is intended to assist CIROH investigators in creating and sharing high-quality data and research products. We begin with guiding principles, after which the specific policy and recommendations are stated. This document also provides guidance and instructions that may be useful to CIROH investigators in meeting the terms of this policy. Finally, we also include an appendix with further details about the specific data sharing requirements of CIROH's partner agencies. We consider this policy to be a living document that will be revised as the needs of CIROH investigators and CIROH partner agencies evolve.\n\n![Water Tools](https://docs.ciroh.org/img/graphics/datasharing.jpeg)\n\n* * *\n\n[**üìÑÔ∏èPolicy and Guidance** \\\\\nPolicy and Guidance for Data and Code Sharing Policy](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Policies)[**üìÑÔ∏èRecommendations** \\\\\nRecommendations for Data and Code Sharing Policy](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations)"
  },
  {
    "idurl": 126,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen Technical Guidance\nsource: https://docs.ciroh.org/docs/policies/NextGen\nscraped_date: 2025-01-31\n---\n\n_Authors: Fred Ogden, Nels Frazier, Keith Jennings, Jonathan Frame, Wouter Knoben, Tadd Bindas,_\n_Yalan Song, Irene Garousi-Nejad, Jeffrey Carver, Andy Wood, Anthony Castronova, Arpita_\n_Patel, Shahabul Alam, Sifan A. Koriche, Junwei Guo, Cyril Th√©bault, Raymond J. Spiteri,_\n_Ahmad J. Khattak, James Halgren, Patrick J. Clemins, Mukesh Kumar, and Martyn Clark_"
  },
  {
    "idurl": 126,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction\n\nThis document provides technical guidance for including models and modules in the Next Generation Water Resources Modeling Framework (NextGen). It covers essential aspects of model integration, best practices, and framework requirements."
  },
  {
    "idurl": 126,
    "idtype": "text",
    "order": 3,
    "content": "## Full Document\n\nFor the complete technical guidance, please refer to the PDF document below:\n\n[Download Technical Guidance PDF](https://docs.ciroh.org/assets/files/Nextgen_Model_Development_Specifications_WG2_v1.docx-01c8ca7525b009168f416107d8aaf29d.pdf)"
  },
  {
    "idurl": 126,
    "idtype": "text",
    "order": 4,
    "content": "## Key Points\n\n- NextGen is a model-agnostic, standards-based framework for water resources modeling\n- It allows for flexible experimentation with hydrologic cycle representations\n- The framework supports explicit coupling of models through sharing of computed states and fluxes\n- Design requirements include maximum flexibility, open-source development, and multi-language support\n\n- [Introduction](https://docs.ciroh.org/docs/policies/NextGen/#introduction)\n- [Full Document](https://docs.ciroh.org/docs/policies/NextGen/#full-document)\n- [Key Points](https://docs.ciroh.org/docs/policies/NextGen/#key-points)"
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Policy and Guidance\nsource: https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Policies\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 2,
    "content": "## Guiding Principles\n\nWe provide the following principles that guide CIROH's activities and associated data sharing:\n\n- Science is reproducible.\n- Reproducibility of scientific work is enabled through openness.\n- Open science is enabled through open access to data, source code, accessible computational resources, and sufficient metadata for interpretation/use.\n- Products of CIROH research are produced at public expense and should be broadly accessible to the public."
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 3,
    "content": "## Policy Statement\n\nCIROH follows NOAA's Data Sharing Directive, which is included in the Terms and Conditions of CIROH's Cooperative Agreement with NOAA and is also available [here](https://nosc.noaa.gov/EDMC/PD.DSP.php) (Version 3.0 at the time of this writing). CIROH is responsible for implementing these conditions and ensuring that they are also met by CIROH sub-recipients and subcontractors. The [Data Management Plan](https://docs.ciroh.org/assets/files/CIROH_Data_Management_Plan-0dc8c6f27a387855880c325c7bb02ff1.pdf) submitted with the original CIROH proposal is included as an Appendix to this document.\n\nThe specific wording included in the CIROH Cooperative Agreement is as follows: - **Data Sharing:** Environmental data collected or created under this Grant, Cooperative Agreement, or Contract must be made publicly visible and accessible in a timely manner, free of charge or at minimal cost that is no more than the cost of distribution to the user, except where limited by law, regulation, policy, or national security requirements. Data are to be made available in a form that would permit further analysis or reuse: data must be encoded in a machine-readable format, preferably using existing open format standards; data must be sufficiently documented, preferably using open metadata standards, to enable users to independently read and understand the data. The location (internet address) of the data should be included in the final report."
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 4,
    "content": "Pursuant to NOAA Information Quality Guidelines, data should undergo quality control (QC), and a description of the QC process and results should be referenced in the metadata. - **Timeliness:** Data accessibility must occur no later than publication of a peer-reviewed article based on the data, or two years after the data are collected and verified, or two years after the original end date of the grant (not including any extensions or follow-on funding), whichever is soonest unless a delay has been authorized by the NOAA funding program. - **Disclaimer:** Data produced under this award and made available to the public must be accompanied by the following statement: \"These data and related items of information have not been formally disseminated by NOAA, and do not represent any agency determination, view, or policy.\"\n- **Failure to Share Data:** Failing or delaying to make environmental data accessible in accordance with the submitted Data Management Plan, unless authorized by the NOAA Program, may lead to enforcement actions and will be considered by NOAA when making future award decisions. Funding recipients are responsible for ensuring these conditions are also met by sub-recipients and subcontractors. - **Funding acknowledgment:** Federal funding sources shall be identified in all scholarly publications. An Acknowledgements section shall be included in the body of the publication stating the relevant Grant Programs and Award Numbers."
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 5,
    "content": "In addition, funding sources shall be reported during the publication submission process using the FundRef mechanism ( [http://www.crossref.org/fundref/](http://www.crossref.org/fundref/)) if supported by the Publisher. - **Manuscript submission:** The final pre-publication manuscripts of scholarly publications produced with NOAA funding shall be submitted to the NOAA Institutional Repository at [http://library.noaa.gov/repository](http://library.noaa.gov/repository) after acceptance and no later than upon publication of the paper by a journal. NOAA will produce a publicly-visible catalog entry directing users to the published version of the article. After an embargo period of one year after publication, NOAA shall make the manuscript itself publicly visible, free of charge, while continuing to direct users to the published version of record. - **Data Citation:** Publications based on data, and new products derived from source data, must cite the data used according to the conventions of the Publisher, using unambiguous labels such as Digital Object Identifiers (DOIs). All data and derived products that are used to support the conclusions of a peer-reviewed publication must be made available in a form that permits verification and reproducibility of the results."
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 6,
    "content": "## Important Definitions\n\nThere are several definitions in NOAA's Data and Publication Sharing Directive that we provide here for interpretation of the above text. For the full list and for the exact statement of these definitions, refer to the full text of NOAA's Data Sharing Directive (Version 3.0) at the link in the section above. 1. **Research Results:** Defined as environmental data and peer-reviewed publications under NOAA's Data Sharing Directive. 2. **Environmental Data:** Defined by NOAA Administrative Order (NAO) 212-15 as:\n   - Recorded and derived observations and measurements of:\n     - Physical, chemical, biological, geological, and geophysical properties and conditions of:\n       - Oceans, atmosphere, space environment, sun, and solid earth. - Correlative data such as socio-economic data, related documentation, and metadata. - Includes digital audio or video recordings of environmental phenomena and numerical model outputs used to support peer-reviewed publications. - Data collected in a laboratory or other controlled environment, including measurements of animals and chemical processes. 3. **Data Sharing Directive:** Defines \"data\" specifically as environmental data. 4. **Sharing Data:** Making data publicly visible and accessible in a timely manner at no cost or minimal cost, in a machine-readable format based on open standards, along with necessary metadata. 5."
  },
  {
    "idurl": 127,
    "idtype": "text",
    "order": 7,
    "content": "**Timeliness:** Data accessibility must occur no later than publication of a peer-reviewed article based on the data or within two years of data collection or grant end date, whichever is soonest, unless authorized delay by NOAA. 6. **Applicability:** Applies to new data created by extramural funding recipients; internally produced NOAA data or collaborative research data are subject to the NOAA Data Access Directive. 7. **Exclusions:** Laboratory notebooks, preliminary analyses, drafts of scientific papers, plans for future research, peer review reports, communications with colleagues, or physical objects are not covered under NOAA's Data Sharing Directive.\n\n- [Guiding Principles](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Policies/#guiding-principles)\n- [Policy Statement](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Policies/#policy-statement)\n- [Important Definitions](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Policies/#important-definitions)"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Recommendations\nsource: https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations\nscraped_date: 2025-01-31\n---\n\nIn the following sections, we provide some practical guidance for CIROH researchers designed to help them meet the terms and conditions of CIROH's Data and Code Sharing Policy, as discussed above, for different types of research products. Each section is focused on providing guidance for a particular type of product, but these sections may not be inclusive of all of the types of products that may fall under NOAA's requirements for sharing data and research products."
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 2,
    "content": "## Recommendations for Sharing Data\n\nDepending on the type and size of data you are producing and using, we recommend the following options for data archiving and sharing data: 1. **HydroShare ( [www.hydroshare.org](http://www.hydroshare.org/))**\n   - **When to use:**\n     - Use for datasets under 1GB (increases are possible)\n     - Datasets that require spatial data services (THREDDS, WMS, etc)\n     - Datasets that need to be accessed from applications through APIs\n     - Datasets that are linked to other datasets\n     - Datasets that require formal publishing with a DOI\n     - For links and pointers to external datasets\n     - Consider using a Creative Commons License for releasing data\n   - **When not to use:**\n     - Very large datasets\n     - Rapidly changing datasets\n     - Data with extensive sharing and license restrictions\n   - **Cost of use:**\n     - Free for researchers up to 20 GB per user\n     - Free for permanently published data\n   - **Where to go for help:**\n     - [HydroShare Help](http://help.hydroshare.org/)\n     - Email [help@cuahsi.org](mailto:help@cuahsi.org) to reach the CUAHSI HydroShare team\n     - HydroShare short videos on CUAHSI YouTube channel\n2."
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 3,
    "content": "**CIROH Cloud Amazon S3 storage via CIROH's AWS account and Google Buckets, Azure, On-Premise**\n   - **When to use:**\n     - Use for large datasets\n     - Data that is part of the NWM workflows ( [https://console.cloud.google.com/storage/browser/national-water-model](https://console.cloud.google.com/storage/browser/national-water-model))\n     - Cloud computing linked data\n     - Consider linking to cloud share from HydroShare for discoverability\n   - **When not to use:**\n     - Smaller datasets you want to formally publish with a DOI (May complicate formal publication with DOI)\n   - **Cost of use:**\n     - Some uses may be covered by CIROH core funds (contact CIROH Cloud Team to start a request)\n     - Extensive uses may be charged to individual CIROH projects\n   - **How to get access to CIROH AWS:**\n     - More information on obtaining accesss to CIROH's AWS resources are available at this link: [https://docs.ciroh.org/docs/services/cloudservices/aws/](https://docs.ciroh.org/docs/services/cloudservices/aws/)\n   - **Where to go for help:**\n     - Email [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu) UA CIROH Cloud Team\n     - CIROH Cloud Slack Channel - #ciroh-ua-it-admin\n     - AWS support Slack Channel - #aws-ciroh-support\n3. **Water Prediction Node ( [https://waternode.ciroh.org/](https://waternode.ciroh.org/))**\n   - **Who to contact for help:**\n     - [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu)"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 4,
    "content": "## Recommendations for Sharing Code 1. **GitHub ( [www.github.com](http://www.github.com/))**\n   - **When to use:**\n     - Generally post your code on your institution's GitHub organization - some projects may be appropriately hosted on CIROH organization\n     - We suggest forking the CIROH template for structured readme files, etc. - Example: [https://github.com/NOAA-OWP/owp-open-source-project-template](https://github.com/NOAA-OWP/owp-open-source-project-template)\n     - When multiple developers are actively developing software or other products\n     - Consider using the three clause BSD3 or MIT license\n     - Consider linking to Zenodo to snapshot and get a DOI for your code\n   - **When not to use:**\n     - Not recommended for proprietary code (although private repositories are available in GitHub at cost)\n   - **Cost of use:**\n     - Free for public repositories\n     - Free for private repositories (limited functionality - e.g. no GitHub Actions/Runners, Wiki and other features)\n   - **Where to go for help:**\n     - GitHub discussion forums\n     - CIROH Slack channels e.g., #ciroh-hydroinformatics-working-group\n2."
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 5,
    "content": "**Jupyter notebooks in HydroShare**\n   - **When to use:**\n     - Sharing code as Jupyter notebooks that you want to be launchable into a computational environment like CIROH JupyterHub\n     - When you want your code to accompany data in one citable resource for reproducibility purposes\n   - **When not to use:**\n     - When code is rapidly changing\n     - When you want your code to be under formal version control\n   - **Cost of use:**\n     - Free for researchers to store up to 20 GB of content in HydroShare\n     - CUAHSI JupyterHub is free to use\n     - CIROH 2i2c JupyterHub is free to use (cost covered by CIROH core funds)\n   - **Where to go for help:**\n     - Email [help@cuahsi.org](mailto:help@cuahsi.org) for help with sharing notebooks in HydroShare and/or launching notebooks into the CUAHSI JupyterHub instance\n     - How to get access to CIROH 2i2c: [https://docs.ciroh.org/docs/services/cloudservices/google/](https://docs.ciroh.org/docs/services/cloudservices/google/)\n     - How to get access to 2i2c using Hydroshare: [https://docs.ciroh.org/docs/services/cloudservices/google/hydroshareintegration](https://docs.ciroh.org/docs/services/cloudservices/google/hydroshareintegration)"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 6,
    "content": "## Recommendations for Sharing Models\n\nModel sharing can be viewed as \"code sharing\" or \"data sharing,\" and many of the suggested methods above can be adopted for model sharing. Consider the following options for sharing models:\n\n- GitHub - Supports sharing of model source codes\n- HydroShare - Supports sharing of model programs and models instances\n- NextGen In A Box (NGIAB) - Use cloud computing or local machine to modify and execute NextGen based models in a docker container\n- CIROH Web Sites - Downloadable executables, model instances, installers, etc can be shared on the [CIROH portal web site](https://portal.ciroh.org//)."
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 7,
    "content": "## Recommendations for Sharing Workflows\n\n- GitHub gists (e.g., to show how to use certain modules)\n- Post materials on the CIROH DocuHub\n- JupyterNotebooks in HydroShare - launch into CIROH JupyterHub environment or CUAHSI JupyterHub"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 8,
    "content": "## Recommendations for Sharing Published Manuscripts\n\n- GitHub (see [https://github.com/NOAA-OWP/OWP-Presentations](https://github.com/NOAA-OWP/OWP-Presentations))\n- CIROH Portal (see [https://portal.ciroh.org/publications](https://portal.ciroh.org/publications))\n- Per NOAA - Don't share preprints prior to peer review"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 9,
    "content": "## Recommendations for Sharing Educational Materials\n\n- HydroLearn - We recommend using [www.HydroLearn.org](http://www.hydrolearn.org/) which allows for and supports learning module sharing and dissemination of educational materials. Hydrolearn modules may be linked from CIROH Portal's [Lessons tab](https://portal.ciroh.org/lessons).\n  - **When to use:**\n    - Broadly applicable learning modules related to hydrology and NWM\n  - **When not to use:**\n    - Highly specific, localized, tailored learning materials for your specific university or departmental courses\n    - Material that requires specific and inaccessible data, software, etc.\n  - **Cost to use:**\n    - Free for open access learning modules\n  - **Where to go for help:**\n    - [HydroLearn Contact Us](https://www.hydrolearn.org/contact-us/)"
  },
  {
    "idurl": 128,
    "idtype": "text",
    "order": 10,
    "content": "- [Recommendations for Sharing Data](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-data)\n- [Recommendations for Sharing Code](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-code)\n- [Recommendations for Sharing Models](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-models)\n- [Recommendations for Sharing Workflows](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-workflows)\n- [Recommendations for Sharing Published Manuscripts](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-published-manuscripts)\n- [Recommendations for Sharing Educational Materials](https://docs.ciroh.org/docs/policies/DataAndCodeSharingPolicy/Recommendations/#recommendations-for-sharing-educational-materials)"
  },
  {
    "idurl": 136,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\nsource: https://docs.ciroh.org/blog/aorc-data-access\nscraped_date: 2025-01-31\n---\n\n![Screenshot of Hydroshare Resource](https://docs.ciroh.org/assets/images/aorc-data-retrieval-hydroshare-8cf1d0e0ef2e88fd3e47058cf2fae473.png)\n\n> _A screenshot of the HydroShare resource page for [Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/)._\n\nThe Analysis of Record for Calibration (AORC) dataset is recognized as a high-value resource for the CUAHSI and CIROH community.\nThis dataset is hosted by NOAA via Amazon Web Services (AWS) and is available in two primary formats:\na [latitude-longitude gridded dataset](https://registry.opendata.aws/noaa-nws-aorc/)\nand the [National Water Model (NWM) projected dataset, part of the NWM Retrospective archive](https://registry.opendata.aws/nwm-archive/).\nTo enhance accessibility and illustrate analysis capabilities, we developed four user-friendly Jupyter Notebooks that enable data retrieval for both specific points of interest and spatial domains defined by shapefiles:"
  },
  {
    "idurl": 136,
    "idtype": "text",
    "order": 2,
    "content": "- **AORC\\_LL\\_PointRetrieval.ipynb**: For retrieving and aggregating data from the latitude-longitude gridded dataset for a specific point using geographic coordinates.\n- **AORC\\_LL\\_ZoneRetrieval.ipynb**: For retrieving and aggregating data from the latitude-longitude gridded dataset for an area defined by a polygon shapefile.\n- **AORC\\_NWMProj\\_PointRetrieval.ipynb**: For retrieving and aggregating data from the NWM projected dataset for a specific point using geographic coordinates.\n- **AORC\\_NWMProj\\_ZoneRetrieval.ipynb**: For retrieving and aggregating data from the NWM projected dataset for an area defined by a polygon shapefile.\n\nThese Jupyter Notebooks, containing instructions and Python code to access the data, enable researchers to retrieve AORC data from AWS.\nFrom there, the notebooks offer options to subset and aggregate the data over user-defined time intervals (beyond the original hourly resolution) and spatial area.\nThese serve as examples for how you could write or modify code to access AORC data in your work.\nThe notebooks are publicly [available on HydroShare](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/)\nand are compatible with JupyterHub computing platforms such as [CIROH 2i2c JupyterHub linked to HydroShare](https://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98/)."
  },
  {
    "idurl": 136,
    "idtype": "text",
    "order": 3,
    "content": "To use these notebooks, go to the [HydroShare resource](https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/),\nselect \"Open With\" at the top right, and choose \"CIROH 2i2c JupyterHub\".\nThis will copy the resource contents (notebooks and data) into the CIROH JupyterHub environment,\nwhere you can open and work through them to access the data.\nNote that you will need a [CUAHSI HydroShare account](https://www.hydroshare.org/) to access \"Open With\" in HydroShare,\nand you will also need to [request CIROH-2i2c JupyterHub access](https://docs.ciroh.org/docs/services/access/#accessing-ciroh-2i2c-jupyterhub) using a GitHub account.\n\nOur work also includes a comparative analysis of the two AORC datasets with a summary of findings.\nWhile we mostly observed small differences, mainly due to projections, users should be aware of potential discrepancies between the datasets.\n\nBy providing these user-friendly tools and highlighting the characteristics of both AORC datasets,\nour work aims to support and facilitate more efficient hydrological and climate-related research within the CUAHSI and CIROH community."
  },
  {
    "idurl": 136,
    "idtype": "text",
    "order": 4,
    "content": "### References:\n\n- Salehabadi, H., D. Tarboton, A. Nassar, A. M. Castronova, P. Dash (2025). Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis, HydroShare, [http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f](http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f)\n- The development versions of these notebooks are available on GitHub: [https://github.com/CUAHSI/notebooks](https://github.com/CUAHSI/notebooks) in the Data Access Examples / AORC - Retrieval of AORC Data for Hydrologic Analysis folder.\n- Patel, A., A. Castronova (2025). CIROH 2i2c JupyterHub, HydroShare, [http://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98](http://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98)\n\n- [References:](https://docs.ciroh.org/blog/aorc-data-access/#references)"
  },
  {
    "idurl": 137,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB\nsource: https://docs.ciroh.org/blog/ismart-ngiab-application\nscraped_date: 2025-01-31\n---\n\n![A poster, titled \"Assessing streamflow forecast over the Hackensack River Watershed using physics- and AI-driven weather prediction models\".](https://docs.ciroh.org/assets/images/blog_CIROH_DevCon-bd5de4e3ca6192611aadd30bb47b1586.png)\n\n> _A poster presented by the I-SMART team at the CIROH Developers Conference, held at the University of Vermont in Burlington from May 28 to 30, 2025._\n\nThe densely populated Hackensack River watershed lies within the New York City Metropolitan Area, which spans northern New Jersey and southern New York.\nAccurate streamflow forecasting within this region is therefore essential to enable effective water resource management, flood prediction, and disaster preparedness.\n\nPrecipitation data is critical for effective hydrological modeling, making the identification of reliable data sources a key priority.\nThis is why the Integrated Spatial Modeling and Remote Sensing Technologies Laboratory (I-SMART),\nan interdisciplinary research unit within the Davidson Laboratory at Stevens Institute of Technology in Hoboken, New Jersey,\nuses the latest developments in both atmospheric and hydrological modeling to address flood risks in the Hackensack Watershed\nwith solutions that could be expanded to the entire New York City Metropolitan Area."
  },
  {
    "idurl": 137,
    "idtype": "text",
    "order": 2,
    "content": "In the past, this work has included key early applications of the Next Generation Water Resources Modeling Framework (NextGen).\nNotably, the I-SMART group was among the first to force the NextGen framework with multiple atmospheric models for comparative analysis during a real-world event:\nthe passage of Superstorm Ida over the New York metropolitan area in September 2021.\nThe recent advent of NextGen In a Box (NGIAB) has provided an opportunity to accelerate these applications even further\nby taking full advantage of NGIAB's containerized, user-friendly, and easily deployed environment.\n\nRecently, the I-SMART team has been testing various regional atmospheric models grounded in physical equations,\nincluding traditional models like WRF and next-generation atmospheric models such as MPAS.\nAdditionally, given the increasing popularity and adoption of AI/ML-based approaches,\nthe team has also begun exploring their potential.\nThe goal of this work is to assess the performance of these approaches in the Hackensack Watershed,\nalong with investigating the sensitivity of the model to various meteorological forcings, including forcings based on the National Water Model.\n_(The initial and/or boundary conditions for all the models were determined using the Global Forecast System.)_"
  },
  {
    "idurl": 137,
    "idtype": "text",
    "order": 3,
    "content": "This investigation required handling a large volume of precipitation data from various models,\neach with different spatial resolutions and in some cases, such as MPAS, using unstructured grids.\nAs such, one of the key challenges was finding a hydrological modeling framework flexible enough to accommodate such diversity.\nThis made the NextGen framework a natural choice, allowing them to integrate precipitation forcings from various sources\nwith the appropriate pre-processing to align them with the model requirements in terms of spatial and temporal scales.\n\nThe complex implementation and execution of these models was faciliated by NextGen In A Box (NGIAB),\nwhich successfully enabled the integration of diverse precipitation sources.\nBy simplifying local deployment and providing full control over model inputs, configurations, and runtime operations,\nNGIAB has given the I-SMART team the tools to conduct their groundbreaking research with even greater efficiency."
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: DevCon 2025: A DevOps and Cyberinfrastructure Success Story\nsource: https://docs.ciroh.org/blog/devcon25-infra\nscraped_date: 2025-01-31\n---\n\nThe recent DevCon 2025 event showcased not just cutting-edge development practices, but also demonstrated how modern DevOps principles and cloud infrastructure can seamlessly support large-scale technical workshops. Our team had the privilege of providing IT infrastructure and support for over 200 attendees, creating a robust learning environment through an exemplary public-private partnership.\n\n![Image of CIROH's Research Cyberinfrastructure and DevOps team. On the left, two graphs are shown depicting usage for the Google Cloud-2i2c and Jetstream2 environments.](https://docs.ciroh.org/assets/images/it_team-224ec60c3871a16d47ce478c486fe8c2.png)\n\n> _CIROH's Research Cyberinfrastructure and DevOps team._\n>\n> _Left to right, top to bottom:_\n>\n> _Manjila Singh, Arpita Patel, Nia Minor, Trupesh Patel, James Halgren; Benjamin Lee._"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 2,
    "content": "## The Power of Collaboration\n\nDevCon 2025 represented an outstanding example of what's possible when public institutions and private enterprises work together:\n\n- **Corporate Sponsors**: AWS and Google Cloud provided funding and infrastructure for the event.\n- **Infrastructure Partners**: NSF JetStream and 2i2c JupyterHub delivered the computational backbone.\n- **Technical Implementation**: The CIROH Research Cyberinfrastructure and DevOps Team managed IT access and software packaging.\n\n![Image of CIROH's Research Cyberinfrastructure DevCon support]"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 3,
    "content": "## Our Technical Approach\n\nSupporting 200+ workshop attendees required careful planning and execution. Our DevOps team and partners implemented:"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 4,
    "content": "### Infrastructure as Code\n\n- Automated deployment of workshop environments\n- Scalable JupyterHub instances configured for concurrent users\n- Pre-packaged software environments ensuring consistency across all workstations"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 5,
    "content": "### Cloud-Native Solutions\n\n- Leveraged AWS S3 bucket and Google Cloud BigQuery\n- Implemented auto-scaling to handle peak workshop loads\n- Ensured high availability across multiple availability zones"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 6,
    "content": "### Seamless User Experience\n\n- Single sign-on authentication for 100+ participants using CIROH 2i2c JupyterHub's new WORKSHOP Hub!!\n- Pre-configured JetStream2 environments with all necessary tools\n- Real-time support slack channels for immediate issue resolution"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 7,
    "content": "### Key Achievements\n\n- **Zero Downtime**: Maintained 100% uptime throughout the entire event\n- **Rapid Onboarding**: All 200 attendees were able to access their environments within minutes\n- **Consistent Experience**: Every participant had identical, fully-functional development environments\n- **Real-time Support**: Our team resolved technical issues with average response time under 5 minutes"
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 8,
    "content": "## Lessons Learned\n\nThis event perfectly demonstrated how DevOps principles‚Äîbridging development and operations‚Äîcombined with cloud computing can transform organizational capabilities. Key takeaways include:\n\n1. **Automation is Essential**: Pre-event automation allowed us to focus on attendee support rather than infrastructure management.\n2. **Public-Private Partnerships Work**: Combining Google Cloud services, AWS services, 2i2c services and NSF's research infrastructure created a best-of-class research workshops solution.\n3. **Preparation Prevents Problems**: Our extensive pre-event testing and redundancy planning paid dividends during the live event."
  },
  {
    "idurl": 138,
    "idtype": "text",
    "order": 9,
    "content": "## Looking Forward\n\nThe success of DevCon 2025's IT infrastructure demonstrates that large-scale technical events no longer need to be limited by traditional IT constraints. By embracing DevOps practices and leveraging cloud partnerships, we can create learning environments that scale effortlessly while maintaining reliability and performance.\n\nWe're proud to have supported the DevCon 2025 community and look forward to applying these lessons to future events. The combination of public research infrastructure, private cloud resources, and dedicated DevOps expertise created an environment where 200+ developers could focus on learning and innovation without worrying about technical barriers.\n\n- [The Power of Collaboration](#the-power-of-collaboration)\n- [Our Technical Approach](#our-technical-approach)\n  - [Infrastructure as Code](#infrastructure-as-code)\n  - [Cloud-Native Solutions](#cloud-native-solutions)\n  - [Seamless User Experience](#seamless-user-experience)\n  - [Key Achievements](#key-achievements)\n- [Lessons Learned](#lessons-learned)\n- [Looking Forward](#looking-forward)"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote\nsource: https://docs.ciroh.org/blog/devcon25-keynote\nscraped_date: 2025-01-31\n---\n\nLast week, I had the incredible opportunity to co-present a keynote at the CIROH\nDevelopers Conference (DevCon 2025), which attracted over 200 attendees. This\npresentation, which I presented alongside Dan Ames, focused on \"CIROH HydroInformatics\nand Research Cyberinfrastructure.\" It was a fantastic experience to share insights\ninto the powerful tools and technologies that CIROH engineers, students, researchers\nhave been developing to advance hydrological research and operations.\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_7.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_0.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_1.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_2.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_3.jpeg)"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 2,
    "content": "- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_4.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_5.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_6.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_7.jpeg)\n\n- ![Arpita Patel and Dan Ames presenting their plenary keynote at CIROH DevCon 2025.](https://docs.ciroh.org/img/blog/2025-06-devcon25/keynote_0.jpeg)\n\nOur keynote aimed to showcase the comprehensive ecosystem that CIROH offers. We highlighted four key pillars:"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 3,
    "content": "- **Computing Resources**: CIROH offers accress to public cloud infrastructure, on-premises HPC, and NSF ACCESS resources.\n- **Data Management**: CIROH members handle and share vast datasets crucial for hydrological modeling. We've provided platforms to do so through [HydroShare](https://hydroshare.org/) and CIROH AWS S3 buckets, as well as streamlining access and analysis of this data through tools like Google BigQuery API and Tethys Platform.\n- **Model Development**: CIROH develops tools and frameworks for developing and refining hydrological models, including the [NGIAB ecosystem](https://ngiab.ciroh.org/).\n- **Knowledge Sharing**: CIROH disseminates findings and best practices through the [DocuHub](https://docs.ciroh.org/) and [Portal](https://portal.ciroh.org/) platforms.\n\nCIROH DevCon 2025 Keynote - CIROH Hydroinformatics & Research Cyberinfrastructure - YouTube\n\n[Photo image of CIROH DocuHub]\n\nCIROH DocuHub\n\n2 subscribers\n\n[CIROH DevCon 2025 Keynote - CIROH Hydroinformatics & Research Cyberinfrastructure](https://www.youtube.com/watch?v=dPuCSRYjjW8)\n\n_Slides are available here: [View slideshow](https://docs.ciroh.org/assets/files/DevCon25-PlenaryKeyNote-Arpita-Dan-67d90820666d6780a04b5512bc0572df.pdf)_"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 4,
    "content": "## Value and Impact\n\nDuring the keynote, we emphasized the value that CIROH brings to new students and researchers. This includes access to:\n\n- Computational resources (free for researchers) that would normally cost thousands of dollars.\n- Datasets that would take months to compile.\n- Tools that streamline research and increase its impact.\n- Publications - [https://portal.ciroh.org/publications](https://portal.ciroh.org/publications)"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 5,
    "content": "# CIROH Ecosystem Tools\n\nWe also showcased several transformative tools within the CIROH ecosystem that are advancing the field of hydrology:"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 6,
    "content": "- **NGIAB (NextGen In A Box) ecosystem** ( [ngiab.ciroh.org](https://ngiab.ciroh.org/)): Revolutionizes water modeling by making the NextGen framework portable and accessible through containerized, open-source solutions.\n- **CIROH DocuHub** ( [docs.ciroh.org](https://docs.ciroh.org/)): Your comprehensive knowledge center featuring technical documentation, monthly insights, and the latest developments in water science.\n- **CIROH Portal** ( [portal.ciroh.org](https://portal.ciroh.org/)): A unified gateway connecting researchers to essential data, cutting-edge tools, and collaborative research opportunities.\n- **Google BigQuery NWM API** ( [Documentation](https://docs.ciroh.org/docs/products/Data%20Management%20and%20Access%20Tools/bigquery-api/)): Streamlines National Water Model data access with powerful cloud-based querying capabilities for efficient analysis.\n- **Tethys Platform**: Transforms complex hydrological data and models into intuitive web applications, making advanced water science accessible to all stakeholders.\n- **AWS, Google Cloud, 2i2c JupyterHub and NSF Access**: CIROH's AWS, Google Cloud accounts, CIROH 2i2c JupyuterHub and NSF Allocations for cloud computing in hydrology.\n- **Pantarhei and Wukong HPC**: CIROH's state-of-the-art high-performance computing infrastructure, powering computationally intensive research and simulations.\n- And many more innovative solutions driving water science forward!"
  },
  {
    "idurl": 139,
    "idtype": "text",
    "order": 7,
    "content": "Our overall message was that CIROH's hydroinformatics and research cyberinfrastructure\necosystem is designed to support and amplify research efforts. We encouraged attendees\nto explore these resources and consider how they could be applied to their own work.\nWhether it's streamlining data workflows, tackling computationally intensive tasks,\nor sharing findings, CIROH provides the tools and infrastructure to push the boundaries\nof hydrological science.\n\nWe want to thank everyone who attended our keynote and engaged in the Mentimeter quiz.\nIt's an exciting time for CIROH, and we're thrilled to be a part of this dynamic community!\n\nCIROH Research CyberInfrastructure and DevOps Team - YouTube\n\n[Photo image of CIROH DocuHub]\n\nCIROH DocuHub\n\n2 subscribers\n\n[CIROH Research CyberInfrastructure and DevOps Team](https://www.youtube.com/watch?v=zE3uFvDCSs4)\n\n> _Video voiced by Quinn Lee and prepared by Manjila Singh, Nia Minor, and Arpita Patel._\n\n- [Value and Impact](#value-and-impact)"
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights\nsource: https://docs.ciroh.org/blog/ewri-devcon25-ngiab\nscraped_date: 2025-01-31\n---\n\n![AWI Science and Technology Team @ CIROH DevCon2025](https://docs.ciroh.org/assets/images/DevCon_AWI_Team-c9f08b48601493edf8e47d7478fdcfff.png)\n\n> _CIROH-AWI Science and Technology Team._\n>\n> _Left to right: Sagy Cohen, Steven Burian, Manjila Singh, Saide Zand, Savalan N. Neisary, Arpita Patel, Nia Minor, Trupesh Patel, Sifan A. Koriche, Jonathan Frame, Reza S. Alipour, Hari T. Jajula, Chad Perry; Josh Cunningham._\n\nMay was a pivotal month for representing the [Cooperative Institute for Research to Operations in Hydrology (CIROH)](https://ciroh.ua.edu/) and our collective work in advancing water science. As one of CIROH's Ambassadors, I had the privilege of connecting with the broader scientific community at two key events: the [Environmental and Water Resources Institute (EWRI) Congress](https://www.ewricongress.org/) in Anchorage, Alaska, and the [2025 CIROH Developers Conference](https://ciroh.ua.edu/devconference/2025-ciroh-developers-conference/) in Burlington, Vermont."
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 2,
    "content": "### Empowering the Community at the [2025 CIROH Developers Conference](https://ciroh.ua.edu/devconference/2025-ciroh-developers-conference/)\n\nThe annual [**CIROH Developers Conference**](https://ciroh.ua.edu/devconference/2025-ciroh-developers-conference/) is a premier three-day event that convenes hundreds of scientists, engineers, and software developers from across the consortium and its partner institutions. It serves as a vital hub for innovation and collaboration, where attendees engage with the latest research findings, project highlights, and hands-on workshops on cutting-edge tools, workflows, and methodologies.\n\nFor me, this year's conference in the beautiful city of [Burlington, Vermont](https://www.google.com/maps/place/Burlington,+VT/), felt like coming full circle, as it marked my one-year anniversary since joining the consortium through [The University of Alabama](https://www.ua.edu/)."
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 3,
    "content": "I had the privilege of leading a [hands-on training workshop on hydrological model calibration](https://github.com/skoriche/NGIAB-Calibration-DevCon25). It was a fantastic experience engaging with the talented developers, researchers, and scientists who are all dedicated to the advancement and acceleration of Community Water Resources Modeling. Our session provided comprehensive, hands-on guidance for calibrating one of core components of the NextGen National Water Model model-confifuration‚Äîspecifically the [Conceptial Functional Equivalent (CFE)](https://github.com/NOAA-OWP/cfe) coupled with [Noah-OWP-Modular](https://github.com/NOAA-OWP/noah-owp-modular)‚Äîwithin the [**NextGen In A Box (NGIAB) ecosystem**](https://ngiab.ciroh.org/).\n\nThe [NGIAB ecosystem](https://ngiab.ciroh.org/) is a comprehensive platform that revolutionizes how we approach hydrological research with integrated capabilities for:\n\n- ‚ú® **1. Data preprocessing**\n- ‚öôÔ∏è **2. Hydrology model simulations and calibration**\n- üìä **3. Advanced evaluation tools**\n- üìà **4. Rich visualization capabilities**\n\nA huge thank you to [CIROH](https://ciroh.ua.edu/) for organizing such an insightful conference, and to the [University of Vermont](https://www.uvm.edu/) for being an excellent host!\n\nFor everyone who attended or is interested in learning more, all training materials, codes, and data from the workshop are openly available in our GitHub repository:"
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 4,
    "content": "‚û°Ô∏è **[NGIAB-Calibration-DevCon25 Workshop Materials](https://github.com/skoriche/NGIAB-Calibration-DevCon25)**\n\n‚û°Ô∏è **More NextGen Workshops**: [DocuHub News](https://docs.ciroh.org/news); May 2025 Updates\n\nI'm inspired by the passion for innovation shown and look forward to seeing how the community leverages these tools to advance water modeling.\n\n![A speaker presents at a conference, with a large screen displaying presentation slides and organization logos.](https://docs.ciroh.org/assets/images/Photo-DevCon2025-382da3028e559ae6994ea0d35240e243.png)"
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 5,
    "content": "### Disseminating Research at the [EWRI Congress](https://www.ewricongress.org/)\n\nThe EWRI Congress was a valuable opportunity to disseminate our research and connect with hundreds of engineers and scientists. I had the privilege of delivering three oral presentations that showcase the use-inspired science outcomes emerging from the CIROH Science and Technology Team and our partners."
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 6,
    "content": "#### [1. Advancing Hydrologic Modeling through Community-Driven Development: The NextGen Framework](https://github.com/CIROH-UA)\n\nThis [presentation](https://alaska2025.eventscribe.net/fsPopup.asp?efp=SUlaTkJWTUEyNDE5NA&PresentationID=1583570&rnd=0.1763265&mode=presInfo) introduced the Community NextGen initiative, a collaborative effort aimed at fostering open innovation and accelerating the framework's development. These efforts aim to facilitate efficient research-to-operations (R2O) and operations-to-research (O2R) transitions, ultimately enhancing our ability to model and predict water resources at various scales."
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 7,
    "content": "#### [2. Enhancing Catchment Based Hydrological Model Performance through Dynamic Sub-Discretization Using Land Surface Attributes](https://alaska2025.eventscribe.net/fsPopup.asp?PresentationID=1583638&mode=presInfo)\n\nThis talk addressed how the simplifications in lumped models can lead to reduced accuracy when applied uniformly in catchments with significant land-surface heterogeneity. The study explores the potential to enhance lumped model performance by introducing dynamic discretization based on key land-surface heterogeneities such as land use, soil type and soil layering, slope aspect, and elevation."
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 8,
    "content": "#### [3. Multi-model Predictions of Design Low Flow Conditions in the Great Salt Lake Basin Using NextGen Water Resources Modeling Framework](https://alaska2025.eventscribe.net/fsPopup.asp?efp=SUlaTkJWTUEyNDE5NA&PresentationID=1603025&rnd=0.5338731&mode=presInfo)\n\nThis research focused on the challenge of accurately estimating low flow conditions in ungauged basins, such as those in the Great Salt Lake (GSL) region. The study paves the way to investigate how different models perform in predicting design low flow, a critical standard for water supply planning, management, and ensuring water quality in the GSL basin at both gauged and ungauged locations.\n\nThese works directly reflects the ongoing efforts and use-inspired science outcomes of the [CIROH Science and Technology Team and Partners across the consortium](https://ngiab.ciroh.org/#:~:text=Alabama-,Water,-Institute). The congress served as a successful platform for showcasing these advancements and CIROH's leadership in community-driven, next-generation water resource prediction.\n\n![A speaker presents at the World Environmental & Water Resources Congress, with a large screen displaying presentation details and a 'Say Cheese' graphic with a QR code.](https://docs.ciroh.org/assets/images/EWRI-Koriche_MD-c223004f78849bdc7a5902671cc47ebf.png)"
  },
  {
    "idurl": 140,
    "idtype": "text",
    "order": 9,
    "content": "- [Empowering the Community at the 2025 CIROH Developers Conference](#empowering-the-community-at-the-2025-ciroh-developers-conference)\n- [Disseminating Research at the EWRI Congress](#disseminating-research-at-the-ewri-congress)"
  },
  {
    "idurl": 141,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Œ¥HBV2.0: How NGIAB and Wukong HPC Streamlined Advanced Hydrologic Modeling\nsource: https://docs.ciroh.org/blog/may-2025-update\nscraped_date: 2025-01-31\n---\n\n![Image of graphical outputs from the Œ¥HBV2.0 model](https://docs.ciroh.org/assets/images/img-e699e9df162042eda9a4a1b4a242ffb1.jpg)\n\nPredicting water flow with precision across the vast U.S. landscape is a complex challenge. That's why Song et al. 2024 developed Œ¥HBV2.0, a cutting-edge hydrologic model. It's built with high-resolution modeling of physics to deliver seamless, highly accurate streamflow simulations, even down to individual sub-basins. It's already proven to be a major improvement, performing better than older tools at about 4,000 measurement sites. We also provide a comprehensive 40-year water dataset for ~180,000 river reaches to support this.\n\nPenn State research group pushed Œ¥HBV2.0 further, training it with even more detailed river data and integrating other trusted models, aiming to make it a key part of the NextGen national water modeling system (as a potential NWM3.0 successor). But here's a common hurdle: making powerful scientific tools like this easy and reliable for everyone to use within a larger framework can be tough. Setup issues, runtime errors, and inconsistent results can frustrate users."
  },
  {
    "idurl": 141,
    "idtype": "text",
    "order": 2,
    "content": "NGIAB stepped in to solve exactly this problem. Team has taken the complexity out of using the operations-ready models within NextGen by creating one unified, reliable package. Thanks to NGIAB, users don't have to worry about tricky setups or whether the model will run correctly. NGIAB ensures that our models are compatible everywhere and, most importantly, that they run exactly as designed, consistently and faithfully, every single time, no babysitting required. This means users get the full power of our advanced modeling, without the headaches."
  },
  {
    "idurl": 141,
    "idtype": "text",
    "order": 3,
    "content": "## üîó Learn more\n\nŒ¥HBV2.0: [https://doi.org/10.5281/zenodo.14827983](https://doi.org/10.5281/zenodo.14827983)\n\nSong et al. 2024: [https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024WR038928](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024WR038928)\n\nDataset: [https://doi.org/10.5281/zenodo.13774373](https://doi.org/10.5281/zenodo.13774373)\n\n- [üîó Learn more](#-learn-more)"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Google Cloud Next 2025: Innovation at Scale ‚ú®\nsource: https://docs.ciroh.org/blog/april-2025-update\nscraped_date: 2025-01-31\n---\n\nLast week at Google Cloud Next representing our CIROH cloud-based computing efforts! With more than 30,000 participants, Google Next always amazes me! It's huge, engaging on so many levels! Engaging booths, networking opportunities, great presentations, workshops, AI coach for basketball, incredible keynote from an amazing team! Event was not just a conference, but a celebration of innovation and a glimpse into the future of cloud computing!\nGreat to see how Gemini is transforming data manipulation in BigQuery. The ability to use natural language to query, transform, and visualize data is revolutionizing how we interact with massive datasets. Gabe Weiss's demo particularly showcased the potential for non-specialists to derive insights from complex data.\n\nIf you missed the keynote, I highly recommend watching the recording here: [GCN25 Keynote Video](https://www.youtube.com/live/VABwMpL3JCo?t=3564s)\n\n- ![BigQuery and Earth Engine demo at Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-6.jpg)\n\n- ![Arpita Patel at the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-1.jpg)\n\n- ![Title slide from the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-2.jpg)\n\n- ![Slide depicting Google's AI stack](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-3.jpg)"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 2,
    "content": "- ![Slide announcing Gemini for Google Distributed Cloud](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-4.jpg)\n\n- ![Slide announcing Google Agentspace](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-5.jpg)\n\n- ![BigQuery and Earth Engine demo at Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-6.jpg)\n\n- ![Arpita Patel at the Google Cloud Next conference](https://docs.ciroh.org/img/blog/2025-04-gcp/gcp-1.jpg)"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 3,
    "content": "## üì¢ Google Distributed Cloud: Bringing Gemini On-Premises\n\nPerhaps the most exciting announcement was that Gemini is now available on-premises via Google Distributed Cloud. This is a game-changer for organizations with strict data sovereignty requirements or specialized computing needs. Running on NVIDIA Confidential Computing and NVIDIA Blackwell infrastructure, this solution brings together:\n\n- Cutting-edge AI innovation\n- Robust data sovereignty controls\n- Enterprise-grade security\n\nThis addresses the most significant barriers many organizations face when adopting advanced AI capabilities. Learn more about this integration here: [NVIDIA and Google Cloud Partnership](https://blogs.nvidia.com/blog/google-cloud-next-agentic-ai-reasoning/?ncid=so-link-994345)"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 4,
    "content": "## üì¢ Infrastructure Advances: Power to Transform\n\nThe infrastructure announcements were equally impressive, with serious power upgrades including:\n\n- Ironwood TPUs offering massive acceleration for machine learning workloads\n- NVIDIA Blackwell GPUs providing unprecedented computing power\n- Rubin GPUs extending capabilities for specialized workloads\n\nThese hardware innovations will enable entirely new classes of applications and make existing workloads significantly more efficient.\n\n* * *\n\nCheck out all the online content that's now available, including:\n\n- A 10-minute recap of the **[opening keynote](https://www.youtube.com/watch?v=dwgmfSOZNoQ)**\n\n- A replay of the **[developer keynote](https://cloud.withgoogle.com/next/25/session-library?session=DEVKEY&utm_source=cloud_sfdc&utm_medium=email&utm_campaign=FY25-Q2-global-EXP106-physicalevent-er-next25-mc&utm_content=global_next25_15a_Op_TY_Gen_Physical_P0&utm_term=-&pref=K&mkt_tok=ODA4LUdKVy0zMTQAAAGZ0-XTI1zyO8s06QZoyvgraE3UtbUUHR0uePcaqzFX_xqCAKR4ho4Qwooku00DFireAGAYlcr90vjFdOkYug_UvoFWtCCvWZQ2D0Bc0I3mEv-Pq4aQOho#all)**"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 5,
    "content": "- **[All 200+ announcements at Next 25](https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2025-wrap-up?e=48754805&utm_source=cloud_sfdc&utm_medium=email&utm_campaign=FY25-Q2-global-EXP106-physicalevent-er-next25-mc&utm_content=global_next25_15a_Op_TY_Gen_Physical_P0&utm_term=-&pref=K&mkt_tok=ODA4LUdKVy0zMTQAAAGZ0-XTI69fG5pO1vzMqcs_tymd4-sQLUtRJH6j9g_F6FnNE3GwFkGx5qaC_d7bpF6BtvFK849zOz5VS6dCHtDlYAj-Pu4us2FvFnOic887xHl2eU-7N7U)**\n\n- **[More than 150 sessions, demos, and workshops](https://cloud.withgoogle.com/next/25/session-library?mkt_tok=ODA4LUdKVy0zMTQAAAGZ0-XTI3kqz4RKnkWcKk0PQCYROhFCDELXMik1KDwmzDWf41vgL_ELHIf_FBVQXs_dqY4WynUDvgOkY-uWPcTqY1LFvQ5_ZTu-4G3DF3Kiw3jjD2WoUlc#all)**\n\n- **[A recap of the event's biggest announcements](https://blog.google/products/google-cloud/next-2025/?utm_source=cloud_sfdc&utm_medium=email&utm_campaign=FY25-Q2-global-EXP106-physicalevent-er-next25-mc&utm_content=global_next25_15a_Op_TY_Gen_Physical_P0&utm_term=-&pref=K&mkt_tok=ODA4LUdKVy0zMTQAAAGZ0-XTI_yfyMB7QGUk4YFUIc7l-BY4tQW4ngSexZ5T54GQiwNwRk29ld_8wAYWVv39I8lBbtKokyjv8Wrdht9YFpHFVhMj8QeYF5Npd2zgjzvC_iX5wnQ)**, all in one place\n\n* * *"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 6,
    "content": "## üì¢ Agent Development: The Next Frontier\n\nWhat particularly caught my attention was Google's focus on agent development technologies:\n\n- **google-adk**: The Agent Development Kit makes building multi-agent applications accessible to developers without deep AI expertise. [Learn more](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/)\n- **A2A (Agent-to-Agent Communication)**: The frameworks for agent-to-agent interaction open up possibilities for complex automated workflows that were previously impossible. [Explore A2A](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/) or jump straight to the [quickstart code samples](https://google.github.io/A2A/#/)\n- **Agentspace**: This collaboration environment for multiple agents feels like the beginning of something truly transformative. [Check it out](https://cloud.google.com/blog/products/ai-machine-learning/google-agentspace-enables-the-agent-driven-enterprise)"
  },
  {
    "idurl": 142,
    "idtype": "text",
    "order": 7,
    "content": "## Final Thoughts\n\nAs I reflect on my time at Google Cloud Next 2025, I'm struck by how the landscape of cloud computing and AI is evolving faster than ever. The convergence of powerful infrastructure, sophisticated AI models like Gemini, and new paradigms for agent-based computing signals a fundamental shift in how we'll architect and manage enterprise systems in the coming years.\n\nI left the conference not just with new knowledge but with a renewed sense of excitement about the possibilities ahead. The future of cloud computing isn't just about faster or cheaper computing resources‚Äîit's about fundamentally new ways of solving problems and creating value.\n\n#GoogleCloudNext25 #CloudAdvocate #AI #A2A #VertexAI #GoogleDistributedCloud #DevOps #EnterpriseArchitecture\n\n- [üì¢ Google Distributed Cloud: Bringing Gemini On-Premises](#-google-distributed-cloud-bringing-gemini-on-premises)\n- [üì¢ Infrastructure Advances: Power to Transform](#-infrastructure-advances-power-to-transform)\n- [üì¢ Agent Development: The Next Frontier](#-agent-development-the-next-frontier)\n- [Final Thoughts](#final-thoughts)"
  },
  {
    "idurl": 143,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: üåü UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolutionüåü\nsource: https://docs.ciroh.org/blog/march-2025-update\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 143,
    "idtype": "text",
    "order": 2,
    "content": "## üåç AWI News\n\nThe Alabama Water Institute (AWI) at the University of Alabama (UA) recently published an article highlighting how NextGen In A Box (NGIAB) could transform hydrological modeling. This article provides great insight into NGIAB's real-world impact:\n\n- üöÄ **30-minute setup** vs days/weeks of configuration\n- üìñ **Provo River Basin Case Study** demonstrating rapid deployment\n\n![ngiab image](https://docs.ciroh.org/img/logos/ngiab.png)\n\n[‚û°Ô∏è Read the full press release here!](https://awi.ua.edu/news/nextgen-in-a-box-ngiab-revolutionizing-hydrological-modeling-with-a-30-minute-setup/)\n\n* * *\n\n- [üåç AWI News](#-awi-news)"
  },
  {
    "idurl": 151,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Developers Conference 2024\nsource: https://docs.ciroh.org/blog/devcon24-retrospective\nscraped_date: 2025-01-31\n---\n\n**CIROH Developers Conference 2024**\n\n![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_01.jpeg)\n\nThe CIROH team recently participated in the **2nd Annual CIROH Developers Conference (DevCon24)**, held from May 29th to June 1st,2024. The conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field of hydrological forecasting.\n\nReflecting CIROH's current research focus, the conference explored topics including hydrological modeling (NextGen), flood inundation mapping, hydroinformatics, social science, and community engagement. Attendees got the opportunity to delve deeper into specific areas through its well-structured training track. This year the tracks were:\n\n- **NextGen**\n- **Flood Inundation Mapping (FIM)**\n- **Hydrological Applications of Machine Learning (ML)**\n- **Hydroinformatics**\n- **Cross-cutting**\n\nThis year, various workshops leveraged cloud technologies. Notably, we provided access to the 2i2c JupyterHub environment, a cloud-based platform for interactive computing, for ten workshops. This facilitated seamless access to powerful computing resources for participants. Additionally, we provided AWS instances to support four workshops.\n\n![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_04.JPG)"
  },
  {
    "idurl": 151,
    "idtype": "text",
    "order": 2,
    "content": "![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_06.png)\n\n**Presentation Slides:**\n\nYou can find the presentation slides [here](https://github.com/CIROH-UA/Conferences/tree/main/CIROHDevCon2024/NextGenTrack). To learn more about CIROH's work or connect with the team, visit our website at [CIROH-website](https://ciroh.ua.edu/).\n\n**Conference Website:**\n\n[Learn More](https://ciroh.ua.edu/devconference/)\n\n![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_02.jpeg)\n\n![DevCon2024](https://docs.ciroh.org/img/blog/2024-05-devcon24/devcon24_05.jpg)"
  }
]