{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7f837d",
   "metadata": {},
   "source": [
    "# CIROH DocuHub - Database Population and Content Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaed94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81918d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "load_dotenv()\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "BASE_URL = \"https://docs.ciroh.org\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the HTML page to scrape the site structure\n",
    "HTML_URL = \"https://docs.ciroh.org/docs/products/intro\"\n",
    "\n",
    "# Download and parse the HTML content\n",
    "response = requests.get(HTML_URL)\n",
    "response.raise_for_status()  # Raise an error if the download fails\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Top-bar navigation links\n",
    "navbar = soup.find(\"nav\", {\"aria-label\": \"Main\"})\n",
    "\n",
    "# Find the left and right blocks\n",
    "navbar_left = navbar.find(\"div\", class_=\"theme-layout-navbar-left\")\n",
    "navbar_right = navbar.find(\"div\", class_=\"theme-layout-navbar-right\")\n",
    "\n",
    "top_level_links = []\n",
    "\n",
    "# 1. Explicit capture of DocuHub\n",
    "docuhub_link = navbar_left.find(\"a\", class_=\"navbar__brand\")\n",
    "if docuhub_link:\n",
    "    name = docuhub_link.get_text(strip=True)\n",
    "    href = docuhub_link.get(\"href\")\n",
    "    top_level_links.append({\"name\": name, \"url\": href})\n",
    "\n",
    "# 2. Left block (except CIROH Portal)\n",
    "for a in navbar_left.find_all(\"a\", class_=\"navbar__item\"):\n",
    "    name = a.get_text(strip=True)\n",
    "    href = a.get(\"href\")\n",
    "    if \"portal\" in name.lower():\n",
    "        continue  # Omit CIROH Portal\n",
    "    top_level_links.append({\"name\": name, \"url\": href})\n",
    "\n",
    "# 3. Right block\n",
    "for a in navbar_right.find_all(\"a\", class_=\"navbar__item\"):\n",
    "    name = a.get_text(strip=True)\n",
    "    href = a.get(\"href\")\n",
    "    top_level_links.append({\"name\": name, \"url\": href})\n",
    "\n",
    "# Show results\n",
    "for link in top_level_links:\n",
    "    print(f'Name: {link[\"name\"]:20} | URL: {link[\"url\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b958594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databes connection\n",
    "conn = psycopg2.connect(\n",
    "    host=PG_HOST,\n",
    "    database=PG_DB,\n",
    "    user=PG_USER,\n",
    "    password=PG_PASS\n",
    ")\n",
    "\n",
    "def execute_query(conn, query, params=None, fetch=False):\n",
    "    cur = conn.cursor()\n",
    "    \"\"\"Execute a SQL query with optional parameters.\"\"\"\n",
    "    try:\n",
    "        if params:\n",
    "            cur.execute(query, params)\n",
    "        else:\n",
    "            cur.execute(query)\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "            return result\n",
    "        else:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b761f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert links into the database\n",
    "for link in top_level_links:\n",
    "    execute_query(conn, \"\"\"\n",
    "        INSERT INTO tblurls (url, idurlparent, name)\n",
    "        VALUES (%s, NULL, %s)\n",
    "        ON CONFLICT DO NOTHING\n",
    "    \"\"\", (link[\"url\"], link[\"name\"]))\n",
    "    print(f'Inserted: {link[\"name\"]} | {link[\"url\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65496060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually insert a page into the database that is not in the top-level links\n",
    "execute_query(conn, \"\"\"\n",
    "    INSERT INTO tblurls (url, idurlparent, name)\n",
    "    VALUES (%s, (SELECT idurl FROM tblurls WHERE name = %s), %s)\n",
    "\"\"\", (\"/docs/contribute\", \"DocuHub\", \"Contributing to CIROH DocuHub\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9302ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs from the left sidebar menu\n",
    "def extract_menu_links(soup):\n",
    "    \"\"\"\n",
    "    Extracts all visible links in the left sidebar menu of the current page.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    main_ul = soup.find(\"ul\", class_=\"theme-doc-sidebar-menu menu__list\")\n",
    "    if not main_ul:\n",
    "        return results\n",
    "    for a_tag in main_ul.find_all(\"a\", class_=\"menu__link\"):\n",
    "        url = a_tag.get(\"href\")\n",
    "        name = a_tag.get_text(strip=True)\n",
    "        if url:\n",
    "            results.append({\"url\": url, \"name\": name})\n",
    "    return results\n",
    "\n",
    "def extract_menu(start_url, verbose=False):\n",
    "    \"\"\"\n",
    "    Recursively extracts the full left sidebar menu structure from a given starting URL.\n",
    "    Returns a list of dictionaries with: url, urlparent, name.\n",
    "    \"\"\"\n",
    "    queue = [start_url]\n",
    "    extracted = []\n",
    "    visited = set()\n",
    "\n",
    "    while queue:\n",
    "        current_url = queue.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        page_url = BASE_URL + current_url\n",
    "        if verbose:\n",
    "            print(f\"Processing: {page_url}\")\n",
    "        try:\n",
    "            resp = requests.get(page_url)\n",
    "            if resp.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            links = extract_menu_links(soup)\n",
    "            for link in links:\n",
    "                # Only add if this URL is not already in 'extracted'\n",
    "                if not any(x['url'] == link['url'] for x in extracted):\n",
    "                    extracted.append({\n",
    "                        \"url\": link['url'],\n",
    "                        \"urlparent\": current_url,\n",
    "                        \"name\": link['name']\n",
    "                    })\n",
    "                    if link['url'] not in visited and link['url'] not in queue:\n",
    "                        queue.append(link['url'])\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {page_url}: {e}\")    \n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert link from left sidebar menu into the database\n",
    "urls_to_insert = execute_query(conn, \"\"\"\n",
    "    SELECT url FROM tblurls \n",
    "    WHERE name in ('Products', 'Services', 'Policies', 'Contributing to CIROH DocuHub') \n",
    "    ORDER BY idurl\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "\n",
    "for url in urls_to_insert:\n",
    "    url = url[0]  # unpack tuple\n",
    "    print(f\"Extracting menu for {url}...\")\n",
    "    df = pd.DataFrame(extract_menu(url, verbose=False))\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if the URL already exists in the database\n",
    "        exists = execute_query(conn, \"\"\"\n",
    "            SELECT 1 FROM tblurls WHERE url = %s\n",
    "        \"\"\", (row['url'],), fetch=True)\n",
    "        if not exists:\n",
    "            # Insert the new URL into the database\n",
    "            execute_query(conn, \"\"\"\n",
    "                INSERT INTO tblurls (url, idurlparent, name)\n",
    "                VALUES (%s, (SELECT idurl FROM tblurls WHERE url = %s), %s)\n",
    "            \"\"\", (row['url'], row['urlparent'], row['name']))\n",
    "            print(f'Inserted: {row[\"name\"]} | {row[\"url\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ba313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract blog links from the sidebar\n",
    "def extract_blog_links(start_url=\"/blog\"):\n",
    "    \"\"\"\n",
    "    Extracts all blog post URLs and titles from the blog sidebar.\n",
    "    Returns a list of dicts with url, urlparent, and name.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    page_url = BASE_URL + start_url\n",
    "    resp = requests.get(page_url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Failed to load {page_url}\")\n",
    "        return results\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # Find the sidebar container\n",
    "    sidebar = soup.find(\"nav\", class_=\"sidebar_brwN\")\n",
    "    if not sidebar:\n",
    "        print(\"Sidebar not found.\")\n",
    "        return results\n",
    "    \n",
    "    # Loop through each year group\n",
    "    for group in sidebar.find_all(\"div\", role=\"group\"):\n",
    "        \n",
    "        ul = group.find(\"ul\", class_=\"sidebarItemList_QwSx\")\n",
    "        print(f\"Found ul: {ul}\")\n",
    "        if not ul:\n",
    "            continue\n",
    "        \n",
    "        for a_tag in ul.find_all(\"a\", class_=\"sidebarItemLink_yNGZ\"):\n",
    "            url = a_tag.get(\"href\")\n",
    "            name = a_tag.get_text(strip=True)\n",
    "            if url:\n",
    "                results.append({\n",
    "                    \"url\": url,\n",
    "                    \"urlparent\": start_url,\n",
    "                    \"name\": name\n",
    "                })\n",
    "                print(f'Found blog link: {name} | {url}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert blog links into the database\n",
    "blog_links = extract_blog_links(\"/blog\")\n",
    "\n",
    "for link in blog_links:\n",
    "    exists = execute_query(conn, \"\"\"\n",
    "        SELECT 1 FROM tblurls WHERE url = %s\n",
    "    \"\"\", (link['url'],), fetch=True)\n",
    "    if not exists:\n",
    "        # Insert the new URL into the database\n",
    "        execute_query(conn, \"\"\"\n",
    "            INSERT INTO tblurls (url, idurlparent, name)\n",
    "            VALUES (%s, (SELECT idurl FROM tblurls WHERE url = %s), %s)\n",
    "        \"\"\", (link['url'], link['urlparent'], link['name']))\n",
    "        print(f'Inserted: {link[\"name\"]} | {link[\"url\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for generating summaries\n",
    "\n",
    "def build_prompt(markdown_content):\n",
    "    \"\"\"\n",
    "    Construye el prompt completo como un solo string, insertando el contenido markdown.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "# ROLE & GOAL\n",
    "You are an AI Knowledge Architect specializing in hydrology and scientific computing for the Cooperative Institute for Research to Operations in Hydrology (CIROH). Your task is to process the content of a webpage from CIROH's DocuHub documentation site.\n",
    "\n",
    "# CONTEXT\n",
    "The output you generate will be stored in a PostgreSQL database to power a Retrieval-Augmented Generation (RAG) system. Specifically, your response will populate a `JSONB` field. The `summary_text` within your JSON response will be used to create a 3072-dimension vector embedding for semantic search. Therefore, the summary must be information-dense, conceptually accurate, and optimized to be found by user queries about CIROH's products, services, policies, and research activities.\n",
    "\n",
    "# CONTENT TO ANALYZE\n",
    "---\n",
    "**[BEGINNING OF MARKDOWN CONTENT]**\n",
    "\n",
    "{markdown_content}\n",
    "\n",
    "**[END OF MARKDOWN CONTENT]**\n",
    "---\n",
    "\n",
    "# INSTRUCTIONS\n",
    "Based on the content provided above, generate a single JSON object. Follow these rules precisely:\n",
    "\n",
    "1.  **Focus on \"What\" and \"Why\"**: The `summary_text` should clearly identify the page's core purpose. Is it a product description, a tutorial, a policy document, a release note, or a blog post?\n",
    "2.  **Extract Named Entities**: The `entities` array must include all specific names of products, services, tools, technologies, standards, or key collaborators mentioned (e.g., \"NextGen in A Box\", \"Research Datastream\", \"Hydrofabric\", \"NOAA\", \"BYU Hydroinformatics Courses\").\n",
    "3.  **Capture Key Concepts**: The `keywords` array should capture the main technical or thematic concepts, not just proper nouns (e.g., \"hydrologic modeling\", \"flood inundation mapping\", \"water quality\", \"data management\", \"cloud computing\", \"machine learning\").\n",
    "4.  **Synthesize, Don't Quote**: Do not copy sentences directly from the text. Synthesize the information in your own words to create a coherent and semantically rich summary.\n",
    "5.  **Be Dense and Direct**: Start the summary directly with the subject. Avoid introductory phrases. For example, instead of writing \"This page describes NextGen In A Box...\", you must write \"NextGen In A Box is a community-accessible...\". Be direct and objective.\n",
    "\n",
    "# REQUIRED OUTPUT FORMAT\n",
    "Generate a single, valid JSON object with the following structure. Do not add any text or explanation before or after the JSON object.\n",
    "\n",
    "{{\n",
    "  \"summary_text\": \"A single, dense paragraph summarizing the content according to the instructions.\",\n",
    "  \"keywords\": [\"An\", \"array\", \"of\", \"string\", \"keywords\"],\n",
    "  \"entities\": [\"An\", \"array\", \"of\", \"string\", \"entities\"],\n",
    "  \"document_type\": \"A single string classifying the document (e.g., 'Product Description', 'Tutorial', 'Policy', 'Release Note', 'Blog Post')\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ce794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary from markdown content\n",
    "\n",
    "SCRAPED_PATH = \"../CIROH-scraper/docs_ciroh_org\"\n",
    "\n",
    "def generate_summary(url_markdown):\n",
    "\n",
    "    def load_markdown_content(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        \n",
    "    # Match URL with Scraped content\n",
    "    processed_path = url_markdown.lstrip('/')\n",
    "    processed_path = processed_path.removeprefix(\"docs/\")\n",
    "    processed_path = processed_path.replace(\"release-notes/\", \"release_notes/\")\n",
    "    processed_path = processed_path.replace(\" \", \"%20\")\n",
    "\n",
    "    full_path = os.path.join(SCRAPED_PATH, processed_path, \"index.md\")\n",
    "\n",
    "    markdown_content = load_markdown_content(full_path)\n",
    "\n",
    "    if markdown_content is None:\n",
    "        print(f\"Markdown content not found at: {full_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI Knowledge Architect specializing in hydrology and scientific computing for the Cooperative Institute for Research to Operations in Hydrology (CIROH).\"},\n",
    "                {\"role\": \"user\", \"content\": build_prompt(markdown_content)},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            # temperature=0.01\n",
    "            reasoning_effort = \"minimal\",\n",
    "            verbosity = \"low\"\n",
    "        )\n",
    "\n",
    "        summary_json = json.loads(response.choices[0].message.content.strip())\n",
    "        print(f\"Generated summary for {url_markdown}\")\n",
    "        return summary_json\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during summary generation for {url_markdown}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_summarize = execute_query(conn, \"\"\"\n",
    "    SELECT idurl, url FROM tblurls ORDER BY idurl LIMIT 10;\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "for idurl, url in urls_to_summarize:\n",
    "    summary_json = generate_summary(url)\n",
    "    if summary_json is not None:\n",
    "        # Insert the summary into the database\n",
    "\n",
    "        print(f'Generated summary for idurl: {idurl}: {summary_json}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_summarize = execute_query(conn, \"\"\"\n",
    "    SELECT idurl, url FROM tblurls ORDER BY idurl;\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "for idurl, url in urls_to_summarize:\n",
    "    summary_json = generate_summary(url)\n",
    "    if summary_json is not None:\n",
    "        # Insert the summary into the database\n",
    "        execute_query(conn, \"\"\"\n",
    "            UPDATE tblurls\n",
    "            SET summary_data = %s\n",
    "            WHERE idurl = %s\n",
    "        \"\"\", (json.dumps(summary_json), idurl))\n",
    "        print(f'Updated summary for idurl: {idurl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadcrumb(conn, url_id):\n",
    "    \"\"\"\n",
    "    Generates the breadcrumb trail for a given idurl using a recursive query.\n",
    "\n",
    "    Args:\n",
    "        conn: The database connection object.\n",
    "        url_id: The idurl of the page for which to generate the trail.\n",
    "\n",
    "    Returns:\n",
    "        A string with the breadcrumb (e.g., \"Home > Products > NGIAB\"),\n",
    "        or None if an error occurs or the idurl is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    WITH RECURSIVE breadcrumb_path AS (\n",
    "        -- Anchor Member: Select the starting page\n",
    "        SELECT\n",
    "            idurl,\n",
    "            name,\n",
    "            idurlparent,\n",
    "            1 AS depth -- Initial depth level\n",
    "        FROM\n",
    "            tblurls\n",
    "        WHERE\n",
    "            idurl = %s\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Recursive Member: Join the table to find the parent\n",
    "        SELECT\n",
    "            u.idurl,\n",
    "            u.name,\n",
    "            u.idurlparent,\n",
    "            bp.depth + 1 -- Increment depth at each level\n",
    "        FROM\n",
    "            tblurls u\n",
    "        JOIN\n",
    "            breadcrumb_path bp ON u.idurl = bp.idurlparent\n",
    "    )\n",
    "    -- Select the final result, aggregating the names into a single string\n",
    "    SELECT\n",
    "        string_agg(name, ' > ' ORDER BY depth DESC) AS breadcrumb\n",
    "    FROM\n",
    "        breadcrumb_path;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = execute_query(conn, query, params=(url_id,), fetch=True)\n",
    "        \n",
    "        if result and result[0] and result[0][0]:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_breadcrumb for idurl {url_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "breadcrumb_trail = get_breadcrumb(conn, 73)\n",
    "print(breadcrumb_trail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, dimensions, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Call OpenAI to get an embedding for the given text.\"\"\"\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=model,\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the summary text\n",
    "urls_to_embed = execute_query(conn, \"\"\"\n",
    "    SELECT idurl, summary_data ->> 'summary_text' summary\n",
    "    FROM tblurls\n",
    "    WHERE summary_data IS NOT NULL\n",
    "    ORDER BY idurl\n",
    "\"\"\", fetch=True)\n",
    "\n",
    "for idurl, summary in urls_to_embed:\n",
    "    embeddings = get_embedding(summary, 1792)\n",
    "    if embeddings is not None:\n",
    "        # Insert the summary into the database\n",
    "        execute_query(conn, \"\"\"\n",
    "            UPDATE tblurls\n",
    "            SET embedding = %s\n",
    "            WHERE idurl = %s\n",
    "        \"\"\", (embeddings, idurl))\n",
    "        print(f'Updated embedding for idurl: {idurl}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d9bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIROH_AIBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
