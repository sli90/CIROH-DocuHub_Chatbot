[
  {
    "idurl": 9,
    "idtype": "text",
    "order": 1,
    "content": "# Contributing to CIROH DocuHub\n\nThank you for your interest in contributing to CIROH's DocuHub, our technical documentation site! Whether you're commenting on project information or submitting a pull request, we appreciate all kinds of contributions. This guide will help you understand the process of contributing code to the CIROH DocuHub.\n\nOur website is constructed using [Docusaurus](https://docusaurus.io/), a modern static website generator."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 2,
    "content": "## Contributing simple changes to DocuHub"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 3,
    "content": "### 1. Visit the documentation\nVisit docs.ciroh.org and navigate to the page you wish to modify."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 4,
    "content": "### 2. Edit page\nClick on \"Edit page\" at the bottom of the page to make any necessary changes."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 5,
    "content": "### 3. Submit a Pull Request\nSubmit a pull request with your changes to the repository."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 6,
    "content": "## Submitting a blog post to DocuHub"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 7,
    "content": "### 1. Write your blog post\nBlog posts submitted to CIROH DocuHub should discuss projects that make use of CIROH's cyberinfrastructure."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 8,
    "content": "### 2. Fill out the issue template\nClick the button below to access and fill out the blog post issue template with your project's information and the content of your blog post."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 9,
    "content": "### 3. Submit the request form\nSubmit the filled-out issue template for your blog posts. CIROH's tech team will review your PR for publishing shortly.\n\n[Blog Post Request Form](https://github.com/CIROH-UA/ciroh-ua_website/blob/main/.github/ISSUE_TEMPLATE/docuhub-blog-post.md)"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 10,
    "content": "## Requesting a product page on DocuHub"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 11,
    "content": "### 1. Upload your project to GitHub\nEnsure that your project is publicly visible on GitHub and includes an informative README.md file."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 12,
    "content": "### 2. Fill out the issue template\nClick the button below to access and fill out the product page issue template with your project's information."
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 13,
    "content": "### 3. Submit the request form\nSubmit the filled-out issue template for your product. CIROH's tech team will review your submission shortly.\n\n[Product Page Request Form](https://github.com/CIROH-UA/ciroh-ua_website/issues/new?assignees=&labels=on-prem&projects=&template=product-request.md)"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 14,
    "content": "## For developers\n\nIf you'd like to get more closely involved with DocuHub's development, please see the subpages below:\n\n- [Working with the DocuHub repository](https://docs.ciroh.org/docs/contribute/repository)\n- [Learn more about DocuHub technologies](https://docs.ciroh.org/docs/contribute/technologies)\n- [Adding blog posts to the DocuHub blog](https://docs.ciroh.org/docs/contribute/blog)"
  },
  {
    "idurl": 9,
    "idtype": "text",
    "order": 15,
    "content": "## Help and support\n\nIf you have any questions or issues contributing, please don't hesitate to reach out via Slack or email.\n\n[Contact us](https://docs.ciroh.org/contact)"
  },
  {
    "idurl": 65,
    "idtype": "text",
    "order": 1,
    "content": "# Installation\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/INSTALL.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/INSTALL.md).\n\nTo run ngen-datastream, clone this repository onto a linux machine and make sure the packages below are installed."
  },
  {
    "idurl": 65,
    "idtype": "text",
    "order": 2,
    "content": "## Required Packages\n\n- docker\n- git\n- pigz\n- tar\n- awscli (if using S3* options)\n- [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI)\n  - If using SUBSET options, make sure to move the hfsubset binary into /usr/bin"
  },
  {
    "idurl": 66,
    "idtype": "text",
    "order": 1,
    "content": "# Usage\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/USAGE.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/USAGE.md).\n\nThis document provides users with an introduction to running `ngen-datastream`."
  },
  {
    "idurl": 66,
    "idtype": "text",
    "order": 2,
    "content": "## Hardware\n\n`ngen-datastream` is a powerful tool designed to utilize compute resources to their maximum extent. While it is possible to run `ngen-datastream` locally, it is often the case that NextGen runs are large enough that running on a dedicated host will be desired. Why?\n\nFirstly, we have to acknowledge the memory required to hold a run of a certain size. For instance, forcingprocessor will create a single data cube of 4 byte elements which has dimensions ncatchments x time x nvar. Without going into the weeds, this example should show that, in general, the memory load of a `ngen-datastream` run will increase with both time and the number of catchments. This means that regardless of the host you're running `ngen-datastream` on, it is possible to request a run large enough (in time or space) to run out of memory.\n\nAnother factor to consider before executing is the maxmimum number of processes to allow. Unless you throttle `ngen-datastream` with the `-n` argument, the system will default to (nprocs_host - 2). For dedicated hosts, this is safe and you can even set `-n` to nprocs_host to maximize compute usage. However, if the user intends to deploy `ngen-datastream` on a non-dedicated host (one which has other programs running), they must be careful not to request too many processes.\n\nThe last factor to consider is the NextGen configuration via the realization file. The NextGen framework allows users to select bmi modules to run in their simulation. More complex modules will lead to longer run times and larger memory requirements.\n\nAll of these factors influence runtime. Larger complex runs with less resources will take longer to complete. To give the user an example of runtime, a complete datastream run for a day (24 time steps) over VPU 09 (11,000 catchments) should take about 5 minutes on an AWS t4g.2xlarge ec2 instance (8 processes on 8vCPU, 32GB, ARM) with CFE, PET, SLOTH, and NOM NextGen configuration."
  },
  {
    "idurl": 66,
    "idtype": "text",
    "order": 3,
    "content": "## Software\n\nIn general, `ngen-datastream` will create all of the required NextGen input files for you. However, the imense configurability of the NextGen framework allows `ngen-datastream` to be executed in a plethora of ways, each of which may have their own unique compute, memory, or bandwidth constraints. With this in mind, `ngen-datastream` allows the user to provide their own input files. To make this specific, if a user wishes to use a different forcing processing alogrithm, it is possible to provide `ngen-datastream` with your own forcing.tar.gz file. Same with ngen bmi module configs. `ngen-datastream` will create these files for you, or you can supply your own with ngen-bmi-configs.tar.gz. The hope is that `ngen-datastream` is \"batteries included\", while not being dogmatic about exactly how to perform the compute."
  },
  {
    "idurl": 66,
    "idtype": "text",
    "order": 4,
    "content": "## Other Considerations\n\nThese are not hard contraints, but rather should provide the user with a rough idea on how to run `ngen-datastream` safely given the configuration of the requested run and the hardware available. If a crash is experienced, increase memory resources or decrease either the size of the run or the number of processes.\n\nAssuming the user is running over a domain of 10,000 catchments\n\n1. For runs with simulation duration of days to weeks (months to year), each process should have access to around 1 (4) GB of RAM. For example, 8 processes would need access to 8 (32) GB RAM.\n2. The ratio of runtime minutes to simulation time steps is ~1:10. In other words, a simulation with 24 hourly time steps should take a few minutes to complete. This ratio will vary with hardware and run configuration."
  },
  {
    "idurl": 67,
    "idtype": "text",
    "order": 1,
    "content": "# NextGen Models\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/NGEN_MODELS.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/NGEN_MODELS.md).\n\nThis document describes which NextGen models you are able to run via NGIAB and `ngen-datastream`.\n\nNextGen is a framework in which physical models can be coupled in numerical simulations via the Basic Model Interface (BMI). NextGen in a Box (NGIAB) is a docker container which contains many pre-built models. `ngen-datastream` only has knowledge of models that are available in NGIAB. For example, models like the Conceptual Functional Equivalent (CFE) Model are available, but the particular model you are interested in running may not yet be integrated in NGIAB. Even if it is, `ngen-datastream` may not yet support the model. Start an issue to include your model if it is not in the lists below."
  },
  {
    "idurl": 67,
    "idtype": "text",
    "order": 2,
    "content": "## NextGen Models Available via `ngen-datastream`\n\n- [SLoTH](https://github.com/NOAA-OWP/SLoTH)\n- [CFE](https://github.com/NOAA-OWP/cfe)\n- [PET](https://github.com/NOAA-OWP/evapotranspiration)\n- [Noah-OWP-Modular](https://github.com/NOAA-OWP/noah-owp-modular)\n- [t-route](https://github.com/NOAA-OWP/t-route)"
  },
  {
    "idurl": 67,
    "idtype": "text",
    "order": 3,
    "content": "## NextGen Models Available in NGIAB and coming soon to `ngen-datastream`\n\n- [SoilFreezeThaw](https://github.com/NOAA-OWP/SoilFreezeThaw)\n- [SoilMoistureProfiles](https://github.com/NOAA-OWP/SoilMoistureProfiles)\n- [TOPMODEL](https://github.com/NOAA-OWP/topmodel)\n- [LSTM](https://github.com/NOAA-OWP/lstm)"
  },
  {
    "idurl": 67,
    "idtype": "text",
    "order": 4,
    "content": "## Coming Soon to NGIAB and `ngen-datastream`\n\n- [Sac-SMA](https://github.com/NOAA-OWP/sac-sma)\n- [Snow17](https://github.com/NOAA-OWP/snow17)\n- [LGAR](https://github.com/NOAA-OWP/LGAR-C)"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 1,
    "content": "# Standard Directories\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/STANDARD_DIRECTORIES.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/STANDARD_DIRECTORIES.md)."
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 2,
    "content": "## `ngen-datastream` Output Directory Structure\n\nWhen the datastream is executed a folder of the structure below will be constructed at `DATA_DIR`\n\n```\nDATA-PATH/\n│\n├── datastream-metadata/\n│\n├── datastream-resources/\n|\n├── ngen-run/\n```\n\nEach folder is explained below"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 3,
    "content": "### `datastream-metadata/`\n\nHolds metadata about the `ngen-datastream` excution that allows for a relatively condensed view of how the execution was performed.\nExample directory:\n\n```\ndatastream-metadata/\n│\n├── datastream.env\n│\n├── datastream_steps.txt\n│\n├── conf_datastream.json\n│\n├── conf_fp.json\n|\n├── conf_nwmurl.json\n|\n├── profile_fp.txt\n|\n├── profile.txt\n|\n├── filenamelist.txt\n|\n├── realization_datastream.json\n|\n├── realization_user.json\n```"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 4,
    "content": "| File Type | Path in Metadata Directory | Description |\n| --- | --- | --- |\n| DATASTREAM CONFIGURATION FILE | datastream-metadata/datastream.env | `CONF_FILE` that can be provided directly to datastreamcli as an argument to reproduce the exact run. |\n| DATASTREAM COMMAND STEPS | datastream-metadata/datastream_steps.txt | Text file that logs each command datastream executes. |\n| DATASTREAM CONFIGURATION | datastream-metadata/conf_datastream.json | Holds metadata about the execution |\n| FORCING PROCESSOR CONFIGURATION | datastream-metadata/conf_fp.json | Configuration file for forcingprocessor. See [here](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor#example-confjson) |\n| NWM URL CONFIGURATION | datastream-metadata/conf_nwmurl.json | Configuration file for nwmurl. See [here](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor#nwm_file) |\n| PROFILE | datastream-metadata/profile_fp.txt | Datetime print statements that allow for profiling each step in forcingprocessor |\n| PROFILE | datastream-metadata/profile.txt | Datetime print statements that allow for profiling each step in datastream |\n| FILENAME LIST | datastream-metadata/filenamelist.txt | Local file paths or URLs to NWM forcings. Generated by [nwmurl](https://github.com/CIROH-UA/nwmurl). |\n| REALIZATION | datastream-metadata/realization_datastream.json | NextGen configuration file. See [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/configs/ngen/realization_cfe_sloth_pet_nom.json). DataStreamCLI can edit the user supplied realization. This is the realization file that is used for the NextGen execution. |\n| REALIZATION | datastream-metadata/realization.json | NextGen configuration file. See [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/configs/ngen/realization_cfe_sloth_pet_nom.json). DataStreamCLI can edit the user supplied realization. This is the exact copy of the user supplied realization file and may differ from the file used for the NextGen execution. |"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 5,
    "content": "### `RESOURCE_DIR` ( `datastream-resources/`)\n\n`datastream-resources/` holds all the input data files required to perform the various computations `ngen-datastream` performs. This folder is not required as input, but will be a faster method for running ngen-datastream repeatedly over a given spatial or time domain.\n\nExamples of the application of the resource directory:\n\n1. Repeated executions. `ngen-datastream` will retrieve files (that are given as arguements) remotely, however this can take time depending on the networking between the data source and host. Storing these files locally in `RESOURCE_DIR` for repeated runs will save time and network bandwith. In addition, this saves on compute required to build input files from scratch.\n2. Communicating runs. ngen-datastream versions everything in `DATA_DIR`, which means a single hash corresponds to a unique `RESOURCE_DIR`, which allows users to quickly identify potential differences between `ngen-datastream` input data."
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 6,
    "content": "#### Guide for building a `RESOURCE_DIR`\n\nThe easiest way to create a reusable resource directory is to execute `ngen-datastream` and save `DATA_DIR/datastream-resources` for later use. A user defined `RESOURCE_DIR` may take the form below. Only one file of each type is allowed (e.g. cannot have two geopackages or two realizations). Not every file is required. `ngen-datastream` will generate all required files by default, but will skip those steps if corresponding files exist in the resource directory.\n\n```\nRESOURCE_DIR/\n|\n├── config/\n|   │"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 7,
    "content": "|   ├── nextgen_09.gpkg\n|   |\n|   ├── realization.json\n|   |\n|   ├── troute.yaml\n|   |\n|   ├── partitions.json\n|   |\n|   ├── cat-config/\n|   │   |\n|   |   ├──PET/\n|   │   |\n|   |   ├──CFE/\n|   │   |\n|   |   ├──NOAH-OWP-M/\n|"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 8,
    "content": "├── nwm-forcings/\n|   |"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 9,
    "content": "|   ├── nwm.t00z.medium_range.forcing.f001.conus\n|   |\n|   ├── ...\n|"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 10,
    "content": "├── ngen-forcings/\n|   |"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 11,
    "content": "|   ├── forcings.nc\n|"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 12,
    "content": "```"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 13,
    "content": "| File Type | Path in Resource Directory | Example Link | Description | Naming |\n| --- | --- | --- | --- | --- |\n| BMI CONFIGURATION | config/cat-config |  | directory holding BMI module configuration files defined in realization file. | See here |\n| REALIZATION | config/realization.json | [link](https://github.com/CIROH-UA/ngen-datastream/blob/main/configs/ngen/realization_cfe_sloth_pet_nom.json) | NextGen configuration | *realization*.json |\n| GEOPACKAGE | config/nextgen_01.gpkg | [link](https://lynker-spatial.s3.amazonaws.com/v20.1/gpkg/nextgen_01.gpkg) | Hydrofabric file of version ≥ v20.1 Ignored if subset hydrofabric options are set in datastream config. See [Lynker-Spatial](https://www.lynker-spatial.com/#v20.1/gpkg/) for complete VPU geopackages or [hfsubset](https://github.com/LynkerIntel/hfsubset) for generating your own custom domain. `hfsubset` can be invoked indirectly through `ngen-datastream` through the subsetting args. | *.gpkg |\n| PARTITIONS | config/patitions_$NPROCS.json |  | File generated by the NextGen framework to distribute processing by spatial domain. | *partitions*.json |\n| FORCINGS | nwm-forcings/*.nc | [link](https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/netcdf/FORCING/2019/201901010000.LDASIN_DOMAIN1) | NetCDF National Water Model forcing files. These are not saved to the resource directory by default. | *.nc |\n| FORCINGS | ngen-forcings/*.nc |  | netcdf holding ngen forcings. | *.nc (by default), *.tar.gz, *.csv, *.parquet |"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 14,
    "content": "### `ngen-run/`\n\nRunning NextGen requires building a standard run directory complete with only the necessary files. The datastream constructs this automatically, but can be manually built as well. Below is an explanation of the standard. Reference for discussion of the standard [here](https://github.com/CIROH-UA/NGIAB-CloudInfra/pull/17).\n\nA NextGen run directory `ngen-run` is composed of three necessary subfolders `config, forcings, outputs` and an optional fourth subfolder `metadata`.\n\n```\nngen-run/\n│\n├── config/\n│\n├── forcings/\n│\n├── lakeout/\n|\n├── metadata/\n│\n├── outputs/\n│\n├── restart/\n```\n\nThe `ngen-run` directory contains the following subfolders:\n\n- `config`: model configuration files and hydrofabric configuration files. A deeper explanation here\n- `forcings`: catchment-level forcing timeseries files. These can be generated with the [forcingprocessor](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor). Forcing files contain variables like wind speed, temperature, precipitation, and solar radiation.\n- `lakeout`: for t-route\n- `metadata` is an optional subfolder. This is programmatically generated and it used within to ngen. Do not edit this folder.\n- `outputs`: This is where ngen will place the output files.\n- `restart`: For restart files"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 15,
    "content": "#### Configuration directory `ngen-run/config/`\n\nThis folder contains the NextGen realization file, which serves as the primary model configuration for the ngen framework. This file specifies which models to run and with which parameters, run parameters like date and time, and hydrofabric specifications.\n\nBased on the models defined in the realization file, BMI configuration files may be required. For those models that require per-catchment configuration files, a folder will hold these files for each model in `ngen-run/config/cat-config`. See [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/NGEN_MODULES.md) for which models ngen-datastream supports automated BMI configuration file generation. See the directory structure convention below.\n\n```\nngen-run/\n|\n├── config/\n|   │"
  },
  {
    "idurl": 68,
    "idtype": "table",
    "order": 16,
    "content": "|   ├── nextgen_09.gpkg\n|   |\n|   ├── realization.json\n|   |\n|   ├── troute.yaml\n|   |\n|   ├── cat-config/\n|   │   |\n|   |   ├──PET/\n|   │   |\n|   |   ├──CFE/\n|   │   |\n|   |   ├──NOAH-OWP-M/"
  },
  {
    "idurl": 68,
    "idtype": "text",
    "order": 17,
    "content": "...\n```"
  },
  {
    "idurl": 69,
    "idtype": "text",
    "order": 1,
    "content": "# DataStream Options\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/DATASTREAM_OPTIONS.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/DATASTREAM_OPTIONS.md).\n\nBelow is the output of `./scripts/datastream -h`. A more in depth of each option is given in the table below.\n\n```\nUsage: ./scripts/datastream [options]\nEither provide a datastream configuration file\n  -c, --CONF_FILE           <Path to datastream configuration file>\nor run with cli args\n  -s, --START_DATE          <YYYYMMDDHHMM or \"DAILY\">\n  -e, --END_DATE            <YYYYMMDDHHMM>\n  -C, --FORCING_SOURCE      <Forcing source option>\n  -d, --DATA_DIR            <Path to write to>\n  -R, --REALIZATION         <Path to realization file>\n  -g, --GEOPACKAGE          <Path to geopackage file>\n  -I, --SUBSET_ID           <Hydrofabric id to subset>\n  -i, --SUBSET_ID_TYPE      <Hydrofabric id type>\n  -v, --HYDROFABRIC_VERSION <Hydrofabric version>\n  -D, --DOMAIN_NAME         <Name for spatial domain>\n  -r, --RESOURCE_DIR        <Path to resource directory>\n  -f, --NWM_FORCINGS_DIR    <Path to nwm forcings directory>\n  -N, --NGEN_BMI_CONFS      <Path to ngen BMI config directory>\n  -F, --NGEN_FORCINGS       <Path to ngen forcings directory, tarball, or netcdf>\n  -S, --S3_BUCKET           <s3 bucket to write output to>\n  -o, --S3_PREFIX           <File prefix within s3 bucket>\n  -n, --NPROCS              <Process limit>\n  -y, --DRYRUN              <True to skip calculations>\n  -E, --EVAL                <True to run TEEHR evaluation service>\n```"
  },
  {
    "idurl": 69,
    "idtype": "text",
    "order": 2,
    "content": "### Explanation of cli args (or variables in defined in `CONF_FILE`)"
  },
  {
    "idurl": 69,
    "idtype": "table",
    "order": 3,
    "content": "| Field | Flag | Description | Required |\n| --- | --- | --- | --- |\n| START_DATE | `-s` | Start simulation time (YYYYMMDDHHMM) or \"DAILY\" | ✅ |\n| END_DATE | `-e` | End simulation time (YYYYMMDDHHMM) | ✅ |\n| FORCING_SOURCE | `-C` | Select the forcings data provider. Format is `NWM_$VERSION_$RUN_TYPE_$INIT_CYCLE_$MEMBER` Options include NWM_RETRO_V2, NWM_RETRO_V3, NWM_V3_SHORT_RANGE_00, NWM_V3_MEDIUM_RANGE_00_0, NWM_V3_ANALYSIS_ASSIM_16, NWM_V3_ANALYSIS_ASSIM_EXTEND, NOMADS, NOMADS_POSTPROCESSED | ✅ |\n| DATA_DIR | `-d` | Absolute local path to construct the datastream run. | ✅ |\n| REALIZATION | `-R` | Path to NextGen realization file | Required here or file exists in `RESOURCE_DIR/config` |\n| GEOPACKAGE | `-g` | Path to hydrofabric, can be s3URI, URL, or local file. Generate file with [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) or use SUBSET args. | Required here or file exists in `RESOURCE_DIR/config` |\n| SUBSET_ID_TYPE | `-i` | id type corresponding to \"id\" [See hfsubset for options](https://github.com/LynkerIntel/hfsubset?tab=readme-ov-file#cli-option) | Required here if user is not providing GEOPACKAGE and GEOPACKAGE_ATTR. |\n| SUBSET_ID | `-I` | catchment id to subset [See hfsubset for options](https://github.com/LynkerIntel/hfsubset?tab=readme-ov-file#cli-option) | Required here if user is not providing GEOPACKAGE and GEOPACKAGE_ATTR. |\n| HYDROFABRIC_VERSION | `-v` | ≥ v20.1 [See hfsubset for options](https://github.com/LynkerIntel/hfsubset?tab=readme-ov-file#cli-option) | Required here if user is not providing GEOPACKAGE and GEOPACKAGE_ATTR. |\n| DOMAIN_NAME | `-D` | Name for spatial domain in run, stripped from gpkg if not supplied |  |\n| RESOURCE_DIR | `-r` | Path to directory that contains the datastream resources. More explanation here. |  |\n| NWM_FORCINGS_DIR | `-f` | Path to local directory containing nwm files. Alternatively, these file could be stored in RESOURCE_DIR as nwm-forcings. |  |\n| NGEN_BMI_CONFS | `-N` | Path to local directory containing NextGen BMI configuration files. Alternatively, these files could be stored in RESOURCE_DIR under `config/`. See here for directory structure. |  |\n| NGEN_FORCINGS | `-F` | Path to local ngen forcings directory holding ngen forcing csv's or parquet's. Also accepts tarball or netcdf. Alternatively, this file(s) could be stored in RESOURCE_DIR at `ngen-forcings/`. |  |\n| S3_BUCKET | `-S` | AWS S3 Bucket to write output to |  |\n| S3_PREFIX | `-o` | Path within S3 bucket to write to |  |\n| DRYRUN | `-y` | Set to \"True\" to skip all compute steps. |  |\n| NPROCS | `-n` | Maximum number of processes to use in any step of `ngen-datastream`. Defaults to `nprocs - 2` |  |\n| CONF_FILE | `-c` | Store CLI args as env variables in a file. |  |\n| EVAL | `-E` | Set to \"True\" to run the TEEHR automated evaluation service on NextGen outputs. |  |"
  },
  {
    "idurl": 69,
    "idtype": "text",
    "order": 4,
    "content": "### Note for supplying file(s) paths\n\nDataStreamCLI is designed to pick up files or folder whether they are local, public, or in AWS s3 object storage.\n\nDataStreamCLI can accept local or non-local paths. The following are all allowed.\n\n```\nFor file like paths (i.e. providing explicit geopackage path)\n1. A local absolute (or relative) path : /home/user/data/nextgen_VPU_09.gpkg\n2. A URL : https://ciroh-community-ngen-datastream.s3.us-east-1.amazonaws.com/v2.2_resources/VPU_09/config/nextgen_VPU_09.gpkg\n3. A s3 URI: s3://ciroh-community-ngen-datastream/v2.2_resources/VPU_09/config/nextgen_VPU_09.gpkg\n\nFor folder like paths (i.e. providing a resource directory)\n1. A local absolute (or relative) folder : /home/user/data/v2.2_resources/VPU_09\n2. A s3 URI prefix : s3://ciroh-community-ngen-datastream/v2.2_resources/VPU_09\n```\n\nNote here that DataStreamCLI will treat a s3 URI prefix like a directory."
  },
  {
    "idurl": 69,
    "idtype": "text",
    "order": 5,
    "content": "### Environment variables\n\nDataStreamCLI can find run options in the form of environment variables. These can be set in the `CONF_FILE`. An example file here from the NextGen Research DataStream.\n\nIn addition to the run options documented in the table above, the following environment variables can also be set."
  },
  {
    "idurl": 69,
    "idtype": "table",
    "order": 6,
    "content": "| Environment Variable | Description |\n| --- | --- |\n| NGIAB_TAG | Sets the NextGen In A Box Docker container tag |\n| DS_TAG | Sets the datastream Docker container tag |\n| FP_TAG | Sets the forcing processor Docker container tag |\n| SKIP_VALIDATION | Set to \"True\" to bypass DataStreamCLI's built-in input directory validation. |"
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 1,
    "content": "# Breakdown\n\n> **WARNING**\n>\n> This document is primarily intended to explain what steps DataStreamCLI performs internally.\n> Performing these steps manually is discouraged, as a typical call to `scripts/datastream` will handle all of this for you.\n> If you're looking for a guide for how to run the script, run `scripts/datastream-guide` after [installation](https://docs.ciroh.org/docs/products/research-datastream/cli/install) for an interactive tutorial.\n\n> **NOTE**\n>\n> Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/BREAKDOWN.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/BREAKDOWN.md)."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 2,
    "content": "# `ngen-datastream` Breakdown\n\nThis document serves as a guide to run `ngen-datastream` step-by-step, which essentially walks the user through an example execution of `ngen-datastream/scripts/ngen-datastream`"
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 3,
    "content": "## Use Case\n\nImagine we want to study the variation in NextGen configurations of retrospective streamflow forecasts for the Colorado River for the day of June 10th, 2019. We will use the steps below to prepare an input data package for NextGen, execute NextGen through NextGen in a Box, and then plot the results."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 4,
    "content": "## Steps\n\n- Prepare Directories\n- Spatial Domain\n- Forcings / Time Domain\n- NextGen Configuration\n- NextGen BMI Configuration File Generation\n- Validation\n- NextGen Execution\n- Equivalent Command\n\nBefore proceeding, please review the [usage](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/USAGE.md) document and the [standard directions](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/STANDARD_DIRECTORIES.md) with special attention to the [ngen-run](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/STANDARD_DIRECTORIES.md#ngen-run) standard run directory portion."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 5,
    "content": "## Prepare Directories\n\nAs you've now read, NextGen in a Box requires a standard directory structure. In this step we prepare that directory and will write data to it in the following steps.\n\nThis guide will assume the user's current working directory is the `ngen-datastream` root folder.\n\nTo build `ngen-run/`\n\n```bash\nmkdir -p ./palisade_2019/ngen-run\nmkdir ./palisade_2019/ngen-run/config\nmkdir ./palisade_2019/ngen-run/forcings\nmkdir ./palisade_2019/ngen-run/lakeout\nmkdir ./palisade_2019/ngen-run/outputs\nmkdir ./palisade_2019/ngen-run/restart\n```\n\nTo hold intermediate files not directly required in NextGen in a Box execution\n\n```bash\nmkdir ./palisade_2019/datastream-metadata\n```"
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 6,
    "content": "## Spatial Domain\n\nThe spatial domain is defined via a geopackage file which is \"cut\" from the hydrofabric. Lynker-Spatial serves this spatial data via [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI). Please refer to hfsubset for the latest documenation as this document may not represent the current version. An example command is provided below:\n\n```bash\nhfsubset \\\n    -w medium_range \\\n    -s nextgen \\\n    -v 2.1.1 \\\n    -l divides,flowlines,network,nexus,forcing-weights,flowpath-attributes,divide-attributes \\\n    -o ./palisade_2019/ngen-run/config/palisade.gpkg \\\n    -t hl \"Gages-09106150\"\n```\n\nSome things to note about this command:\n\n- `-l` : These layers are required: `divides`, `flowlines`, `network`, `nexus`, `forcing-weights`, `flowpath-attributes`, and `divide-attributes` (Use `model-attributes` instead of `divide attributes` for <v2.2 hydrofabric)\n- `-w` : Defines which grid the weights are calculated from. In this case, it's the medium_range forcing from the National Water Model\n- `-v` : Defines the hydrofabric version\n- The last arguement `\"Gages-09106150\"`, corresponds to the stream gauge that hfsubset will use to trace the upstream network. This effectively defines the spatial domain. This is a gauge on the Colorado River in Palisade, Colorado.\n\nSee [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) for more options.\n\nUse the [Lynker Spatial Hydrolocation Viewer](https://www.lynker-spatial.com/hydrolocations.html) to find the point of interest ( `poi`) you want to use to subset. See hfsubset documentation to see how to subset with Gage Id and other options.\n\nUse the [NGIA geopackage viewer](https://ngageoint.github.io/geopackage-viewer-js/) to make sure you've got the spatial domain set properly.\n\n[hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) is integrated into `ngen-datastream`, which means the user can simply provide these three cli args to `ngen-datastream` to offload the [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) call to `ngen-datastream`\n\n```bash\n  -I, --SUBSET_ID           <Hydrofabric id to subset>\n  -i, --SUBSET_ID_TYPE      <Hydrofabric id type>\n  -v, --HYDROFABRIC_VERSION <Hydrofabric version>\n```"
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 7,
    "content": "## Forcings / Time Domain\n\nDefining the time over which the simulation will run is essentially captured in the forcings (precipitation, wind speed, etc.). There are many sources of forcings that can be used in NextGen. While it is certainly possible to use forcings from a variety of sources (NWM, AORC, etc.) `ngen-datastream` integrates [nwmurl](https://github.com/CIROH-UA/nwmurl), which will provide National Water Model forcings filenames based on the time period the user provides. To generate these forcings independently, follow the steps below:\n\nFirst, we use [nwmurl](https://github.com/CIROH-UA/nwmurl) to generate a list of nwm forcing filenames that [forcingprocessor](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor) reads from. We define exactly which forcings files we want with the options in conf_nwmurl.json.\n\nThere are two types of this conf_nwmurl.json. One for operational forcings which looks like this:\n\n```json\n{\n    \"forcing_type\" : \"operational_archive\",\n    \"start_date\"   : \"\",\n    \"end_date\"     : \"\",\n    \"fcst_cycle\"   : [0],\n    \"lead_time\"    : [],\n    \"varinput\"     : 5,\n    \"geoinput\"     : 1,\n    \"runinput\"     : 2,\n    \"urlbaseinput\" : 7,\n    \"meminput\"     : 0\n}\n```\n\nOne for retrospective forcings which looks like this:\n\n```json\n{\n    \"forcing_type\" : \"retrospective\",\n    \"start_date\"   : \"\",\n    \"end_date\"     : \"\",\n    \"urlbaseinput\" : 4,\n    \"selected_object_type\" : [1],\n    \"selected_var_types\"   : [6],\n    \"write_to_file\" : true\n}\n```\n\nSee [nwmurl](https://github.com/CIROH-UA/nwmurl) for an in-depth explaination of the options.\n\nNow let's generate the 2019 retrospective forcings. Let's make our own conf_nwmurl.json:\n\n```json\n{\n    \"forcing_type\" : \"retrospective\",\n    \"start_date\"   : \"201906100000\",\n    \"end_date\"     : \"201906102300\",\n    \"urlbaseinput\" : 4,\n    \"selected_object_type\" : [1],\n    \"selected_var_types\"   : [6],\n    \"write_to_file\" : true\n}\n```\n\nSave this file to `./palisade_2019/datastream-metadata/conf_nwmurl_retro.json`\n\nHere's an example command to generate the list of forcing filenames with this configuration:\n\n```bash\npython ./forcingprocessor/src/forcingprocessor/nwm_filenames_generator.py ./palisade_2019/datastream-metadata/conf_nwmurl_retro.json\n```\n\nThis should write out a file named `retro_filenamelist.txt`. List the first and last few lines to make sure we have the forcings we want.\n\n```bash\nhead -n 20 ./retro_filenamelists.txt\n```\n\n```bash\ntail -n 20 ./retro_filenamelist.txt\n```\n\nSave this file as `./palisade_2019/datastream-metadata/retro_filenamelist.txt`\n\nNow that we have the forcing filenames in a text file, we can set up `forcingprocessor` by creating our own `conf_fp.json`. See [here](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor#confjson-options) for a deeper explanation of the options.\n\n```json\n{\n    \"forcing\"  : {\n        \"nwm_file\"   : \"./palisade_2019/datastream-metadata/retro_filenamelist.txt\",\n        \"gpkg_file\"  : [\"./palisade_2019/ngen-run/config/palisade.gpkg\"]\n    },\n\n    \"storage\":{\n        \"storage_type\"      : \"local\",\n        \"output_path\"       : \"./palisade_2019/ngen-run/forcings\",\n        \"output_file_type\" : [\"csv\"]\n    },\n\n    \"run\" : {\n        \"verbose\"       : true,\n        \"collect_stats\" : true,\n        \"nprocs\"        : 8\n    },\n\n    \"plot\":{\n        \"ngen_vars\"  : [\\\n            \"TMP_2maboveground\"\\\n        ]\n    }\n}\n```\n\nSave this file as `./palisade_2019/datastream-metadata/conf_fp.json`. Note the `plot` field which will generate gifs of the nwm -> ngen forcings as shown below.\n\nExecute `forcingprocessor` with the following command\n\n```bash\npython ./forcingprocessor/src/forcingprocessor/processor.py ./palisade_2019/datastream-metadata/conf_fp.json\n```\n\nThis should write out NextGen forcings csv files to `./palisade_2019/ngen-run/forcings`"
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 8,
    "content": "## NextGen Configuration\n\nNextGen configuration is determined by the realization file. See the [NextGen repo](https://github.com/NOAA-OWP/ngen/wiki/Realization-Config) for documentation on building this file. Create and save it to `./palisade_2019/ngen-run/config/realization.json`. There are template realizations located in `ngen-datastream` at `./configs/ngen`."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 9,
    "content": "## NextGen BMI Configuration File Generation\n\nEach model defined in the realization file will require the creation of configuration files, often for each catchment defined in the geopackage. `ngen_configs_gen.py` will generate all of the required BMI configuration files based on which models are found in the realization file. See [this document](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/NGEN_MODULES.md) for which models are currently integrated into `ngen-datastream`. If you define a model in the realization file that is not integrated, you will need to manually create the necessary BMI configuration files.\n\nIf Noah-OWP-Modular is defined in the realization file, compute the noah-owp pickle file. This is admittedly hacky and will be depreciated soon.\n\n```bash\npython ./python/src/datastream/noahowp_pkl.py \\\n    --hf_file ./palisade_2019/ngen-run/config/palisade.gpkg \\\n    --outdir ./palisade_2019/datastream-metadata\n```\n\nTo generate the NextGen BMI configuration models:\n\n```bash\npython ./python/src/datastream/ngen_configs_gen.py \\\n    --hf_file ./palisade_2019/ngen-run/config/palisade.gpkg \\\n    --outdir ./palisade_2019/ngen-run/config \\\n    --pkl_file ./palisade_2019/datastream-metadata/noah-owp-modular-init.namelist.input.pkl \\\n    --realization ./palisade_2019/ngen-run/config/realization.json\n    --ignore \"\"\n```\n\nInspect `./palisade_2019/ngen-run/config` to see what BMI model configuration files were created."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 10,
    "content": "## Validation\n\nWe have now completed the required steps to generate the input data package for a NextGen simulation. As you can tell by now, there are many steps and thus many places to make a mistake. The `run-validator.py` script provides a quick way to check that the input data package contains the required files and that they are consistent to one another. See [here](https://github.com/CIROH-UA/ngen-datastream/tree/main/python_tools#run_validatorpy) for an explanation of the validation.\n\nExecute the validator on the `ngen-run` package we have created.\n\n```bash\npython ./python/src/datastream/run_validator.py --data_dir ./palisade_2019/ngen-run\n```\n\nIf the run package is valid, it will complete without error. If you find the error messaging not clearly communicating the fault with the input package, start an issue in the repo to have the messaging improved.\n\nProceed only when the input package passes validation."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 11,
    "content": "## NextGen Execution\n\nLet's do this thing! Feed the input data package to NextGen in a Box with auto mode.\n\n```bash\ndocker run --rm -v $(pwd)/palisade_2019/ngen-run:/ngen-run awiciroh/ciroh-ngen-image:latest /ngen-run auto 8\n```\n\nIn this command we mount the `./palisade_2019/ngen-run` directory to the docker container and execute on `/ngen-run` with 8 processes. When the execution completes, output data should be located in `/palisade_2019/ngen-run/outputs/`."
  },
  {
    "idurl": 70,
    "idtype": "text",
    "order": 12,
    "content": "## Equivalent Command\n\nNow that we have performed each component of the datastream individually, let's take a look at the single command that would have accomplished all of the steps we just went through.\n\n```bash\n./scripts/ngen-datastream \\\n    -s 201901010000 \\\n    -e 201912312300 \\\n    -i hl \\\n    -I Gages-09106150 \\\n    -v 2.1.1 \\\n    -R $(pwd)/configs/ngen/realization_sloth_nom_cfe_pet_troute.json \\\n    -d $(pwd)/palisade_2019\n```"
  },
  {
    "idurl": 71,
    "idtype": "text",
    "order": 1,
    "content": "# Python Tools\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/python_tools/README.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/python_tools/README.md).\n\nScripts to create ngen bmi module configuration files and validate ngen-run packages."
  },
  {
    "idurl": 71,
    "idtype": "text",
    "order": 2,
    "content": "## `ngen_configs_gen.py`\n\nNote: see noahowp_pkl before running this script. Currently, the datastream creates the noahowp config files from a template config within the repository.\nThe t-route config is also created from a template within this repository. Soon these will be generated with pydantic models within [ngen-cal](https://github.com/NOAA-OWP/ngen-cal)\n\n```\nusage: ngen_configs_gen.py [-h] [--hf_file HF_FILE] [--outdir OUTDIR]\n                           [--pkl_file PKL_FILE] [--realization REALIZATION]\noptions:\n  -h, --help                 show this help message and exit\n  --hf_file HF_FILE          Path to the .gpkg\n  --outdir OUTDIR            Path to write ngen configs\n  --pkl_file PKL_FILE        Path to the noahowp pkl\n  --realization REALIZATION  Path to the ngen realization\n\n```"
  },
  {
    "idurl": 71,
    "idtype": "text",
    "order": 3,
    "content": "## `noahowp_pkl.py`\n\nGenerate .pkl file read by `ngen_configs_gen.py`, from which noahowp configs will be generated for each catchment present in the attributes file.\n\n```\nusage: noahowp_pkl.py [-h] [--hf_lnk_file HF_LNK_FILE] [--outdir OUTDIR]\noptions:\n  -h, --help                show this help message and exit\n  --hf_lnk_file HF_LNK_FILE Path to the .gpkg attributes\n  --outdir OUTDIR           Path to write ngen configs\n\n```"
  },
  {
    "idurl": 71,
    "idtype": "text",
    "order": 4,
    "content": "## `configure-datastream.py`\n\nGenerates the three configuration files for the datastream. One each for the datastream itself, forcingprocessor, and nwmurl.\n\n```\nusage: configure-datastream.py [-h] [--docker_mount DOCKER_MOUNT] [--start_date START_DATE] [--end_date END_DATE] [--data_path DATA_PATH] [--gpkg GPKG] [--gpkg_attr GPKG_ATTR] [--resource_path RESOURCE_PATH]\n                               [--subset_id_type SUBSET_ID_TYPE] [--subset_id SUBSET_ID] [--hydrofabric_version HYDROFABRIC_VERSION] [--nwmurl_file NWMURL_FILE] [--nprocs NPROCS] [--host_type HOST_TYPE]\n                               [--domain_name DOMAIN_NAME]\noptions:\n  -h, --help                                  show this help message and exit\n  --docker_mount DOCKER_MOUNT                 Path to DATA_PATH mount within docker container\n  --start_date START_DATE                     Set the start date\n  --end_date END_DATE                         Set the end date\n  --data_path DATA_PATH                       Set the data directory\n  --gpkg GPKG                                 Path to geopackage file\n  --resource_path RESOURCE_PATH               Set the resource directory\n  --subset_id_type SUBSET_ID_TYPE             Set the subset ID type\n  --subset_id SUBSET_ID                       Set the subset ID\n  --hydrofabric_version HYDROFABRIC_VERSION   Set the Hydrofabric version\n  --nwmurl_file NWMURL_FILE                   Provide an optional nwmurl file\n  --nprocs NPROCS                             Maximum number of processes to use\n  --host_type HOST_TYPE                       Type of host\n  --domain_name DOMAIN_NAME                   Name of spatial domain\n\n```"
  },
  {
    "idurl": 71,
    "idtype": "text",
    "order": 5,
    "content": "## `run_validator.py`\n\nValidates a ngen-run package. More specifically the following criteria must be met:\n\n1. Standard ngen-run directory formatting. See [here](https://github.com/CIROH-UA/ngen-datastream/blob/main/README.md#ngen-run) for more on the ngen-run folder.\n2. A single geopackage exists\n3. A single realization file exists\n4. A forcing file is found at the path supplied in the realization for each catchment found in the geopackage.\n5. A configuration file is found at the bmi module config path supplied in the realization for each catchment found in the geopackage.\n\n```\nusage: run_validator.py [-h] [--data_dir DATA_DIR] [--tarball TARBALL]\n\noptions:\n  -h, --help           show this help message and exit\n  --data_dir DATA_DIR  Path to the ngen input data folder\n  --tarball TARBALL    Path to tarball to be validated as ngen input data folder\n\n```"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 1,
    "content": "# Forcing Processor\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/README.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/README.md).\n\nForcingprocessor converts National Water Model (NWM) forcing data into Next Generation National Water Model (NextGen) forcing data. The motivation for this tool is NWM data is gridded and stored within netCDFs for each forecast hour. Ngen inputs this same forcing data, but in the format of per-catchment csv files that hold time series data. Forcingprocessor is driven by a configuration file that is explained, with an example, in detail below. The config argument accepts an s3 URL.\n\n[![forcing_gif](https://github.com/CIROH-UA/ngen-datastream/raw/main/docs/gifs/T2D_2_TMP_2maboveground_cali.gif)](https://github.com/CIROH-UA/ngen-datastream/blob/main/docs/gifs/T2D_2_TMP_2maboveground_cali.gif)"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 2,
    "content": "## Install\n\n```\ncd /ngen-datastream/forcingprocessor/ && pip install -e .\n\n```"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 3,
    "content": "## Run the forcingprocessor\n\n```\npython ./src/forcingprocessor/processor.py ./configs/conf.json\n\n```\n\nPrior to executing the processor, the user will need to obtain a geopackage file to define the spatial domain. [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) will provide a geopackage which contains a necessary layer, `forcing-weights`, for `processor.py`. The user will define the time domain by generating the forcing filenames for `processor.py` via `nwm_filenames_generator.py`, which is explained here."
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 4,
    "content": "## Example `conf.json`\n\n```\n{\n    \"forcing\"  : {\n        \"nwm_file\"     : \"\",\n        \"gpkg_file\"    : \"\"\n    },\n\n    \"storage\":{\n        \"output_path\"      : \"\",\n        \"output_file_type\" : []\n    },\n\n    \"run\" : {\n        \"verbose\"       : true,\n        \"collect_stats\" : true,\n        \"nprocs\"        : 2\n    },\n\n    \"plot\":{\n        \"nts\"        : 24,\n        \"ngen_vars\"  : [\\\n            \"DLWRF_surface\",\\\n            \"APCP_surface\",\\\n            \"precip_rate\",\\\n            \"TMP_2maboveground\"\\\n        ]\n    }\n}\n\n```"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 5,
    "content": "## `conf.json` Options"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 6,
    "content": "### 1. Forcing"
  },
  {
    "idurl": 72,
    "idtype": "table",
    "order": 7,
    "content": "| Field | Description | Required |\n| --- | --- | --- |\n| nwm_file | Path to a text file containing nwm file names. One filename per line. Tool to create this file | ✅ |\n| gpkg_file | Geopackage file to define spatial domain. Use [hfsubset](https://github.com/lynker-spatial/hfsubsetCLI) to generate a geopackage with a `forcing-weights` layer. Accepts local absolute path, s3 URI or URL. Also acceptable is a weights parquet generated with [weights_hf2ds.py](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/src/forcingprocessor/weights_hf2ds.py), though the plotting option will no longer be available. | ✅ |"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 8,
    "content": "### 2. Storage"
  },
  {
    "idurl": 72,
    "idtype": "table",
    "order": 9,
    "content": "| Field | Description | Required |\n| --- | --- | --- |\n| storage_type | Type of storage (local or s3) | ✅ |\n| output_path | Path to write data to. Accepts local path or s3 | ✅ |\n| output_file_type | List of output file types, e.g. [\"tar\",\"parquet\",\"csv\",\"netcdf\"] | ✅ |"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 10,
    "content": "### 3. Run"
  },
  {
    "idurl": 72,
    "idtype": "table",
    "order": 11,
    "content": "| Field | Description | Required |\n| --- | --- | --- |\n| verbose | Get print statements, defaults to false | ✅ |\n| collect_stats | Collect forcing metadata, defaults to true | ✅ |\n| nprocs | Number of data processing processes, defaults to 50% available cores |  |"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 12,
    "content": "### 4. Plot\n\nUse this field to create a side-by-side gif of the nwm and ngen forcings"
  },
  {
    "idurl": 72,
    "idtype": "table",
    "order": 13,
    "content": "| Field | Description | Required |\n| --- | --- | --- |\n| nts | Number of timesteps to include in the gif, default is 10 |  |\n| ngen_vars | Which ngen forcings variables to create gifs of, default is all of them |  |"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 14,
    "content": "```\nngen_variables = [\\\n    \"UGRD_10maboveground\",\\\n    \"VGRD_10maboveground\",\\\n    \"DLWRF_surface\",\\\n    \"APCP_surface\",\\\n    \"precip_rate\",\\\n    \"TMP_2maboveground\",\\\n    \"SPFH_2maboveground\",\\\n    \"PRES_surface\",\\\n    \"DSWRF_surface\",\\\n]\n\n```"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 15,
    "content": "## nwm_file\n\nA text file given to forcingprocessor that contains each nwm forcing file name. These can be URLs or local paths. This file can be generated with the [nwmurl tool](https://github.com/CIROH-UA/nwmurl) and a [generator script](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/src/forcingprocessor/nwm_filenames_generator.py) has been provided within this repo. The config argument accepts an s3 URL.\n\n```\npython nwm_filenames_generator.py conf_nwm_files.json\n\n```\n\nAn example configuration file:\n\n```\n{\n   \"forcing_type\" : \"operational_archive\",\n   \"start_date\"   : \"202310300000\",\n   \"end_date\"     : \"202310300000\",\n   \"runinput\"     : 1,\n   \"varinput\"     : 5,\n   \"geoinput\"     : 1,\n   \"meminput\"     : 0,\n   \"urlbaseinput\" : 7,\n   \"fcst_cycle\"   : [0],\n   \"lead_time\"    : [1]\n}\n\n```"
  },
  {
    "idurl": 72,
    "idtype": "text",
    "order": 16,
    "content": "## Weights\n\nTo calculate NextGen forcings, \"weights\" must be calculated to extract polygon averaged data from gridded data. The weights are made up of two parts, the `cell_id` and `coverage`. These are calculated via [exactextract](https://github.com/isciences/exactextract) within [weights_hf2ds.py](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/src/forcingprocessor/weights_hf2ds.py), which is optionally called from forcingprocessor.\n\nIf a geopackage is supplied to forcingprocessor, it will be searched for the layer `forcings-weights`. If this layer is found, these weights are used during processing. If not, forcingprocessor will call [weights_hf2ds.py](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/src/forcingprocessor/weights_hf2ds.py) to calculate the weights (cell_id and coverage) for every divide-id in the geopackage. This can take time, so forcingprocessor will write a parquet of weights out in the metadata, that can be reused in future forcingprocessor executions.\n\nExample of direct call\n\n```\npython3 /ngen-datastream/forcingprocessor/src/forcingprocessor/weights_hf2ds.py \\\n--outname ./weights.parquet \\\n--input_file ./nextgen_VPU_03W.gpkg\n\n```"
  },
  {
    "idurl": 73,
    "idtype": "text",
    "order": 1,
    "content": "# Research DataStream\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/README.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/README.md).\n\nThe Research DataStream is an array of daily NextGen executions in the AWS cloud. An exciting aspect of the Research DataStream is the NextGen configuration is open-sourced and community editable, which allows any member of the hydrologic community to contribute to improving forecasts. By making the NextGen forcings, outputs, and configuration publicly available, it is now possible to leverage regional expertise and incrementally improve hydrologic forecasts made with the NextGen Framework.\n\nNow generating daily NextGen data! -> [https://datastream.ciroh.org/index.html](https://datastream.ciroh.org/index.html)\n\nFind out how you can contribute [here!](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/CONTRIBUTE.md)\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/CONTRIBUTE.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/research_datastream/CONTRIBUTE.md).\n\nThe current configuration of the NextGen Research DataStream are public [here](https://datastream.ciroh.org/index.html#realizations/). These files are picked up directly during the NextGen executions in AWS Cloud.\n\nTo contribute to this realization file follow these steps.\n\n1. Improve upon parameterization in the VPU realization file\n2. Email James Halgren at [jshalgren@ua.edu](mailto:jshalgren@ua.edu) with your updated NextGen configuration"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 1,
    "content": "# NWMURL Library\n\nnwmurl is a Python library developed by CIROH 2023. It provides utility functions specifically designed to subset and generate National Water Model (NWM) data URLs. This library simplifies the process of accessing NWM data for various purposes such as analysis, modeling, and visualization.\n\nDeveloped by CIROH 2023"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 2,
    "content": "## Installation\n\nYou can install `nwmurl` using pip:\n\n```bash\npip install nwmurl\n```"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 3,
    "content": "## Usage\n\n1. In the code, you can modify the input parameters, such as `start_date`, `end_date`, `fcst_cycle`, `lead_time`, `varinput`, `geoinput`, and `runinput`, to customize the NWM data retrieval.\n\n2. The code will generate a list of JSON header URLs tailored to your specified parameters using the `generate_urls` function."
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 4,
    "content": "## Customize Your Data Retrieval for Operational Dataset\n\n- `start_date`: A string representing the starting date in the format \"YYYYMMDDHHMM\".\n- `end_date`: A string representing the ending date in the same format.\n- `fcst_cycle`: A list of integers specifying forecast cycle numbers, e.g., `[0, 1, 2, 3, 4]`. These cycles represent specific points in time for which URLs will be generated.\n- `lead_time`: A list of integers indicating lead times in hours for forecasts. It determines the time ahead of the forecast start, e.g., `[1, 2, 3, 4]`.\n- `varinput`: An integer or string representing the variable of interest within the NWM data. Available options include:\n  - `1` or `\"channel_rt\"` for channel routing data.\n  - `2` or `\"land\"` for land data.\n  - `3` or `\"reservoir\"` for reservoir data.\n  - `4` or `\"terrain_rt\"` for terrain routing data.\n  - `5` or `\"forcing\"` for forcing data.\n- `geoinput`: An integer or string specifying the geographic region of interest. Options include:\n  - `1` or `\"conus\"` for the continental United States.\n  - `2` or `\"hawaii\"` for Hawaii.\n  - `3` or `\"puertorico\"` for Puerto Rico.\n- `runinput`: An integer or string representing the NWM run configuration. Available options include:\n  - `1` or `\"short_range\"` for short-range forecasts.\n  - `2` or `\"medium_range\"` for medium-range forecasts.\n  - `3` or `\"medium_range_no_da\"` for medium-range forecasts without data assimilation.\n  - `4` or `\"long_range\"` for long-range forecasts.\n  - `5` or `\"analysis_assim\"` for analysis-assimilation runs.\n  - `6` or `\"analysis_assim_extend\"` for extended analysis-assimilation runs.\n  - `7` or `\"analysis_assim_extend_no_da\"` for extended analysis-assimilation runs without data assimilation.\n  - `8` or `\"analysis_assim_long\"` for long analysis-assimilation runs.\n  - `9` or `\"analysis_assim_long_no_da\"` for long analysis-assimilation runs without data assimilation.\n  - `10` or `\"analysis_assim_no_da\"` for analysis-assimilation runs without data assimilation.\n  - `11` or `\"short_range_no_da\"` for short-range forecasts without data assimilation.\n- `urlbaseinput`: An integer representing the NWM dataset. Available options include:\n  - `1`: \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/\".\n  - `2`: \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/post-processed/WMS/\".\n  - `3`: \"https://storage.googleapis.com/national-water-model/\".\n  - `4`: \"https://storage.cloud.google.com/national-water-model/\".\n  - `5`: \"gs://national-water-model/\".\n  - `6`: \"gcs://national-water-model/\".\n  - `7`: \"https://noaa-nwm-pds.s3.amazonaws.com/\".\n  - `8`: \"s3://noaa-nwm-pds/\".\n  - `9`: \"https://ciroh-nwm-zarr-copy.s3.amazonaws.com/national-water-model/\".\n- `meminput`: An integer representing the ensemble member designation ranging from 0 to 7\n- `write_to_file`: A Boolean variable that saves the output urls into a .txt file if set 'True'"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 5,
    "content": "## Customize Your Data Retrieval for Retrospective Dataset\n\n- `start_date`: A string representing the starting date in the format \"YYYYMMDDHHMM\".\n- `end_date`: A string representing the ending date in the same format.\n- `urlbaseinput`: An integer representing the NWM dataset. Available options include:\n  - `1`: \"https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/\".\n  - `2`: \"s3://noaa-nwm-retrospective-2-1-pds/model_output/\".\n  - `3`: \"https://ciroh-nwm-zarr-retrospective-data-copy.s3.amazonaws.com/noaa-nwm-retrospective-2-1-zarr-pds/\".\n  - `4`: \"https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/netcdf/\".\n- `selected_object_type`: An integer representing the object type. Available options include:\n  - `1` for forcing data\n  - `2` for model_output\n- `selected_var_types`: An integer or string representing the variable of interest within the NWM data. Available options include:\n  - `1`: \".CHRTOUT_DOMAIN1.comp\"\n  - `2`: \".GWOUT_DOMAIN1.comp\"\n  - `3`: \".LAKEOUT_DOMAIN1.comp\"\n  - `4`: \".LDASOUT_DOMAIN1.comp\"\n  - `5`: \".RTOUT_DOMAIN1.comp\"\n  - `6`: \".LDASIN_DOMAIN1.comp\"\n- `write_to_file`: A Boolean variable that saves the output urls into a .txt file if set `True`"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 6,
    "content": "## Examples of how to use\n\n1. For Operational dataset:\n\n```python\nimport nwmurl\n\nstart_date = \"202201120000\"\nend_date   = \"202201130000\"\nfcst_cycle = [0,8]\nlead_time = [1,18]\nvarinput = 1\ngeoinput = 1\nruninput = 1\nurlbaseinput = 2\nmeminput = 1\nwrite_to_file = False\n\nfile_list = nwmurl.generate_urls_operational(\n    start_date, end_date, fcst_cycle,\n    lead_time,\n    varinput,\n    geoinput,\n    runinput,\n    urlbaseinput,\n    meminput,\n    write_to_file\n)\n```\n\n2. For Retrospective dataset:\n\n```python\nimport nwmurl\n\nstart_date = \"200701010000\"\nend_date = \"200701030800\"\nurlbaseinput = 2\nselected_var_types = [1, 2]\nselected_object_types = [1]\nwrite_to_file = True\n\nfile_list = nwmurl.generate_urls_retro(\n    start_date,\n    end_date,\n    urlbaseinput,\n    selected_object_types,\n    selected_var_types,\n    write_to_file\n)\n```"
  },
  {
    "idurl": 74,
    "idtype": "text",
    "order": 7,
    "content": "## How to Contribute\n\nWe welcome contributions to nwmurl! To contribute to the development of this library, please follow these steps:\n\n1. Fork the repository on GitHub.\n\n2. Clone your fork to your local machine:\n\n`git clone https://github.com/CIROH-UA/nwmurl.git`\n\n3. Create a new branch for your contribution:\n\n`git checkout -b feature/your-feature-name`\n\n4. Make your code changes and improvements.\n\n5. Before submitting a pull request, make sure to update the package version in `setup.py` if necessary.\n\n6. Commit your changes with descriptive commit messages.\n\n7. Push your changes to your fork.\n\n8. Open a pull request on the main repository, describing your changes and why they should be merged.\n\nWe appreciate your contributions and will review your pull request as soon as possible. Thank you for helping improve nwmurl!"
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 1,
    "content": "# Getting Started with NETWA"
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 2,
    "content": "## Accessing the Testbed"
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 3,
    "content": "### Remote Desktop Software\n\nOne of the easiest ways to access the NETWA is through the a remote desktop software, such as Remote Desktop Connection, which is a pre-installed software on Windows. Microsoft also has a [version available for MacOs](https://apps.apple.com/us/app/microsoft-remote-desktop/id1295203466?mt=12) as well, but any software that allows remote desktop access will do. Below are steps to access your own virtual desktop on the testbed:\n\n1. Launch Remote Desktop Connection or similar software to connect to your virtual desktop\n2. In the \"computer:\" field, enter \"ciroh-testbed.uvm.edu\"\n3. Click \"connect\" and enter your university username and password when prompted\n4. Congratulations! You've just logged on to your very own NETWA virtual desktop"
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 4,
    "content": "### SSH\n\nAnother way to access the testbed is through the Secure Shell Protocol, more commonly known as SSH. This option is more appropriate for users who are familiar with using the Linux terminal or other command-line interfaces.\n\n1. Open a terminal application on your machine (there are built-in terminals on MacOS and Windows, or you can use a third-party app such as [PuTTY](https://putty.org/) or [MobaXterm](https://mobaxterm.mobatek.net/))\n2. Type the command `ssh your_netId@ciroh-testbed.uvm.edu`, using your actual netID\n3. Enter your password when prompted\n4. Now you should be logged in to the testbed and be able to navigate via the command-line"
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 5,
    "content": "## Data Storage\n\nThe `/netfiles/ciroh/` directory on the NETWA has over 40 terabytes of disk space that researchers can use to store data - that's equivalent to 40 million megabytes! Needless to say, there's plenty of space for data. If you are using any of the data modules in `forecast-workflow` that require a directory in which to store downloaded files, such as `nwm_fc.py` or `gfs_fc_thredds.py`, please use \"/netfiles/ciroh/downloadedData/\" as the download path. These modules will check the download path to see if the data you are requesting is already downloaded before attempting to get it again. Using a common shared path for large downloadable datasets like the NWM and GFS ensures an efficient use of disk space and also saves time by not having users redownload existing data! You can specify this download path in the module `get_data()` functions with the `data_dir` parameter. If you have other private data that doesn't make sense to share, please create your own directory in `/netfiles/ciroh` titled after your netID. For example, \"John Smith\" would store their personal data under `/netfiles/ciroh/jsmith`."
  },
  {
    "idurl": 75,
    "idtype": "text",
    "order": 6,
    "content": "## Setting up Mamba / Conda\n\n[Mamba](https://mamba.readthedocs.io/en/latest/index.html) is a lightweight version of [Conda](https://docs.conda.io/projects/conda/en/stable/), a popular package manager for a variety of programming languages. It is used on the testbed to set up virtual environments that contain all of the relevant packages and dependencies for a given software repository or workflow. There are a few existing mamba/conda Python environments on the testbed already, such as one that houses all of the packages necessary to run the forecast-workflow repo (more specific instructions for accessing that [here](https://docs.ciroh.org/docs/products/data-management/netwa/)). This section will demonstrate how to initialize mamba or conda for a new user and how to see what virtual environments are available.\n\n1. On the testbed, open a terminal and run the following command: `/usr/local/miniforge3/bin/mamba shell init` to install mamba or '/usr/local/miniforge3/bin/conda init' to install conda or both to install both!\n   1. _Note:_ you only have to do this once, not every time you open a new terminal or want to use mamba or conda.\n   2. If the command ran successfully, you should see something like this now at the command line:\n      1. `(base) [jsmith@ciroh-testbed ~]$`\n2. The `(base)` text indicates that you are in the base environment. To see the list of packages in said environment (or any environment you happen to be in), run `mamba list` or `conda list`.\n3. To see a list of available environments, run `mamba env list` or, if using conda, 'conda info --envs'.\n   1. Most relevant environments will be located in `/data/condaShared/envs`.\n   2. _Note:_ This is one of the VERY few commands that differ between mamba and conda. Thus, you can mostly use `mamba` and `conda` at the command line interchangeably.\n4. To activate an environment, simply run `mamba activate env_name` replacing `env_name` with the actual name of the environment."
  },
  {
    "idurl": 82,
    "idtype": "text",
    "order": 1,
    "content": "# Forcing Sources\n\n> **NOTE**\n>\n>  Below content is rendered from [https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/FORCING_SOURCES.md](https://github.com/CIROH-UA/ngen-datastream/blob/main/forcingprocessor/FORCING_SOURCES.md).\n\nThis forcings processor uses [nwmurl](https://github.com/CIROH-UA/nwmurl) to generate forcings file names from different data sources (accessed via `urlbaseinput`). Which means this processor must handle files stored in both the National Water Model v3 operational forcings in amazon Google cloud storage and v2 retrospective forcings in amazon s3 object storage, just to name a couple data sources. Not all of these data sources have been implemented as of summer 2024. A check below indicates that this forcings processor knows how to read in data from the source.\n\nOperational forcings sources\n\n- 1: \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/\" ✅\n- 2: \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/post-processed/WMS/\"\n- 3: \"https://storage.googleapis.com/national-water-model/\" ✅\n- 4: \"https://storage.cloud.google.com/national-water-model/\" ✅\n- 5: \"gs://national-water-model/\" ✅\n- 6: \"gcs://national-water-model/\" ✅\n- 7: \"https://noaa-nwm-pds.s3.amazonaws.com/\" ✅\n- 8: \"s3://noaa-nwm-pds/\" ✅\n- 9: \"https://ciroh-nwm-zarr-copy.s3.amazonaws.com/national-water-model/\"\n\nRetrospective forcings sources\n\n- 1: \"https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/\" ✅\n- 2: \"s3://noaa-nwm-retrospective-2-1-pds/model_output/\" ✅\n- 3: \"https://ciroh-nwm-zarr-retrospective-data-copy.s3.amazonaws.com/noaa-nwm-retrospective-2-1-zarr-pds/\"\n- 4: \"https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/netcdf/\" (✅, [issue 52](https://github.com/CIROH-UA/nwmurl/issues/52))"
  },
  {
    "idurl": 130,
    "idtype": "text",
    "order": 1,
    "content": "# Working with the DocuHub Repository\n\nDocuHub is hosted on GitHub, which offers plenty of options to contribute your changes to the site."
  },
  {
    "idurl": 130,
    "idtype": "text",
    "order": 2,
    "content": "## Minor Edits\n\nTo make minor edits, follow these steps:\n\n1. Visit [docs.ciroh.org](https://docs.ciroh.org/) and navigate to the page you wish to modify.\n2. Click on \"Edit page\" at the bottom of the page to make any necessary changes.\n3. Submit a Pull Request.\n4. An admin will review and merge your changes."
  },
  {
    "idurl": 130,
    "idtype": "text",
    "order": 3,
    "content": "## Major Edits\n\nFor significant modifications, please adhere to these steps:\n\n1. Fork the repository from [https://github.com/CIROH-UA/ciroh-ua_website](https://github.com/CIROH-UA/ciroh-ua_website).\n2. After forking, implement your changes and commit them to your local repository.\n3. Open a pull request. Once submitted, an admin will review and merge it.\n4. GitHub Actions will automatically compile and publish the updates.\n\nIf you encounter any issues or have inquiries, please feel free to email us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu). Your contributions are highly valued!"
  },
  {
    "idurl": 130,
    "idtype": "text",
    "order": 4,
    "content": "## Testing changes locally\n\n> Please be sure to create a fork using the steps above before continuing.\n\n1. If you haven't already, download and install the LTS version of Node.js from [here](https://nodejs.org/en).\n2. To build and run the project locally, navigate to the project directory in your command line interface of choice.\n3. Execute the following commands:\n\n```bash\nnpm install\nnpm run build\nnpm run start\n```\n\nTo quickly test out your changes, use this command:\n\n```bash\n$ npm run start\n```\n\nThis command launches a local development server and opens a browser window. Changes are typically reflected instantly without requiring a server restart.\n\nTo deploy the website, you can instead compile the project into a deployable package:\n\n```bash\n$ npm run build\n```\n\nThis will create a build directory within your project folder. You can then deploy the contents of this directory to your web server."
  },
  {
    "idurl": 130,
    "idtype": "text",
    "order": 5,
    "content": "## Video Tutorial\n\nStill need some extra help? If so, the following video is a step-by-step guide on how to make minor edits, major edits, and test changes locally:\n\n[How to Contribute to CIROH DocuHub? - YouTube](https://www.youtube.com/watch?v=B8wp_eTW204)"
  },
  {
    "idurl": 131,
    "idtype": "text",
    "order": 1,
    "content": "# Adding Blog Posts to DocuHub\n\nThis page explains how to add new posts to the DocuHub Blog's feed as a developer.\n\nIf you're a researcher looking to submit a blog post, you may want to consider filling out the [Blog Post Request Form](https://github.com/CIROH-UA/ciroh-ua_website/blob/main/.github/ISSUE_TEMPLATE/docuhub-blog-post.md) instead.\n\n> **Info**: Please remember that blog posts submitted to CIROH DocuHub should discuss projects that make use of CIROH's cyberinfrastructure."
  },
  {
    "idurl": 131,
    "idtype": "text",
    "order": 2,
    "content": "## Adding posts manually\n\n> Before continuing, you may want to set up a local environment for DocuHub. Please see the \"Major Edits\" section of the [Working with the DocuHub Repository](https://docs.ciroh.org/docs/contribute/repository#major-edits) page for more information.\n\nAll DocuHub blog posts are written in the markdown format, with the extension `.md`. As such, they're compatible with all of the typical markdown syntax. Among other things, you can _italicize text_ with `*one asterisk on either side*`, write **bold text** using `**two asterisks**`, or create subheaders by `## prefacing a line with two or three hash marks.`\n\n> A markdown cheatsheet is available [here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)."
  },
  {
    "idurl": 131,
    "idtype": "text",
    "order": 3,
    "content": "### Naming files\n\nTo publish in the blog, create a Markdown file within the `/blog/` directory.\nFor example, the file at `/blog/2019-09-05-hello-docuhub.md` represents the first post written for this site.\n\nDocuHub will extract a YYYY-MM-DD date from many patterns such as `YYYY-MM-DD-my-blog-post-title.md` or `YYYY/MM/DD/my-blog-post-title.md`. This allows for blog posts to easily be sorted and grouped by when they were published."
  },
  {
    "idurl": 131,
    "idtype": "text",
    "order": 4,
    "content": "### Formatting blog posts\n\nThe top of the blog post should contain metadata, which is used to store information about your article. This includes:\n\n- Your article's title and description.\n- Your article's \"slug\", or the text used for its URL. Please be sure to only include letters, numbers, and dashes (-) in the slug!\n- Author information. This includes your name, a bit of information about yourself, a link to a website that represents you, and a photo.\n- Tags! These can be used to find articles relevant to a specific topic, so it's a good idea to several that are relevant to your article. This can include the tools discussed in your article, as well as the institutions or events where the relevant research was hosted. For some examples of relevant, helpful tags, check out the [current blog posts](https://docs.ciroh.org/blog/).\n\nThis metadata is wrapped by three dashes ( `---`) on the lines above and below it, and is parsed as YAML data. If you're not sure what that means, don't worry - the template below offers an example of the format.\n\nAdditionally, blog posts should include a line containing the text `<!-- truncate -->` directly below their introductions. This tells DocuHub where the blog post's introduction ends, which allows it to display the introduction as a short-form summary in the blog's feed."
  },
  {
    "idurl": 131,
    "idtype": "text",
    "order": 5,
    "content": "### Blog post template\n\nTo use the template below, copy and paste it into an empty `.md` file in the `/blogs/` folder.\n\n```markdown\n---\ntitle: Your article's title!\ndescription: A one-sentence summary of your blog post.\nslug: your-articles-title\nauthors:\n  - name: John Doe\n    title: Co-creator of Product 1\n    url: <Your github product or external article link>\n    image_url: <Author pic url>\n  - name: Jane Doe\n    title: Co-creator of Product 2\n    url: <Your github product or external article link>\n    image_url: <Author pic url>\ntags: [hello, docuhub, nextgen]\nhide_table_of_contents: false\n---\n\nThe text above the truncate line below will be displayed as a\nstandalone summary of your article, so it's a great place for\na quick and informative introduction.\n\n<!-- truncate -->\n\nBelow the truncate line, you can add the rest of your blog post!\n```"
  },
  {
    "idurl": 132,
    "idtype": "text",
    "order": 1,
    "content": "# DocuHub Technologies\n\nHere's a quick look at some of the infrastructure that underpins DocuHub."
  },
  {
    "idurl": 132,
    "idtype": "text",
    "order": 2,
    "content": "## Docusaurus\n\nDocusaurus is a modern static website generator specially tuned to make it easier to maintain and update documentation. Docusaurus forms the core of DocuHub, and is responsible for handling most of the site's content.\n\n- [Docusaurus Homepage](https://docusaurus.io/)\n- [Docusaurus Documentation](https://docusaurus.io/docs/)"
  },
  {
    "idurl": 132,
    "idtype": "text",
    "order": 3,
    "content": "## Markdown\n\nMarkdown is used by Docusaurus for interpreting plain-text documentation. If you need extra help with markdown, check out the cheatsheet below.\n\n[Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"
  },
  {
    "idurl": 132,
    "idtype": "text",
    "order": 4,
    "content": "## Infima\n\nInfima is an open-source UI styling framework that makes it easier to develop websites using minimal CSS and JS knowledge. All Infima classes are freely available to style and shape your work on Docuhub.\n\n[Infima Documentation](https://docusaurus.io/docs/styling-layout#styling-your-site-with-infima)"
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB Surpasses 10,000 Docker Pulls: A Milestone for Accessible Water Modeling\n\n![NGIAB Banner](https://docs.ciroh.org/assets/images/ngiab-banner-439fbd18af5380efe86aaaf146649287.png)\n\nWe're thrilled to announce that **NextGen In A Box (NGIAB)** has surpassed **10,000 Docker pulls** — a significant milestone reflecting the growing adoption of water modeling tools that are accessible to all. This achievement creates opportunities for researchers, practitioners, and students worldwide to leverage advanced water prediction frameworks without infrastructure barriers, accelerating global water science innovation."
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 2,
    "content": "## From Research Innovation to Community Tool\n\nWhen we first containerized the NextGen Water Resources Modeling Framework into NGIAB, our goal was simple yet ambitious: **remove the technical barriers** that prevented many researchers from accessing NOAA's next-generation water modeling capabilities.\n\nToday, with over 10,000 downloads, it's clear the community was ready for this transformation.\n\nThe University of Alabama recently highlighted NGIAB's impact in their news feature, _[\"UA Software Makes Water Modeling More Accessible\"](https://news.ua.edu/2025/07/ua-software-makes-water-modeling-more-accessible/)_, recognizing how this tool is changing the landscape of hydrologic research and education. As the article notes, NGIAB turns what was once a complex, infrastructure-heavy process into something that researchers can run on their laptops in minutes."
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 3,
    "content": "## What 10,000 Pulls Really Means\n\nBehind this number are stories of:\n\n🎓 **Graduate students** exploring advanced modeling techniques with no setup headaches\n\n🏫 **Educators** bringing cutting-edge tools into the classroom\n\n🏢 **Researchers at smaller institutions** gaining access to national-scale modeling\n\n🌍 **International collaborators** contributing to water modeling advancement without infrastructure constraints\n\n🚨 **Emergency managers** rapidly deploying models for flood prediction"
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 4,
    "content": "## Community Growth and Impact\n\nThe rapid adoption of NGIAB reflects a broader movement:\n\n**1. Democratization of Advanced Modeling**\n\nNo longer do researchers need access to specialized HPC resources or deep DevOps knowledge to run sophisticated water models. NGIAB levels the playing field.\n\n**2. Reproducible Science**\n\nEvery one of those 10,000 pulls represents the exact same computational environment, ensuring that research results can be replicated anywhere in the world.\n\n**3. Accelerated Innovation**\n\nBy removing setup friction, researchers can focus on science rather than software configuration, leading to faster iterations and discoveries."
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 5,
    "content": "## Looking Forward: The Next 10,000\n\nAs we celebrate this milestone, we're already working on what's next:\n\n📦 **Expanded model library**: Adding more BMI-compliant models to the NGIAB ecosystem\n\n☁️ **Cloud integration**: Seamless deployment on AWS, Google Cloud, and Azure\n\n📊 **Enhanced visualization**: Built-in tools for analyzing and presenting model outputs\n\n🤝 **Community contributions**: Making it easier for users to share their configurations and improvements"
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 6,
    "content": "## Join the NGIAB Community\n\nWhether you're pull number 10,001 or have been with us since the beginning, you're part of a growing community that's transforming water resources modeling.\n\nHere's how to get involved:\n\n👉 [**Try NGIAB**](https://docs.ciroh.org/training-NGIAB-101): Visit the NGIAB 101 tutorial to get started\n\n✍️ [**Share your use case**](https://docs.ciroh.org/contact): Tell us how you're using NGIAB in your research or operations\n\n🛠 [**Contribute**](https://docs.ciroh.org/docs/products/ngiab/distributions/ngiab-docker/contribute): Submit bug reports, feature requests, or code contributions\n\n📢 **Spread the word**: Help others discover how NGIAB can accelerate their water modeling work"
  },
  {
    "idurl": 133,
    "idtype": "text",
    "order": 7,
    "content": "## Thank You to Our Community\n\nThis milestone belongs to everyone who downloaded, tested, provided feedback, contributed code, or helped spread the word about NGIAB. Your engagement drives our continuous improvement and motivates us to make water modeling even more accessible.\n\nAs we reflect on reaching 10,000 Docker pulls, we're reminded that each download represents a researcher, student, or practitioner working to better understand and predict our water resources. Together, we're building a more resilient future through accessible, advanced water modeling.\n\nHere's to the next 10,000 pulls and the continued growth of our community!\n\n---\n\n_NGIAB is developed and maintained by the Cooperative Institute for Research to Operations in Hydrology (CIROH) at the University of Alabama. Learn more about our work in making water modeling accessible at [ciroh.org](https://ciroh.org/)._"
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 1,
    "content": "# CIROH at NHWC 2025: Advancing Hydrologic Science Through Innovation and Collaboration\n\n![NHWC 2025 Banner at Tucson, Arizona](https://docs.ciroh.org/assets/images/team-2d5a6ca72cac67eb9feaeb97aef20dc1.JPG)\n\n> _CIROH team at NHWC 2025 in Tucson, Arizona, standing by the event's official banner._\n\nCIROH had a strong showing at the 15th Biennial National Hydrologic Warning Conference (NHWC 2025), with our researchers presenting innovative solutions and engaging with the broader hydrologic warning community. The conference brought together field personnel, innovators, engineers, hydrologists, forecasters, water resource managers, and emergency management officials from across the country to advance flood warning systems and address emerging challenges in evolving climate and drought management."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 2,
    "content": "## Advancing Model Reliability in Hydrologic Forecasting\n\nDuring the NextGen presentation track, moderated by Dr. Steve Burian, CIROH researchers focused on improving evaluation techniques for hydrologic models—particularly in the context of the National Water Model (NWM)."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 3,
    "content": "## NWS NextGen Program\n\n![Edwin Welles presenting on NextGen Operational Foundation](https://docs.ciroh.org/assets/images/edwin-21f1942f25afc6c83876533bf898543d.JPG)\n\n> _Edwin Welles from Deltares USA presenting the NextGen framework._\n\n**Edwin Welles** from Deltares USA kicked things off with insights into a groundbreaking initiative.\nThe Office of Water Prediction, of the National Weather Service has undertaken an ambitious project to implement and apply a new framework for hydrological modeling: the\nThe Next Generation Water Resources Modeling Framework, or the NextGen framework, is designed to emphasize modularity, interoperability, and reproducibility, creating an environment where disparate modeling components can be freely combined to enhance modeling capabilities.\nThese advantages offer critical support to the National Water Model, alongside other key initiatives such as national Flood Inundation Mapping research.\nThe system is being implemented by Raytheon with subcontract support from Deltares USA. His presentation provided a summary of the project and envisioned outcomes."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 4,
    "content": "## Leading the NextGen Revolution\n\n![Arpita Patel presenting NGIAB](https://docs.ciroh.org/assets/images/arpita1-9a1e419967282204712df89b4d846879.JPG)\n\n> _Arpita Patel presenting \"Navigating the NextGen Ecosystem and NextGen In A Box (NGIAB).\"_\n\n**Arpita Patel** followed with _\"Navigating the NextGen Ecosystem and NextGen In A Box (NGIAB),\"_ a presentation focused on making the NextGen framework more accessible to researchers, forecasters, and water managers.\nThis was achieved through the NextGen In A Box platform, which harnesses containerization and supportive tooling to make the NextGen framework accessible and easy to use.\nThis work directly supports CIROH's mission to democratize access to advanced hydrologic modeling capabilities and foster broader adoption of next-generation water prediction tools.\n\n🥉 Patel also demonstrated exceptional engagement throughout the conference. Her outstanding participation earned her an impressive **#3 ranking on the NHWC 2025 conference leaderboard** with 70,200 points.\nThis demonstrated not only her technical expertise, but also her active involvement in conference activities and community engagement."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 5,
    "content": "## Rethinking Evaluation\n\n![Dr. Md Shahabul Alam presenting model evaluation methods](https://docs.ciroh.org/assets/images/shahab-e8e53d36f761568f5640a4f085e5c08e.jpg)\n\n> _Dr. Md Shahabul Alam presenting different model evaluation methods._\n\nDr. **Md Shahabul Alam** also delivered a compelling talk titled \"Rethinking Model Reliability: A Dual Evaluation of NWM Using Time Series and Extreme Events\" during the NextGen program track. His work focuses on advancing operational hydrology by developing more rigorous and resilient methods to evaluate the National Water Model across varying temporal scales and extreme event conditions—a crucial step in improving forecasting and decision-making under a changing climate.\nThis work represents CIROH's commitment to enhancing the reliability and accuracy of national-scale hydrologic prediction systems, directly supporting the operational forecasting community's need for trustworthy model outputs during both routine conditions and extreme weather events.\n\n🥇 Dr. Alam earned also earned the **#1 ranking on the NHWC 2025 conference leaderboard**, maintaining a commanding lead of over 100,000 points throughout the week and finishing with a remarkable 194,700 points.\nHis name quickly became a focal point of the conference. Each morning, attendees checked the Whova app with anticipation—only to see Shahab had once again pulled further ahead.\nHis energetic participation and engaging presence made him a standout at the conference, combining technical depth with social and technological savvy.\nDr. Alam not only demonstrated exceptional leadership and charisma at the NHWC 2025 conference, but also brought well-deserved visibility to CIROH and its ongoing mission."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 6,
    "content": "## Strengthening Community Connections\n\nThe inclusion of these presentations within the NextGen presentation track highlights CIROH's central role in advancing the NextGen Water Resources Modeling Framework. This alignment demonstrates our organization's commitment to translating research innovations into operational capabilities that serve the broader water resources community.\n\nFollowing the presentations, many researchers expressed immediate interest in using NextGen In A Box for both their research applications and operational forecasting. The tool's modular design, ease of deployment, and compatibility with diverse datasets resonated strongly with attendees seeking flexible and scalable modeling solutions. Several participants noted its potential for accelerating innovation in hydrologic modeling and expressed intent to integrate it into their ongoing and future projects.\n\nThe conference provided valuable opportunities for our researchers to connect with operational forecasters, emergency managers, and fellow scientists, fostering collaborations that will enhance the real-world impact of CIROH's research initiatives. These interactions are essential for ensuring our scientific advances address the practical challenges faced by those responsible for flood warning and water resource management."
  },
  {
    "idurl": 134,
    "idtype": "text",
    "order": 7,
    "content": "## Looking Ahead\n\nCIROH's strong presence at NHWC 2025 underscores our dedication to bridging the gap between research and operations in hydrologic science. Our researchers' contributions to model evaluation methodologies and NextGen ecosystem development represent key steps toward more reliable, accessible, and effective water prediction systems.\n\nAs we continue to advance hydrologic science, conferences like NHWC 2025 provide essential platforms for sharing innovations, gathering feedback from the operational community, and building the collaborative relationships that drive progress in water resources management and flood warning systems.\n\nThe success of our team at NHWC 2025 reflects CIROH's growing impact in the hydrologic sciences community and our commitment to developing solutions that serve both scientific advancement and societal needs."
  },
  {
    "idurl": 135,
    "idtype": "text",
    "order": 1,
    "content": "# Tethys Summit 2025: Advancing Geoscientific Web Applications\n\n![Musicians at Tethys Summit 2025](https://docs.ciroh.org/img/blog/2025-08-tethys-summit/tethys-summit-6.jpg)"
  },
  {
    "idurl": 135,
    "idtype": "text",
    "order": 2,
    "content": "## My Experience at Tethys Summit 2025\n\nEarlier this month, I had the opportunity to attend Tethys Summit 2025 in Tampa, FL. It was a rewarding experience to learn about the Tethys Platform and how researchers, hydrologists, and geospatial scientists are applying it in their work. Through workshops and technical demonstrations, I gained insights into how this open-source Earth science platform is advancing environmental problem-solving.\n\nOn Day 1, I participated in the **Tethys Dash** workshop led by Corey Krewson, where I learned to build a Tethys dashboard without writing any code. During the hands-on session, I was surprised by how easily I was able to build dashboards using a drag-and-drop interface, as much of the technical implementation was abstracted for simplicity. I also attended the **Tethys Component Apps (ReactPy)** workshop by Shawn Crawley. Unlike the no-code approach of the first session, this workshop focused on building a basic interactive Tethys web app in under 50 lines of Python code.\n\nOn Day 2, I also took part in the **Contributing to Tethys Core** workshop led by Nathan Swain. This workshop outlined the steps for first-time contributors to get involved and submit pull requests to the Tethys Core repository. Aside from the workshops, I took part in Tethys App Competition where I presented the Tethys app I developed. It was a great learning experience to understand more about web app developments using Tethys. I ended up winning second place and got a bonsai tree LEGO set!\n\nOverall, it was a very engaging and informative summit. For anyone who missed the session, the workshop materials are available here: [**Talks and Workshop Materials**](https://drive.google.com/drive/folders/18sSPsMkjSRtBZLAClEUV4AetKx9dPS6B)\n\n_— Manjila_\n\n---"
  },
  {
    "idurl": 135,
    "idtype": "text",
    "order": 3,
    "content": "## What is Tethys?\n\n[**Tethys Platform**](https://www.tethysplatform.org/) is an open-source, Python-powered framework that lets geoscientists and developers turn complex environmental data and models into browser-based, interactive web GIS apps—without having to become web gurus. Built on [Django](https://www.djangoproject.com/), it bundles a full spatial stack ([PostGIS](https://postgis.net/), [GeoServer](http://geoserver.org/), [OpenLayers](https://openlayers.org/)/[Cesium](https://cesium.com/)) plus handy \"Gizmos\" for maps, charts, and dashboards, and it integrates seamlessly with scientific libraries like NumPy, Pandas, Plotly, and Bokeh. A built-in job manager runs heavy climate or hydrologic models—locally or in the cloud—while APIs make it easy to pull data from THREDDS servers or PostGIS databases. The result is a low-barrier path to custom interfaces, 2D/3D visualizations, and scalable computing that's already powering water-resource tools, climate-scenario explorers, and disaster-monitoring apps worldwide. Backed by the [Tethys Geoscience Foundation](https://www.tethysgeoscience.org/), the platform's active open-source community keeps it evolving to meet new Earth-science challenges."
  },
  {
    "idurl": 135,
    "idtype": "text",
    "order": 4,
    "content": "## Community Impact\n\nThanks to these capabilities, Tethys Platform is widely adopted in the geoscience community. Scientists around the world use it for a range of applications—water resource management, climate change analysis, natural disaster monitoring, environmental planning, and more. In each case, Tethys helps turn complex datasets and model outputs into accessible tools for decision-makers. As an open-source project under the [**Tethys Geoscience Foundation**](https://www.tethysgeoscience.org/), it benefits from an active user community that continually contributes improvements and new features. This collaborative growth ensures the platform keeps pace with emerging needs in Earth science, effectively lowering the barrier for geoscientists to share data and models as powerful web applications."
  },
  {
    "idurl": 135,
    "idtype": "text",
    "order": 5,
    "content": "## Tethys Platform Resources\n\nTethys Platform offers several resources to help you get started, including comprehensive official documentation, step-by-step tutorials, and a live demo portal. [The official documentation](https://docs.tethysplatform.org/) covers everything from installation and \"Hello World\" examples to advanced topics and architecture (explaining how the Django backend and various services fit together). For hands-on learning, the [tutorials](https://docs.tethysplatform.org/en/stable/tutorials.html) guide you through setting up your first app, incorporating spatial data with built-in map layouts, connecting to GeoServer or THREDDS data servers, and even running cloud computations. And if you want to see Tethys in action without installing anything, the [**Tethys Demo Portal**](https://demo.tethysplatform.org/apps) showcases a gallery of live example apps, such as a Gizmo Showcase of interactive widgets and a Wildfire Visualizer that displays wildfire data on a map, giving you a clear idea of the platform's capabilities and inspiring ideas for your own geoscientific web applications.\n\nEach of these resources will help you dive deeper into the Tethys Platform. Whether you're looking to develop a water resources tool, a climate data viewer, or any geoscientific web application, the combination of documentation, tutorials, and live demos will support you on your journey with Tethys. **Happy app building!**\n\n_— Gio_"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling\nsource: https://docs.ciroh.org/blog/February%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\nPennsylvania State University (PSU) researchers have been leveraging CIROH Cyberinfrastructure to tackle complex hydrological modeling challenges. This post highlights their innovative approach using the [Wukong computing platform](https://docs.ciroh.org/docs/services/on-prem/Wukong/) in conjunction with Amazon S3 bucket storage to efficiently process and analyze large-scale environmental datasets. 🚀\n\n* * *"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 2,
    "content": "## 💻 The Computing and Storage Infrastructure"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 3,
    "content": "### **Wukong Computing Platform**\n\nThe PSU team has been utilizing Wukong, a high-performance computing (HPC) cluster specifically designed for data-intensive scientific applications, such as the high-resolution physics-informed machine learning for national water modeling (Song et al. 2024[YS1] ). Wukong provides the computational power necessary for running complex simulations and processing large environmental datasets that traditional computing resources would struggle with. 🔍"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 4,
    "content": "#### Key advantages of Wukong include:\n\n- 🎯 **Large GPU capacity** for high-resolution ML/differentiable process-based models\n- ⚙️ **Scalable parallel processing** capabilities\n- 🚀 **Optimized performance** for data-intensive workloads\n- ⏳ **Reduced processing time** for big data\n- 🌐 **Support for multi-node computation** to handle larger geographical areas"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 5,
    "content": "#### **S3 Bucket Integration**:\n\nTo complement Wukong's computational power, the PSU researchers and AWI DevOps staff implemented **Amazon S3 (Simple Storage Service)** buckets as their secondary data storage solution. This integration offers several benefits:\n\n- 🗄️ **Virtually unlimited storage capacity** for growing datasets\n- 🔒 **Data durability and redundancy**\n- 💰 **Cost-effective long-term storage**, with the use of S3 intelligent tiering to automate the storage cost savings by moving data when access patterns change\n- 🔄 **Seamless data transfer** between computing nodes\n- 📝 **Version control** for dataset iterations\n- 🤝 **Easy data sharing** with users not on Wukong\n\n* * *"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 6,
    "content": "## 🔬 Research Applications\n\nThe PSU team has applied this powerful computing infrastructure to several critical research areas:\n\n1. **National Streamflow Modeling** 🌊\n\nTraining differentiable hydrologic models with high-resolution forcing and static attribute data across extensive geographical regions using observations from thousands of gauges, followed by whole-domain forwarding.\n\n2. **National River Routing** 🗺️\n\nConducting river routing on MERIT/HydroFabric river networks, combined with neural network-supported routing parameter learning.\n\n3. **NextGen Candidate Models & Data Assimilation** 🔄\n\nApplying multiple NextGen candidate models and data assimilation algorithms within the differentiable modeling framework, which supports compliance with [BMI](https://doi.org/10.5281/zenodo.14827983).\n\n4. **Foundation Model Development** 🏞️\n\nDeveloping a foundation model to explore co-evolution between landscapes.\n\n\n* * *"
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 7,
    "content": "### Thank you to all those who contributed towards this effort."
  },
  {
    "idurl": 144,
    "idtype": "text",
    "order": 8,
    "content": "## 🔗 Learn More\n\nFor more details on the Wukong computing platform, check out the official documentation:\n\n👉 [Wukong Documentation](https://docs.ciroh.org/docs/services/on-prem/Wukong/)\n\nFor the full research paper by Song et al. (2024), visit:\n\n👉 [DOI: 10.22541/essoar.172736277.74497104/v1](https://doi.org/10.22541/essoar.172736277.74497104/v1)\n\n* * *\n\n- [💻 The Computing and Storage Infrastructure](#-the-computing-and-storage-infrastructure)\n  - [**Wukong Computing Platform**](#wukong-computing-platform)\n- [🔬 Research Applications](#-research-applications)\n  - [Thank you to all those who contributed towards this effort.](#thank-you-to-all-those-who-contributed-towards-this-effort)\n- [🔗 Learn More](#-learn-more)"
  },
  {
    "idurl": 145,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH at AGU 2024\nsource: https://docs.ciroh.org/blog/December%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\nAGU24 brought together the world's leading minds in Earth and space sciences. CIROH participated actively, showcasing advances in water prediction, modeling techniques and many more technologies."
  },
  {
    "idurl": 145,
    "idtype": "text",
    "order": 2,
    "content": "## **Presentations and Posters** 📊\n\nThe conference provided an excellent platform for CIROH researchers to present their groundbreaking work. Our team delivered impactful presentations and poster sessions highlighting CIROH's innovative work, including advancements in water prediction systems\nand community water modeling.\n\nThese sessions sparked thought-provoking discussions and fostered collaborations with other researchers. For those who missed it, **posters and presentation slides are now available** [here](https://github.com/CIROH-UA/Conferences/tree/main/AGU24). Feel free to explore these materials and share your thoughts. 📝"
  },
  {
    "idurl": 145,
    "idtype": "text",
    "order": 3,
    "content": "### Contribute Your Posters or Presentations!\n\nIf any CIROH researchers would like to add their own posters or presentations to this repository, please feel free to send a [pull request (PR)](https://github.com/CIROH-UA/Conferences/pulls). We welcome contributions and encourage collaboration!"
  },
  {
    "idurl": 145,
    "idtype": "text",
    "order": 4,
    "content": "## Photo Gallery 📸\n\nWe captured some unforgettable moments from AGU 2024. From vibrant sessions to meaningful conversations at our booth, here's a glimpse:\n\n![AGU24_02](https://docs.ciroh.org/assets/images/AGU24_02-043c5ded45817d5b40046511dce3d6e9.gif)![AGU24_01](https://docs.ciroh.org/assets/images/AGU24_01-f8b35ac0af019ad8db90cb0f23c0aafa.jpg)![AGU24_05](https://docs.ciroh.org/assets/images/AGU24_05-28874672330bf51db3cecb641dee6537.gif)![AGU24_06](https://docs.ciroh.org/assets/images/AGU24_06-2e5e291911fb7893493a855712929db7.jpeg)![AGU24_07](https://docs.ciroh.org/assets/images/AGU24_07-dea1654f5692045854f57b78b3d712a6.jpg)![AGU24_03](https://docs.ciroh.org/assets/images/AGU24_03-3c9c69c1b5a3921c6538cfc4ae1592ce.gif)![AGU24_04](https://docs.ciroh.org/assets/images/AGU24_04-655be30f0b803f751457b66ecd6cd5d8.gif)\n\n- [**Presentations and Posters** 📊](#presentations-and-posters-)\n  - [Contribute Your Posters or Presentations!](#contribute-your-posters-or-presentations)\n- [Photo Gallery 📸](#photo-gallery-)"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Community NextGen Updates\nsource: https://docs.ciroh.org/blog/November%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\nThe Community NextGen framework has seen significant advancements in November 2024, with major updates across multiple components and exciting new resources for users. Let's dive into the key developments that are making hydrologic modeling more accessible and powerful than ever."
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 2,
    "content": "## 🎉 NGIAB-CloudInfra v1.3.0: A Major Milestone\n\nThe release of NGIAB-CloudInfra v1.3.0 marks a significant evolution in our Docker-based solution. This version introduces several groundbreaking improvements:\n\n- Integration with CIROH-UA's forked repositories of `ngen` and `t-route`\n- Full compatibility with the latest hydrofabric v2.2\n- New sample input data AWI-007\n- Enhanced visualization capabilities through the NGIAB Visualizer\n- Streamlined CI/CD pipeline with a unified Dockerfile\n- TEEHR integration for improved evaluation capabilities"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 3,
    "content": "## 📝 New End-to-End Workflow Guide\n\nWe're excited to announce a comprehensive new end-to-end workflow video tutorial for NGIAB! This resource walks users through the entire process, making it easier than ever to get started with NextGen modeling. The tutorial is available in our [documentation](https://docs.ciroh.org/training-NGIAB-101) and provides step-by-step guidance for both new and experienced users."
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 4,
    "content": "## 🔄 Infrastructure Updates"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 5,
    "content": "### NGIAB-HPCInfra\n\nThe Singularity-based solution for high-performance computing has been updated to maintain parity with the Docker implementation, including:\n\n- Updated repositories alignment\n- New sample data compatibility\n- Enhanced documentation"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 6,
    "content": "### Data Processing Tools\n\n- **NGIAB-data-preprocess v3.1.2**: Major update ensuring seamless compatibility with hydrofabric v2.2\n- **ngen-datastream**: Significant improvements with 22 new commits enhancing data handling capabilities"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 7,
    "content": "## 🌟 New Features"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 8,
    "content": "### Hydrofabric v2.2\n\nThe latest hydrofabric data model brings improved spatial representation and data organization. This update enhances the framework's ability to represent complex hydrologic systems accurately."
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 9,
    "content": "### Repository Updates\n\n- The NextGen Model Framework now operates from CIROH-UA's main branch\n- T-route implementation has been updated to use the specialized datastream branch\n- Both updates provide better integration and enhanced functionality within the NGIAB ecosystem"
  },
  {
    "idurl": 146,
    "idtype": "text",
    "order": 10,
    "content": "## What's Next?\n\nThese November updates represent our ongoing commitment to improving the NextGen framework's accessibility and capabilities. With the new workflow video and documentation in place, we're making it easier than ever for the hydrologic modeling community to leverage these powerful tools.\n\nWe encourage users to:\n\n1. Explore the new v1.3.0 release\n2. Watch the end-to-end workflow video\n3. Test out the updated data preprocess and NGIAB tools\n4. Provide feedback on your experience\n\nStay tuned for more updates as we continue to enhance and expand the NextGen In A Box ecosystem!\n\n* * *\n\nFor detailed information about these updates, visit our repositories:\n\n- [NGIAB-CloudInfra](https://github.com/CIROH-UA/NGIAB-CloudInfra)\n- [NGIAB-HPCInfra](https://github.com/CIROH-UA/NGIAB-HPCInfra)\n- [ngen-datastream](https://github.com/CIROH-UA/ngen-datastream)\n- [NGIAB-data-preprocess](https://github.com/CIROH-UA/NGIAB_data_preprocess)\n\n- [🎉 NGIAB-CloudInfra v1.3.0: A Major Milestone](#-ngiab-cloudinfra-v130-a-major-milestone)\n- [📝 New End-to-End Workflow Guide](#-new-end-to-end-workflow-guide)\n- [🔄 Infrastructure Updates](#-infrastructure-updates)\n  - [NGIAB-HPCInfra](#ngiab-hpcinfra)\n  - [Data Processing Tools](#data-processing-tools)\n- [🌟 New Features](#-new-features)\n  - [Hydrofabric v2.2](#hydrofabric-v22)\n  - [Repository Updates](#repository-updates)\n- [What's Next?](#whats-next)"
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Science Meeting 2024\nsource: https://docs.ciroh.org/blog/October%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\nThe 2024 CIROH Science Meeting was a huge success, bringing together researchers, federal partners, and consortium members both in person and virtually. We're excited to share the valuable resources from this year's meeting with the wider CIROH community.\n\nSlides and pictures from the various sessions, keynotes, and the Federal Town Hall have all been uploaded to a shared drive for easy access. You can find links to these materials here: [Access the Shared Drive with Presentation Slides](https://drive.google.com/drive/folders/1NsJEWHQBD92ndc3FD_bFEKgqTUqTxVM3)\n\nThank you to everyone who participated in making the 2024 CIROH Science Meeting a resounding success. We look forward to continuing our collaborative research efforts across the consortium."
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 2,
    "content": "## CIROH 2024 Science Meeting: Fostering Innovation in Hydrology\n\nThe third annual CIROH Science Meeting, held at The University of Alabama from October 14-17, 2024, brought together our vibrant community of researchers, practitioners, and stakeholders. This four-day event showcased the remarkable progress in our mission to advance national water prediction capabilities."
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 3,
    "content": "## Meeting Highlights\n\n- Over 300 participants gathered to share knowledge and forge new collaborations\n\n- Engaging discussions spanning critical topics in hydrology\n\n- Special focus on CIROH's four key research themes:\n  - Next Generation Water Prediction\n\n  - Community Modeling\n\n  - Hydroinformatics\n\n  - Social Science in Water Resources"
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 4,
    "content": "## Community Engagement\n\nThe meeting featured presentations from our consortium members, research partners, and key stakeholders including NOAA/NWS. The diverse agenda included keynotes, project presentations, roundtable discussions, poster sessions, and HIF tour that fostered meaningful discussions about the future of water prediction.\n\nRepresentatives from our member institutions and various partnering organizations contributed to rich dialogues about advancing the National Water Model and related technologies. The collaborative atmosphere exemplified CIROH's commitment to building a stronger hydroinformatics community."
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 5,
    "content": "## Key Outcomes\n\n- Strengthened partnerships across the consortium\n\n- Shared progress on ongoing research initiatives\n\n- Identified new opportunities for cross-institutional collaboration\n\n- Advanced discussions on improving national water prediction capabilities\n\n- Enhanced understanding of community needs and priorities"
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 6,
    "content": "## Looking Forward\n\nThe success of the 2024 Science Meeting reinforces CIROH's role in shaping the future of water prediction. The connections made and ideas shared during these four days will continue to influence our work throughout the year as we strive to improve water prediction capabilities for the nation."
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 7,
    "content": "## Photo Gallery\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/2024CSM_GroupPano.png)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/2024CSM_WomenOfCIROH.png)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/science-meeting2024-2.jpeg)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/science-meeting2024-3.jpeg)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/science-meeting2024-1.jpeg)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/science-meeting2024-4.jpeg)\n\n![CIROH 2024 Science Meeting](https://docs.ciroh.org/img/blog/2024-10-sci-meet/science-meeting2024-5.jpeg)"
  },
  {
    "idurl": 147,
    "idtype": "text",
    "order": 8,
    "content": "## Conclusion\n\nThe 2024 CIROH Science Meeting demonstrated the power of collaboration in advancing hydrology. As we move forward, the energy and insights from this gathering will fuel our continued efforts to enhance water prediction capabilities and serve our communities better.\n\n- [CIROH 2024 Science Meeting: Fostering Innovation in Hydrology](#ciroh-2024-science-meeting-fostering-innovation-in-hydrology)\n- [Meeting Highlights](#meeting-highlights)\n- [Community Engagement](#community-engagement)\n- [Key Outcomes](#key-outcomes)\n- [Looking Forward](#looking-forward)\n- [Photo Gallery](#photo-gallery)\n- [Conclusion](#conclusion)"
  },
  {
    "idurl": 148,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Accessing National Water Model (NWM) Data via Google Cloud BigQuery API\nsource: https://docs.ciroh.org/blog/September%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\n![gcp architectrure diagram](https://docs.ciroh.org/img/gcp_architecture_diagram.png)_Image Source: [https://github.com/BYU-Hydroinformatics/api-nwm-gcp](https://github.com/BYU-Hydroinformatics/api-nwm-gcp)_\n\nSeveral important historical and ongoing National Water Model (NWM) datasets are now available on Google Cloud BigQuery, which makes them queryable through SQL using Google Cloud console. Some of these data sets are also accessible through an API (e.g. using Python). These datasets and their current status are as follows:\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| Product | Cloud Console SQL | CIROH API | Historical | Daily Updates |\n| Medium-range forecasts | X | X | X | X |\n| Long-range forecasts | X | X | X | X |\n| Analysis and Assimilation | X | X | X | X |\n| Retrospective Data (NWM v3) | X |  | X |  |\n| Return Periods | X |  | X |  |\n\nTo gain access and run queries on BigQuery:\n\n- Submit the NWM BigQuery API Access Form, available [here.](https://docs.ciroh.org/docs/products/data-management/bigquery-api/)\n- Once approved, the CIROH IT team will cover the costs associated with your queries.\n- Initially, each organization or project may request a usage limit of up to $500 per month for accessing the data through BigQuery.\n\nThis service offers a convenient and efficient way to interact with NWM data for your research and analysis needs."
  },
  {
    "idurl": 148,
    "idtype": "text",
    "order": 2,
    "content": "## Public-Private Partnership: Advancing Water Resource Management\n\nThe National Water Model (NWM) BigQuery project exemplifies a successful collaboration between public and private sectors, uniting government-generated data with modern, cutting-edge cloud technology. This collaboration addresses several key aspects:\n\n- **Improved Data Access**: By leveraging Google Cloud BigQuery, a private sector platform, the project dramatically improves access to public NWM data. This partnership makes valuable water resource information more readily available to researchers, policymakers, and the public.\n- **Technological Innovation**: The integration of NWM data with BigQuery showcases how private sector technology can enhance the utility of public sector data. This synergy promotes innovation in data analysis and visualization techniques.\n- **Cost-Effective Solutions**: The CIROH DevOps team's commitment to covering query costs demonstrates how public funding can be strategically used to make private sector tools accessible to a wider audience, particularly in the academic and research communities.\n- **Capacity Building**: This initiative helps build capacity across sectors by providing researchers and organizations with powerful tools to analyze water resource data, potentially leading to better-informed decision-making in water management.\n- **Scalability and Efficiency**: By utilizing Google's cloud infrastructure, the project ensures that the growing volume of NWM data can be efficiently stored, accessed, and analyzed, addressing the scalability challenges often faced by public sector IT resources.\n- **Cross-Sector Collaboration**: This project fosters collaboration between government agencies, academic institutions, and private technology companies, creating a model for future partnerships in environmental and resource management.\n- **Open Science Promotion**: By making NWM data more accessible, this partnership supports the principles of open science, encouraging transparency and reproducibility in water resource research.\n\nThis public-private partnership not only enhances the value of the National Water Model but also sets a precedent for future collaborations that can drive innovation in environmental data management and analysis.\n\n- [Public-Private Partnership: Advancing Water Resource Management](#public-private-partnership-advancing-water-resource-management)"
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Cloud User Success Story\nsource: https://docs.ciroh.org/blog/August%20Monthly%20Blog%20Update\nscraped_date: 2025-01-31\n---\n\nThis month, we are excited to showcase two case studies that utilized our cyberinfrastructure tools and services. These case studies demonstrate how CIROH's cyberinfrastructure is being utilized to support hydrological research and operational advancements."
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 2,
    "content": "## 1. ngen-datastream and NGIAB\n\n![ngen-datastream image](https://docs.ciroh.org/img/blog/2024-08-case-studies/ngen-datastream-august-blog.jpg)"
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 3,
    "content": "### Overview:\n\nCIROH's cloud computing resources have allowed for the development of ngen-datastream, which automates the process of collecting and formatting input data for NextGen, orchestrating the NextGen run through NextGen In a Box (NGIAB), and handling outputs. This software allows users to run NextGen in an efficient, relatively painless, and reproducible fashion, increasing community access to the NextGen framework. ngen-datastream is already community accessible ( [https://github.com/CIROH-UA/ngen-datastream/tree/main](https://github.com/CIROH-UA/ngen-datastream/tree/main)) and making an impact on research. A major component of this software is the Amazon Web Services (AWS) cloud-based research datastream ( [https://github.com/CIROH-UA/ngen-datastream/tree/main/research_datastream](https://github.com/CIROH-UA/ngen-datastream/tree/main/research_datastream)). The research datastream is a CONUS-wide recurring NextGen simulation configured by the community. The terraform to build the AWS infrastructure exists in the ngen-datastream repository and current development focuses on CI/CD and enabling community contribution to the research datastream via edits to the NextGen configuration. Ultimately, these tools help distribute access throughout the community to cutting edge hydrologic research, maximizing the pace of progress of research to operations in hydrology."
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 4,
    "content": "### Contribution to CIROH:\n\n- **Automation**: It automates the process of collecting, formatting, and validating input data for NextGen, streamlining model preparation.\n- **Flexibility**: It allows users to provide their own input files to run NextGen.\n- **Scalable Infrastructure**: It utilizes AWS state machine to provide access to high-performance computing (HPC) resources."
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 5,
    "content": "### Infrastructure Utilized:\n\n- **Elastic Compute Cloud (EC2)**\n- **Simple Storage Service (S3)**\n- **AWS Lamda and Step Functions**"
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 6,
    "content": "## 2. TEEHR\n\n- **PI** : Katie van Wekhoven\n- **Co-PI** : Matt Denno (Development Lead)\n- **Developer** : Sam Lamont"
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 7,
    "content": "### Project Overview:\n\nThe goal of this project is to investigate, design, and build a prototype hydrologic model/forecast evaluation system (TEEHR) that will significantly improve our ability to evaluate continental-scale datasets and will provide a robust and consistent evaluation tool for CIROH and OWP research. Design priorities include easy integration into common research workflows, rapid execution of large-scale evaluations, simplified exploration of performance trends and drivers, inclusion of common and emergent evaluation methods, efficient data structures, open-source and community development, and easy extensibility.\n\n![teehr image](https://docs.ciroh.org/img/blog/2024-08-case-studies/teehr-image.png)"
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 8,
    "content": "### Contribution to CIROH:\n\n- **TEEHR-HUB**: It is a JupyterHub environment, running the TEEHR image, with AWS services (EFS and S3) to provide a scalable platform for hydrologic research.\n- **Data Processing**: TEEHR-HUB has successfully processed the AORC (v3.0 retrospective) gridded precipitation data to the MERIT basins, as well as the CONUS 40-year retrospective (v3.0 and USGS).\n- **Testbed Integration**: TEEHR-HUB's compatibility with various testbeds allows researchers to experiment with different hydrologic models and datasets.\n- **Evaluation** - TEEHR is being used (or is planned for use) by several CIROH research teams to evaluate large scale model results."
  },
  {
    "idurl": 149,
    "idtype": "text",
    "order": 9,
    "content": "### Infrastructure Utilized:\n\n- **Elastic Kubernetes Service (EKS)** (including supporting AWS services) - Scalable computing resources to host JupyterHub Dask and Spark\n- **Elastic File System (EFS)** - Shared data drive for cached data and shared documents (notebooks, etc.)\n- **Simple Storage Service (S3)** - Bucket storage for large public and private datasets\n\n- [1. ngen-datastream and NGIAB](#1-ngen-datastream-and-ngiab)\n  - [Overview:](#overview)\n  - [Contribution to CIROH:](#contribution-to-ciroh)\n  - [Infrastructure Utilized:](#infrastructure-utilized)\n- [2. TEEHR](#2-teehr)\n  - [Project Overview:](#project-overview)\n  - [Contribution to CIROH:](#contribution-to-ciroh-1)\n  - [Infrastructure Utilized:](#infrastructure-utilized-1)"
  },
  {
    "idurl": 150,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: CIROH Research CyberInfrastructure Update\nsource: https://docs.ciroh.org/blog/July%20Monthly%20IT%20Update\nscraped_date: 2025-01-31\n---\n\nWe're excited to share some recent developments and updates from CIROH's Research CyberInfrastructure team:"
  },
  {
    "idurl": 150,
    "idtype": "text",
    "order": 2,
    "content": "### Cloud Infrastructure\n\n- CIROH's Google Cloud Account is now fully operational and managed by our team. You can find more information [here.](https://docs.ciroh.org/docs/services/cloudservices/google-cloud/)\n- We're in the process of migrating our 2i2c JupyterHub to CIROH's Google Cloud account.\n- We've successfully deployed the Google BigQuery API (developed by BYU and Google) for NWM data in our cloud. To access this API, please contact us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu). Please refer to [NWM BigQuery API](https://docs.ciroh.org/docs/products/data-management/bigqeury-api/) to learn more."
  },
  {
    "idurl": 150,
    "idtype": "text",
    "order": 3,
    "content": "### Support and Services\n\n- Monthly AWS office hours are ongoing. For more details on how to join, email us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu).\n- We provided IT support for the Summer Institute 2024, REU students, and team leads this summer.\n\n![summer-institute-students](https://docs.ciroh.org/img/blog/2024-07-it/summer-institute.jpg)\n\n![reu-institute-students](https://docs.ciroh.org/img/blog/2024-07-it/reu.jpg)"
  },
  {
    "idurl": 150,
    "idtype": "text",
    "order": 4,
    "content": "### Security Enhancements\n\n- New security features have been added to our CIROH-UA GitHub repository to prevent commits containing sensitive information.\n- We've updated our AWS best practices, particularly regarding key management. If your project uses CIROH AWS resources, please review these updates at [AWS Best Practices](https://docs.ciroh.org/docs/services/cloudservices/aws/AWS%20best%20Practices/)."
  },
  {
    "idurl": 150,
    "idtype": "text",
    "order": 5,
    "content": "### Resources and Access\n\n- For external IT resources needed for your projects, check out NSF Access Allocations [here.](https://docs.ciroh.org/docs/services/external-resources/nsf-access/)\n- GPU allocation is now available on CIROH's 2i2c JupyterHub. To request access, please fill out [this form.](https://forms.office.com/pages/responsepage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNlpMNTIwT0ZHUjg5VzdTNzNOSVk5OEFNMC4u)\n\n![summer-institute-students](https://docs.ciroh.org/img/blog/2024-07-it/si.jpg)\n\nFor more information on our services, please refer to our [services page](https://docs.ciroh.org/docs/services/intro).\n\nWe're continually working to improve our IT infrastructure and support. If you have any questions or need assistance, don't hesitate to reach out to us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu).\n\n- [Cloud Infrastructure](#cloud-infrastructure)\n- [Support and Services](#support-and-services)\n- [Security Enhancements](#security-enhancements)\n- [Resources and Access](#resources-and-access)"
  },
  {
    "idurl": 152,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: AWRA 2024 Spring Conference\nsource: https://docs.ciroh.org/blog/AWRA%202024%20Spring%20Conference\nscraped_date: 2025-01-31\n---\n\n**AWRA 2024 Spring Conference**\n\nThe **CIROH CyberInfrastructure team** recently participated in the **AWRA 2024 Spring Conference**, co-hosted by **the Alabama Water Institute at the University of Alabama.**\n\nThemed \"Water Risk and Resilience: Research and Sustainable Solutions,\" the conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field.\n\nCIROH CyberInfrastructure team presented on these topics:\n\n- **Accelerating Community Contribution to the Next Generation Water Resources Modeling Framework**\n- **Creating a community dataset for high-speed national water model data access**\n- **Model structure selection for the flood and drought predictions using the NextGen Framework based on the extreme event simulations**\n\nCIROH team member **James Halgren** presented the work on **\"Accelerating Community Contribution to the Next Generation Water Resources Modeling Framework.\" The presentation focused on building and sharing a continuous research data stream using the NextGen Water Resources Modeling Framework with NextGen IN A Box (NGIAB).** This project, a collaboration with Lynker members, showcases the potential for open-source tools and community-driven efforts to advance water resources modeling and research.\n\n![AWRA2024Spring](https://docs.ciroh.org/img/blog/2024-04-AWRA/awra2024.jpeg)\n\nCIROH team member **Sepehr Karimi** presented the work on **\"Creating a community dataset for high-speed national water model data access\"**\n\n![AWRA2024Spring-2](https://docs.ciroh.org/img/blog/2024-04-AWRA/AWRA2024-2.png)\n\nCIROH team member **Shahab Alam** presented the work on **\"Model structure selection for the flood and drought predictions using the NextGen Framework based on the extreme event simulations\"**\n\n![AWRA2024Spring-3](https://docs.ciroh.org/img/blog/2024-04-AWRA/AWRA2024-3.png)\n\nThese presentations showcased CIROH's expertise in open-source tools, community-driven efforts, and water resources modeling. The team's contributions sparked insightful discussions and potential collaborations for future projects.\n\n**Call to Action:**\n\nTo learn more about CIROH's work or connect with the team, visit our website at [CIROH-website](https://ciroh.ua.edu/).\n\n**Conference Website:**\n\n[AWRA 2024 Spring Conference website link](https://www.awra.org/Members/Events_and_Education/Events/2024-Spring-Conference/2024_Spring_Conference.aspx)"
  },
  {
    "idurl": 153,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Google Cloud Next '24: A Flood of Innovation and Inspiration\nsource: https://docs.ciroh.org/blog/Google%20Cloud%20Next%202024\nscraped_date: 2025-01-31\n---\n\n**Google Cloud Next '24**\n\nHello everyone, and thanks for stopping by!\n\nI recently had the incredible opportunity to attend Google Cloud Next 2024 in person for the first time, and it was truly an **amazing experience**. From **insightful keynote presentations and workshops to vibrant booths** buzzing with connections, the event was a whirlwind of innovation and inspiration.\n\nOne of the highlights was undoubtedly the abundance of AI announcements and advancements. Google continues to push the boundaries of what's possible, and it was exciting to witness the future of technology unfold.\n\nAmong the many highlights, **CIROH** achieved a significant milestone with its **first-ever session at Google Cloud Next**. The presentation, titled **\"Channel the Flood Data Deluge: Unlocking the American National Water Model,\"** [link](https://cloud.withgoogle.com/next/session-library?session=DEV209&utm_source=copylink&utm_medium=unpaidsoc&utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&utm_content=next-homepage-social-share&utm_term=-#all%3E) led by **Kel Markert (Google), Dr. Dan Ames (BYU), and Michael Ames(SADA)** was a resounding success. The session [link](https://assets.swoogo.com/uploads/3771985-6615e3b5da2cf.pdf) shed light on the immense potential of the National Water Model and its ability to revolutionize water resource management.\n\n![GoogleCloudNext2024](https://docs.ciroh.org/img/blog/2024-04-gcn/GoogleCloudNext2024.png)\n\nThe conference was a truly enjoyable experience, especially collaborating with Dan, Kel, Michael and others. We had a great time together and sharing our insights.\n\n![GoogleNext-1](https://docs.ciroh.org/img/blog/2024-04-gcn/googlenext-1.jpeg)\n\n![GoogleNext-2](https://docs.ciroh.org/img/blog/2024-04-gcn/googlenext-2.jpeg)\n\n![GoogleNext-3](https://docs.ciroh.org/img/blog/2024-04-gcn/googlenext-3.jpeg)\n\nThe **energy and enthusiasm** throughout the event were contagious, and I left feeling incredibly **motivated and inspired**. I connected with numerous individuals from diverse backgrounds, fostering **new collaborations** and sparking exciting ideas for the future of water research and technology.\n\nIf you're curious to see more about my Google Cloud Next experience, head over to **my LinkedIn post [link](https://www.linkedin.com/in/arpita0911patel/)** where I've shared pictures from all three days.\n\nThank you for reading and stay tuned for more updates on the exciting advancements in water research and technology!\n\n**Want to delve deeper into the insights and announcements from Google Cloud Next?** Check out these valuable resources:\n\n**SADA Live:** Recap Key Cloud Technology Insights from Google Cloud Next '24: [link](https://www.linkedin.com/events/sadalive-recapkeycloudtechnolog7181342146724073472/)\nThis LinkedIn event offers a comprehensive overview of the key takeaways and technological advancements unveiled at the conference.\n\n**Day 2 Google Blog Recap:** Dive into the specifics of Day 2 at Google Cloud Next with this insightful blog post, covering topics ranging from AI and data analytics to infrastructure and security. [link](https://cloud.google.com/blog/topics/google-cloud-next/next24-day-2-recap?utm_source=linkedin&utm_medium=unpaidsoc&utm_campaign=fy24q1-googlecloud-blog-ai-in_feed-no-brand-global&utm_content=-&utm_term=-)\n\n**AI Takes Center Stage:**\n\n- **Gemini for Google Cloud:** The introduction of Gemini 1.5 Pro, integrated with various Google Cloud services, promises enhanced functionality, security, and AI performance across diverse applications.\n- **AI Infrastructure Advancements:** The AI Hypercomputer provides exceptional computational power for complex AI tasks, while Gemini API now offers models tailored for various scales, enriching the development environment.\n- **Vertex AI Enhancements:** New tools for low-latency applications and improved Gemini integration empower developers to build more efficient and sophisticated AI-driven applications.\n- **Secure AI Framework (SAIF):** Establishes rigorous security standards for AI implementations, ensuring secure and responsible AI integrations.\n- **AI Database Assistant:** Leverages Gemini to simplify complex queries and deepen AI integration into database management.\n- **Google Vids:** This innovative Workspace feature utilizes Gemini and Vertex AI to enhance digital storytelling and collaboration, revolutionizing workplace communication.\n\n**Infrastructure and Development:**\n\n- **Google Axion Processor:** This cutting-edge processor boasts significant performance and energy efficiency improvements compared to traditional x86 instances, setting a new standard for computational efficiency.\n[link](https://lnkd.in/g544-Tdt)\n\n- **Google Distributed Cloud (GDC) Sandbox:** Enables developers to build and test services for GDC within a Google Cloud environment, simplifying the development process.\n[link](https://lnkd.in/gcDBFt4t)\n\n- **Migrate to Containers (M2C) CLI:** This new tool facilitates seamless migration of applications to containers, supporting deployment on GKE or Cloud Run.\n[link](https://lnkd.in/gz_AmHCY)\n\n\n**Security and Data Analytics:**\n\n- **AI Cyber Defense Initiative:** Revolutionizes cybersecurity by leveraging AI for innovative solutions against cyber threats.\n- **BigQuery as a Unified Platform:** Transforms BigQuery into a comprehensive platform for managing multimodal data and executing AI tasks, seamlessly integrated with Gemini.\n\nCheck out all the announcements: [link](https://lnkd.in/gNGYrg44)"
  },
  {
    "idurl": 154,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Monthly News Update - March 2024\nsource: https://docs.ciroh.org/blog/Community%20NextGen%20Updates%20March%202024\nscraped_date: 2025-01-31\n---\n\n**Accelerating Innovation: CIROH's March 2024 Update**\n\nThe CIROH team has been diligently accelerating research cyberinfrastructure capabilities this month. We're thrilled to share key milestones achieved in enhancing the Community NextGen project and our cloud/on-premises platforms.\n\nA significant highlight was the successful launch of our new fully operational on-premises infrastructure. Comprehensive documentation is now available [here](https://docs.ciroh.org/docs/services/on-prem/), ensuring seamless access and utilization. Additionally, we've fortified the NextGen in a Box (NGIAB) ecosystem with bug fixes, repository enhancements, and initiated work on automating the CI pipeline for the [Singularity Repo](https://github.com/CIROH-UA/Ngen-Singularity)\n\nEmpowering our community remains a top priority. We've expanded the DocuHub knowledge base with dedicated sections on on-premises access guidelines, as well as policies and best practices for optimized infrastructure usage [here](https://docs.ciroh.org/docs/policies/intro) . Furthermore, our team represented CIROH at the AWRA Geospatial Water Technology Conference in Orlando, sharing insights on leveraging geospatial data for water research. Refer [here](https://github.com/CIROH-UA/Conferences/tree/main/2024-03-26_AWRA_GeospatialWaterTechnology)\n\nAs we continue driving advancements, we extend our gratitude for your unwavering support of the Community NextGen project and CIROH's cyberinfrastructure endeavors. Be on the lookout for more exciting updates next month as we strive to unlock new frontiers in water science through robust computing capabilities.\n\n[Click Here to Visit Community NextGen and NGIAB News from March 2024](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 155,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: Monthly News Update - February 2024\nsource: https://docs.ciroh.org/blog/Community%20NextGen%20Updates%20Feb%202024\nscraped_date: 2025-01-31\n---\n\nWelcome to the February edition of the CIROH DocuHub blog, where we bring you the latest updates and news about the Community NextGen project and CIROH's Cloud and on-premise Infrastructure.\n\nOur team has been hard at work enhancing CIROH's Infrastructure and Community NextGen tools. Here are some highlights from February 2024:\n\n1. We successfully launched our new On-premises Infrastructure, which is now fully operational. You can find documentation for it [here](https://docs.ciroh.org/docs/services/on-prem/).\n\n2. For NGIAB, we've made improvements to the CI pipeline for pull requests submitted with forked repositories. Now, we automatically build and test these submissions using the CI pipeline.\n\n3. We've added documentation for the NWMURL python package, which offers utility functions tailored for accessing National Water Model (NWM) data URLs. This library streamlines the process of accessing NWM data for various purposes, including analysis, modeling, and visualization. You can explore the documentation [here](https://docs.ciroh.org/docs/products/data-management/dataaccess/).\n\n4. We're thrilled to announce the NextGen Track for DevCon24. The schedule is now available at: [DevCon24 Schedule](https://ciroh.ua.edu/devconference/).\n\n\nThank you for your ongoing interest and support in the Community NextGen project. Stay tuned for more exciting updates and developments next month. 😊\n\n[Click Here to Visit Community NextGen and NGIAB News from Feb 2024](https://docs.ciroh.org/news/)"
  },
  {
    "idurl": 156,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen Monthly News Update - January 2024\nsource: https://docs.ciroh.org/blog/NextGen%20Updates%20Jan%202024\nscraped_date: 2025-01-31\n---\n\nWelcome to the January edition of the CIROH DocuHub blog, where we share the latest updates and news about the Community NextGen project monthly. NextGen is a cutting-edge hydrologic modeling framework that aims to advance the science and practice of hydrology and water resources management. In this month's blog, we will highlight some of the recent achievements and developments of the Community NextGen team.\n\nFirst, we are excited to announce that NextGen In A Box (NGIAB) is now available with Singularity support. This means that you can run NGIAB on any HPC system that does not support Docker, using Singularity containers. Singularity is a popular tool for creating and running portable and reproducible computational environments. To learn how to use NGIAB with Singularity, please visit our GitHub repository: [Ngen-Singularity](https://github.com/CIROH-UA/Ngen-Singularity).\n\nSecond, we have made several improvements and enhancements to NGIAB, such as updating the sample input data, upgrading the Boost library, adding auto mode run, and supporting geopackage format. You can find more details about these updates on our GitHub repository: [NGIAB-CloudInfra](https://github.com/CIROH-UA/NGIAB-CloudInfra).\n\nThird, we would like to share with you is the development of NextGen Datastream, a tool that automates the process of collecting and formatting input data for NextGen, orchestrating the NextGen run through NextGen In a Box (NGIAB), and handling outputs. The NextGen Datastream is a shell script that orchestrates each step in the process, using a configuration file that specifies the data sources, parameters, and options for the NextGen run. The NextGen Datastream can also generate its own internal configs and modify the configuration file as needed. You can find more details and instructions on how to use the NextGen Datastream on our GitHub repository: [ngen-datastream](https://github.com/CIROH-UA/ngen-datastream/blob/main/README.md).\n\nWe hope you enjoyed this blog and found it informative and useful. If you have any questions, comments, or feedback, please feel free to contact us at [ciroh-it-admin@ua.edu](mailto:ciroh-it-admin@ua.edu). Thank you for your interest and support in the Community NextGen project. Stay tuned for more exciting news and developments in the next month. 😊\n\n[Visit NGIAB News](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 157,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen Monthly News Update - December 2023\nsource: https://docs.ciroh.org/blog/NextGen%20Updates%20Dec%202023\nscraped_date: 2025-01-31\n---\n\nHappy New Year!!! We are back from SFO after attending AGU last month. We are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings, and Community Support from December 2023.\n\n[Visit NGIAB News](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 158,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen In A Box v1.1.0 Release\nsource: https://docs.ciroh.org/blog/NextGen%20In%20A%20Box%20Release%20Notes\nscraped_date: 2025-01-31\n---\n\nWe've released NGIAB v1.1.0! This release fixes issues:\n\n- [#21](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/21)\n- [#67](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/67)\n- [#44](https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/44)\n\nMore info: [https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0](https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0)\n\n[Visit NGIAB News](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 159,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen Monthly News Update - November 2023\nsource: https://docs.ciroh.org/blog/NextGen%20Updates%20Nov%202023\nscraped_date: 2025-01-31\n---\n\nWe are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings and Community Support.\n\n[Visit NGIAB News](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 160,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen In A Box Updates\nsource: https://docs.ciroh.org/blog/NextGen-In-A-Box%20Release%20Notes\nscraped_date: 2025-01-31\n---\n\nWe've introduced a fresh addition within DocuHub, offering the most up-to-date insights on NGIAB and NextGen monthly updates.\n\n[Visit NGIAB Release Notes Page](https://docs.ciroh.org/news)"
  },
  {
    "idurl": 161,
    "idtype": "text",
    "order": 1,
    "content": "---\ntitle: NextGen Framework Forcings\nsource: https://docs.ciroh.org/blog/NextGen%20Forcings\nscraped_date: 2025-01-31\n---"
  },
  {
    "idurl": 161,
    "idtype": "text",
    "order": 2,
    "content": "## NextGen Framework Forcings\n\nA new [forcing processor](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor) tool has been made public. This tool converts any National Water Model based forcing files into ngen forcing files. This process can be an intensive operation in compute, memory, and IO, so this tool facilitates generating ngen input and ultimately makes running ngen more accessible."
  },
  {
    "idurl": 161,
    "idtype": "text",
    "order": 3,
    "content": "### Read more\n\n[Visit Github](https://github.com/CIROH-UA/ngen-datastream/tree/main/forcingprocessor)\n\n- [NextGen Framework Forcings](#nextgen-framework-forcings)\n  - [Read more](#read-more)"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 1,
    "content": "# NGIAB 101: Summary and Setup\n\nThis is a new lesson built with [The Carpentries Workbench](https://carpentries.github.io/sandpaper-docs).\n\nWelcome to the NextGen In A Box (NGIAB) 101 training module!"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 2,
    "content": "## Source Code and Documentation\n\nThe source code for NextGen In A Box (NGIAB) is found at [our GitHub repository](https://github.com/CIROH-UA/NGIAB-CloudInfra). The documentation for NGIAB and its extensions are found on the [CIROH docs](https://docs.ciroh.org/docs/products/Community%20Hydrologic%20Modeling%20Framework/).\n\nTo dive right into NGIAB as quickly as possible, follow the quick start guide on our [GitHub repository](https://github.com/CIROH-UA/NGIAB-CloudInfra), or head straight to our second episode, Installation. If you are interested in taking your time and learning in-depth about the features of NGIAB, follow the episodes in this module.\n\nFor all users, the [Key Points summary page](https://docs.ciroh.org/training-NGIAB-101/key-points.html) and the [Glossary](https://docs.ciroh.org/training-NGIAB-101/reference.html) may be useful references. The [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html) is optional, but may contain useful content depending on your specific use case.\n\nNote: This module requires you to use Bash scripting in your command line and Git. All commands will be given in this module. However, if you are interested in learning more, you can refer to this [quick Bash tutorial by Ubuntu](https://ubuntu.com/tutorials/command-line-for-beginners#1-overview), [this basic Git tutorial in the Git documentation](https://git-scm.com/docs/gittutorial), and this [Git workflow tutorial by CIROH](https://github.com/AlabamaWaterInstitute/data_access_examples/blob/main/doc/GIT_USAGE.md)."
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 3,
    "content": "## DevCon 2025 NGIAB Workshop Attendees\n\nAre you joining us for the DevCon 2025 NGIAB workshop? We have prepared two DevCon-specific documents for you.\n\n- [DevCon 2025 Jetstream Instructions](https://docs.ciroh.org/training-NGIAB-101/devcon25js.html) will walk you through the specific steps we will take in this hands-on workshop.\n- [DevCon 2025 Troubleshooting](https://docs.ciroh.org/training-NGIAB-101/troubleshooting.html) contains useful reference material if you are stuck during the workshop.\n\n**These documents are key, since we will not be following the default episodes exactly as written.**\n\nAfter the workshop, feel free to explore the rest of this module to learn more about NGIAB!"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 4,
    "content": "## HPC Users\n\nAre you planning on running NGIAB on an HPC? You can follow the episodes in order, but replace Episode 2: Installation and Setup with the HPC-specific instructions in the [Advanced Topics module](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html)."
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 5,
    "content": "## System Requirements\n\nThe Installation episode will walk you through the steps to install Windows Subsystem for Linux (WSL), Docker, NGIAB, and retrieve sample data sets. This page summarizes system requirements."
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 6,
    "content": "### Windows\n- **Software:** Docker, Git, WSL, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 7,
    "content": "### MacOS\n- **Software:** Docker, Git, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 8,
    "content": "### Linux\n- **Software:** Docker, Git, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 9,
    "content": "## Episodes\n\n1. [Introduction](https://docs.ciroh.org/training-NGIAB-101/introduction.html)\n2. [Installation and Setup](https://docs.ciroh.org/training-NGIAB-101/installation.html)\n3. [Data Preparation](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html)\n4. [Model Execution](https://docs.ciroh.org/training-NGIAB-101/model-execution.html)\n5. [Evaluation](https://docs.ciroh.org/training-NGIAB-101/evaluation.html)\n6. [Visualization](https://docs.ciroh.org/training-NGIAB-101/visualization.html)\n7. [Calibration](https://docs.ciroh.org/training-NGIAB-101/calibration.html)\n8. [Advanced Topics](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html)"
  },
  {
    "idurl": 162,
    "idtype": "text",
    "order": 10,
    "content": "## Resources\n\n- [Key Points](https://docs.ciroh.org/training-NGIAB-101/key-points.html)\n- [Glossary](https://docs.ciroh.org/training-NGIAB-101/reference.html#glossary)\n- [Learner Profiles](https://docs.ciroh.org/training-NGIAB-101/profiles.html)\n- [DevCon 2025 Troubleshooting](https://docs.ciroh.org/training-NGIAB-101/troubleshooting.html)\n- [DevCon2025 Jetstream VM Instructions](https://docs.ciroh.org/training-NGIAB-101/devcon25js.html)"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 1,
    "content": "# Introduction\n\nLast updated on 2025-05-06"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- What is the NextGen Framework?\n- What is NextGen in a Box (NGIAB)?\n- What is containerization?\n- Why should I use NGIAB?"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Identify key components of the NGIAB architecture\n- Describe NGIAB's role in the NextGen Framework\n- Determine use cases for NGIAB"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 5,
    "content": "## Introduction to NextGen\n\nThe U.S. National Water Model (NWM) provides hydrologic predictions for over 2.7 million river reaches across the United States ([Cosgrove et al., 2024](https://doi.org/10.1111/1752-1688.13184)). **The Next Generation Water Resources Modeling Framework (NextGen) is an advancement of the NWM**, setting the stage for a more flexible modeling approach. NextGen promotes model interoperability and standardizes data workflows, allowing the integration of various hydrologic models tailored to specific regional processes, providing key flexibility needed for future success with continental-scale modeling. The NextGen framework continues to undergo testing, improvements, and updates through research efforts at the NOAA Cooperative Institute for Research to Operations in Hydrology (CIROH)."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 6,
    "content": "## What is NGIAB?\n\nManaging NextGen's complex software ecosystem remains challenging. The NextGen framework's implementation requires handling numerous software libraries and dependencies. To streamline this, we developed NextGen In A Box (NGIAB)—an open-source, containerized solution that encapsulates the NextGen framework and essential modeling components into a self-contained, reproducible application. By eliminating manual configuration burdens, NGIAB enables researchers to focus on scientific inquiry rather than software setup and maintenance. Beyond simplifying deployment of the NextGen Framework, NGIAB fosters collaboration among researchers, academic institutions, and government agencies by providing a scalable, community-driven modeling environment. **In essence, NGIAB provides a unified solution that powers NextGen models, including future versions of the NWM starting with version 4.**"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 7,
    "content": "### Terminology\n- NextGen: overarching framework\n- NGIAB: containerized packaging of NextGen\n- `ngen`: the engine used within NextGen\n- NWM: specific operational implementation used by the National Weather Service"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 8,
    "content": "## Containerization\n\n- Containerization addresses compatibility issues and hardware variation challenges by **encapsulating applications, their dependencies, and runtime environments into a single, portable unit**.\n- Think of containerization like putting a model and all its tools into a sealed toolbox – you can carry and run it anywhere, and everything needed is inside.\n- This ensures consistent execution across diverse computing environments, regardless of differences in hardware or software configurations.\n- NGIAB leverages Docker ([Boettiger, 2015](https://doi.org/10.1145/2723872.2723882)) and Singularity ([Hunt et al., 2005](https://www.researchgate.net/publication/236160050_An_Overview_of_the_Singularity_Project)) to streamline deployment."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 9,
    "content": "## Architectural Components\n\nNGIAB is designed as a multi-layered containerized tool that encapsulates the NextGen framework and many components relevant to the NWM within a reproducible environment.\n\n![NGIAB Containerization Architecture](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-1.png)\n\nFigure 1: Architecture of the NGIAB, highlighting its core modeling foundation, CI/CD pipelines, containerized tools and supporting technologies.\n\nFigure 1 illustrates the layered architecture of NGIAB.\n\n- **Layer 1:** At its core (Layer 1) lies a suite of integrated hydrological modeling components and hydrofabric (a geospatial dataset representing hydrologic features like rivers, basins, and connections), designed to work together within the NextGen framework. Hydrologic models in NGIAB are Basic Model Interface (BMI) compliant, meaning that they follow a standard structure and can be swapped in and out for one another.\n- **Layer 2:** Layer 1 is wrapped by the Continuous Integration/Continuous Deployment (CI/CD) Pipeline layer (Layer 2). CI/CD are tools and practices that automate code testing and updates. NGIAB leverages GitHub Actions to ensure automated testing, integration, and deployment capabilities for reproducible workflows.\n- **Layer 3:** The NGIAB Containerization layer (Layer 3) provides the containerized environment and essential configuration tools.\n- **Layer 4:** The outermost layer (Layer 4), Technologies & Methods, provides broader infrastructure, best practices, and support for deployment across different computing environments (local, cloud, HPC), and facilitates community engagement and contribution.\n\nThe architecture emphasizes four key aspects:\n- core hydrological modeling framework capabilities,\n- simplified access to modeling tools,\n- facilitation of rapid development and reliability,\n- and integration of supportive tools and practices."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 10,
    "content": "## Extensions of NGIAB\n\nSeveral extensions of NGIAB are already integrated with NextGen-related tools like [Data Preprocess](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html), [Tools for Exploratory Evaluation in Hydrologic Research (TEEHR)](https://docs.ciroh.org/training-NGIAB-101/evaluation.html), and [Data Visualizer](https://docs.ciroh.org/training-NGIAB-101/visualization.html) (Figure 2). These extensions will be discussed in later episodes.\n\n![NGIAB model execution process](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-2.png)\n\nFigure 2: Workflow of data acquisition, model execution, evaluation, and results visualization."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 11,
    "content": "## Example Applications\n\nSteps common to all hydrologic modeling frameworks include data collection and preparation, framework setup and model execution, evaluation, results visualization, and calibration. Researchers can use NGIAB to execute model runs for their basins of interest. _Note that calibration is not yet an integrated capability within NGIAB._ Figures 3 and 4 show examples of how NGIAB and its extensions have been used to simulate streamflow for five years in the Provo River basin.\n\n![Provo River network and basin boundaries](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-4.png)\n\nFigure 3: Map showing the drainage basin used as our demonstration case, the Provo River near Woodland, UT (Gage-10154200). This view shows the NGIAB interactive preprocessing tool. The highlighted region (light orange area; downstream-most basin in pink) represents the specific study basin, illustrating the river network (blue lines), sub-basins (orange), and surrounding USGS gaging stations (black dots).\n\n![NextGen in a Box Visualizer web interface](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-5.png)\n\nFigure 4: Map showing the geospatial visualization using the Data Visualizer for a selected outlet point as well as displaying a time series plot between observed (labeled \"USGS\"; blue line) and simulated (labeled \"ngen\"; orange line) with the performance metrics (Kling-Gupta Efficiency (KGE), Nash-Sutcliffe Efficiency (NSE), and relative bias). These metrics assess how closely simulated results match observed data. The Visualizer can also show the performance of the NWM 3.0 compared to the observed time series."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 12,
    "content": "## Why should I use NGIAB?\n\nNGIAB makes **community contribution** possible in research settings by simplifying setup and providing demos, allowing hydrologists and researchers to configure and modify localized water models. Built on open-source code and the `ngen`/BMI foundation, NGIAB allows integration of a hydrology process model into a larger hydrologic simulation framework, allowing a researcher to focus on their area of specific modeling expertise. Its lightweight container size also empowers hydrologists to execute large-scale runs efficiently and reduce computational bottlenecks. By strengthening collaboration across research teams, NGIAB will help drive the evolution of community-scale water modeling and accelerate the transition from academic innovation to real-world operational use."
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 13,
    "content": "## Your Turn\n\nHere are some self-assessment questions for discussion or consideration:\n- Do I understand how NGIAB fits into the NextGen Framework?\n- What are the key design features and extensions of NGIAB?\n- How can I use NGIAB to answer my research questions?\n- How can I use NGIAB to contribute my expertise to the NextGen Framework?"
  },
  {
    "idurl": 163,
    "idtype": "text",
    "order": 14,
    "content": "## Key Points\n\n- The Next Generation Water Resources Modeling Framework (NextGen) advances the National Water Model with flexible, modular, and regionally adaptive hydrologic modeling at national scale.\n- NextGen In A Box (NGIAB) packages the complex NextGen system into an open-source, containerized application for easier access and usability.\n- NGIAB uses Docker and Singularity for portability across local machines, cloud platforms, and HPC systems.\n- NGIAB's multi-layered architecture integrates hydrologic modeling tools, CI/CD pipelines, and supportive technologies and is complemented by a suite of extensions that allow for end-to-end workflows from data acquisition to visualization and evaluation.\n- NGIAB fosters an open ecosystem where researchers, developers, and practitioners actively contribute new models, extensions, and workflows."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 1,
    "content": "# Installation and Setup\n\nLast updated on 2025-05-23\n\nThis episode can be a standalone tutorial for those who want a quick introduction to NGIAB. This tutorial follows the case study from our [CloudInfra repository](https://github.com/CIROH-UA/NGIAB-CloudInfra). Users who wish to learn more about NGIAB can explore our other episodes in this module."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I install and set up NGIAB?\n- What are the prerequisites for running NGIAB?\n- How do I verify my installation?"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Install and verify Docker\n- Set up NGIAB project directories\n- Run a sample NGIAB run"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 5,
    "content": "## Introduction\n\nThis lesson guides you through installing and setting up NGIAB, a containerized solution designed to simplify running the NextGen modeling framework locally. NGIAB leverages Docker containers to ensure consistent and reproducible runs.\n\n> **Note**: Are you using an HPC? Instead of following these instructions, follow the guidance in the HPC sections in [Advanced Topics](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html)."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 6,
    "content": "## System Requirements\n\nBefore installing NGIAB, ensure you have:\n- **Operating System:** Windows (with WSL), macOS, or Linux\n- **Software:** Docker, Git\n- **Recommended Minimum RAM:** 8 GB\n\n> **Note**: Connecting to a remote machine through SSH? To use the Data Visualizer through an Secure Shell (SSH) connection, you will have to set up port forwarding when connecting to the remote machine. Port forwarding will allow you to access a remotely hosted browser session on your local machine. See the instructions under \"Using NGIAB through an SSH connection\" in the [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html) in this training module."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 7,
    "content": "## Docker Installation"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 8,
    "content": "### Windows (WSL)\n\nNote: Users who already have Docker installed will still need to install a separate WSL distro and set it as their default, if they have not already.\n\n1. Install Windows Subsystem for Linux (WSL):\n```powershell\nwsl --install\n```\n\n2. Install Docker Desktop from [Docker's official website](https://docs.docker.com/desktop/setup/install/windows-install/).\n\n3. Launch Docker Desktop and open WSL terminal as administrator.\n\n4. Verify Docker installation:\n```bash\ndocker run hello-world\n```\nThis should generate a message that shows that your installation is working.\n\n5. Install Astral UV:\n```bash"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 9,
    "content": "# Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 10,
    "content": "# Alternatively, install via pip if the above fails\npip install uv\n```\n\n> **CAUTION: WSL distributions**\n> NGIAB commands cannot be run through the `docker-desktop` distribution. If you installed Docker before WSL, you will likely need to install a new WSL distribution and set it as your default.\n> \n> For example, Ubuntu can be installed and set as the default distribution with the following commands:\n> ```bash\n> wsl --install -d Ubuntu\n> wsl --setdefault Ubuntu\n> ```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 11,
    "content": "### macOS\n\n1. Install Docker Desktop from [Docker's official Mac installer](https://docs.docker.com/desktop/setup/install/mac-install/).\n\n2. Launch Docker Desktop.\n\n3. Verify Docker installation:\n```bash\ndocker run hello-world\n```\nThis should generate a message that shows that your installation is working.\n\n4. Install Astral UV:\n```bash"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 12,
    "content": "# Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 13,
    "content": "# It can be installed via pip if that fails"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 14,
    "content": "# pip install uv\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 15,
    "content": "### Linux\n\n1. Install Docker by following the [official Docker guide](https://docs.docker.com/desktop/setup/install/linux/).\n\n2. Start Docker service and verify:\n```bash\nsudo systemctl start docker\ndocker run hello-world\n```\nThis should generate a message that shows that your installation is working.\n\n3. Install Astral UV:\n```bash"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 16,
    "content": "# Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 17,
    "content": "# It can be installed via pip if that fails"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 18,
    "content": "# pip install uv\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 19,
    "content": "### Verify Docker\n\nRun the command below:\n```bash\ndocker ps -a\n```\n\nConfirm it executes without errors. If errors occur, revisit Docker installation steps."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 20,
    "content": "#### Troubleshooting\nIf `docker ps -a` fails, ensure Docker Desktop is running, or Docker service is active on Linux:\n```bash\nsudo systemctl start docker\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 21,
    "content": "## NGIAB Setup\n\nThese steps will lead you through the process of running NGIAB with a set of pre-configured input data and realization files. A realization file is a scenario using a specific model on a specific region."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 22,
    "content": "### Step 1: Create Project Directory\n```bash\nmkdir -p NextGen/ngen-data\ncd NextGen/ngen-data\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 23,
    "content": "### Step 2: Download Sample Data\n\nChoose one of the following datasets. File sizes and model configurations are provided so that you can download a dataset suitable for your interests, available disk space, and network speed."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 24,
    "content": "#### Option 1: AWI-009\nModels: SLOTH (dummy model), NoahOWP (land surface model), CFE (conceptual rainfall-runoff model, functionally equivalent to NWM)\n\nCompressed file size: 249 MB | Extracted file size: 1.77 GB\n\n```bash\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\ntar -xf AWI_16_10154200_009.tar.gz\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 25,
    "content": "#### Option 2: AWI-007\nModels: SLOTH, NoahOWP, CFE\n\nCompressed file size: 1.87 MB | Extracted file size: 5.53 MB\n\n```bash\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-007/AWI_16_2863657_007.tar.gz\ntar -xf AWI_16_2863657_007.tar.gz\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 26,
    "content": "#### Option 3: AWI-008\nModels: SLOTH, LSTM (long short-term memory, recurrent neural network)\n\nCompressed file size: 1.50 MB | Extracted file size: 5.46 MB\n\n```bash\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-008/AWI_16_2863806_008.tar.gz\ntar -xf AWI_16_2863806_008.tar.gz\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 27,
    "content": "### Check: Did You Download the Dataset?\n\nRun this command:\n```bash\nls ~/NextGen/ngen-data\n```\n\nYou should see a folder like:\n```\nAWI_16_10154200_009\n```\n\nIf you see it, your dataset was downloaded and extracted correctly.\n\nIf you see nothing or just the `.tar.gz` file, run the following again:\n```bash\ntar -xf AWI_16_10154200_009.tar.gz\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 28,
    "content": "### Step 3: Clone and Run NGIAB\n\n> **CAUTION: For Windows users: pulling with LFs**\n> Before cloning the repository, please ensure that Git is configured to pull with LF line breaks instead of CRLFs. If CRLFs are used, then the carriage return characters will prevent the shell scripts from running properly.\n> \n> There are a couple options to configure this:\n> 1. Visual Studio Code can be used to manually toggle between line breaks.\n> 2. Git can be configured from the command line:\n> ```bash\n> git config --global core.autocrlf false\n> ```\n> 3. Download, extract, and run [this interactive `.bat` script](https://docs.ciroh.org/training-NGIAB-101/data/ngiab-newline-fixer.zip)\n\n```bash\ncd ../ # back to NextGen folder\ngit clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\ncd NGIAB-CloudInfra\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 29,
    "content": "## ✅ Ready to Go!\n\nIf you've completed the steps above and verified your dataset and working directory, you are ready to run the interactive guide script `guide.sh`. It will prompt you to select input data, processing modes, and will initiate your run.\n\n```bash\n./guide.sh\n```\n\nThis will walk you through the NGIAB setup and launch your first run."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 30,
    "content": "### `guide.sh` Tips\n- A series of prompts will appear that ask you if you want to use the existing Docker image or update to the latest image. Updating to the latest image will take longer, so for the purposes of this tutorial, using the existing Docker image is fine.\n- When prompted to run NextGen in serial or parallel mode, either is fine.\n- The option to open a Bash shell (interactive shell) will allow you to explore the data directory without quitting NGIAB.\n- Redirecting command output to `/dev/null` significantly reduces the amount of output. Either is fine, but if you are curious about what is happening inside the model, we suggest that you do not redirect the output."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 31,
    "content": "## Troubleshooting\n\n- Ensure Docker is running before executing `guide.sh`.\n- For permission errors on Linux, run Docker commands with `sudo` or add your user to the Docker group:\n\n```bash\nsudo usermod -aG docker ${USER}\nnewgrp docker\n```"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 32,
    "content": "### Are You in the Right Directory?\n\nBefore running any script, always check your current folder:\n```bash\npwd\n```\n\nYou **should see something like**:\n```\n/home/yourname/NextGen/NGIAB-CloudInfra\n```\n\nIf not, move into the folder:\n```bash\ncd ~/NextGen/NGIAB-CloudInfra\n```\n\n> **Tip:** Always execute a quick run with provided sample datasets to verify the successful setup of NGIAB."
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 33,
    "content": "## Additional Resources\n\nAre you interested in customizing your run with your own catchments (watersheds) and run configurations? Do you want to explore more functionalities of NGIAB? Check out the following episodes:\n\n- [Data Preparation - NGIAB Data Preprocessor](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html)\n- [Evaluation - NGIAB TEEHR Integration](https://docs.ciroh.org/training-NGIAB-101/evaluation.html)\n- [Visualization - Data Visualizer](https://docs.ciroh.org/training-NGIAB-101/visualization.html)\n- [Advanced Topics](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html)"
  },
  {
    "idurl": 164,
    "idtype": "text",
    "order": 34,
    "content": "## Key Points\n\n- NGIAB simplifies NextGen framework deployment through Docker.\n- Use `guide.sh` for interactive configuration and run execution.\n- Always confirm successful setup by executing provided sample runs."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 1,
    "content": "# Data Preparation\n\nLast updated on 2025-05-09"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How should I prepare my run directory?\n- What is the Data Preprocess tool?"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Identify the required data structure of a NextGen run in NGIAB\n- Explain how the Data Preprocess tool interacts with NGIAB\n- Prepare data for an NextGen run in NGIAB"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 5,
    "content": "## Data Preprocess Tool\n\n**The Data Preprocess tool streamlines data preparation for NextGen runs in NGIAB.** This tool provides a graphical user interface (GUI) and a command line interface (CLI) to prepare input data and execute model runs. A graphical user interface facilitates catchment and date range selection options via an interactive map, simplifying the subsetting of hydrofabrics, generation of forcings, and creation of default NextGen realizations. While this module reduces procedural complexity, it incorporates pre-defined assumptions that may limit user flexibility in specific applications [(Cunningham, 2025)](https://github.com/CIROH-UA/NGIAB_data_preprocess)."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 6,
    "content": "### Installing and Using the Data Preprocess Tool\n\nThe Data Preprocess tool (like all of our software) is constantly being updated and refined. As of the time of writing (see last updated date above), there are two ways to run the tool. Instructions for installation, environment management, and the GUI/CLI are found on the [Data Preprocess GitHub page](https://github.com/CIROH-UA/NGIAB_data_preprocess). We will cover some examples of the CLI usage, but full documentation is on the GitHub page."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 7,
    "content": "#### Example 1\n\nThis command allows you to run the Data Preprocess CLI tool without installing it. It produces forcings and a NextGen realization file for the catchments upstream of gage-10154200 for the time period 2017-09-01 to 2018-09-01. Forcing data is sourced from the zarr files in the [Analysis of Record for Calibration (AORC) dataset](https://registry.opendata.aws/noaa-nws-aorc/), which allows for a **faster processing time**.\n\nAstral UV is required to run the Data Preprocess tool without installing it.\n\n```bash\nuvx --from ngiab_data_preprocess cli -i gage-10154200 -sfr --start 2017-09-01 --end 2018-09-01 --source aorc\n```\n\n`uvx --from ngiab_data_preprocess cli` indicates that the Data Preprocess tool will run without the user installing it. The `--source` flag determines where the Data Preprocess tool will pull forcing data from."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 8,
    "content": "#### Example 2\n\nThis command produces forcings and a NextGen realization file for the catchments upstream of gage-10155000 for the time period 2022-08-13 to 2022-08-23 after installing the Data Preprocess tool. The forcing data source defaults to the NetCDF files in the [NWM 3.0 retrospective](https://aws.amazon.com/marketplace/pp/prodview-g6lcchc7brshw). Using this command requires you to have Astral UV (a package installer and environment manager) installed. Instructions for installing Astral UV are found in the [Astral UV documentation](https://docs.astral.sh/uv/#highlights).\n\nTo install the Data Preprocess tool, follow the latest instructions on the [Data Preprocess GitHub page](https://github.com/CIROH-UA/NGIAB_data_preprocess).\n\n```bash\nuv run cli -i gage-10155000 -sfr --start 2022-08-13 --end 2022-08-23\n```\n\n`uv run cli` indicates that the Data Preprocess CLI within your activated Astral UV environment will run. The `-i` flag indicates the **ID** of the feature that is used to subset the hydrofabric. The `-sfr` flags indicate that the Data Preprocess tool will **s**ubset the hydrofabric to the desired catchments, produce **f**orcings over the desired area and time period, and produce a NextGen **r**ealization file. The `--start` and `--end` flags indicate the start and end dates of the desired time period."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 9,
    "content": "#### Example 3\n\nThis command allows you to run the Data Preprocess CLI tool from a regular `pip install ngiab_data_preprocess`. **However, using Astral UV is highly recommended for its speed.** This command produces forcings for the catchments upstream of cat-7080 for the time period 2022-01-01 to 2022-02-28.\n\n```bash\npython -m ngiab_data_cli -i cat-7080 -f --start 2022-01-01 --end 2022-02-28\n```\n\n`python -m ngiab_data_cli` indicates that the Data Preprocess CLI tool will execute.\n\n![USGS National Map](https://docs.ciroh.org/training-NGIAB-101/fig/fig3-1.png)\n\nFigure 1: Map showing an example drainage basin. View from the USGS National Map.\n\n![Data Preprocess tool view](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-4.png)\n\nFigure 2: Map showing an example drainage basin. View from the Data Preprocess tool. The highlighted region (light orange area; downstream-most basin in pink) represents the specific study basin, illustrating the river network (blue lines), sub-basins (orange), and surrounding USGS gaging stations (black dots)."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 10,
    "content": "## NextGen Run Directory Structure (`ngen-run/`)\n\nRunning NextGen requires building a standard run directory complete with only the necessary files. This is done automatically with the Data Preprocess tool. Below is an explanation of the standard run directory.\n\nA NextGen run directory `ngen-run` contains the following subfolders:\n\n- `config`: model configuration files and hydrofabric configuration files. (required)\n- `forcings`: catchment-level forcing timeseries files. Forcing files contain variables like wind speed, temperature, precipitation, and solar radiation. (required)\n- `lakeout`: for t-route (optional)\n- `metadata`: programmatically generated folder used within ngen. Do not edit this folder. (automatically generated)\n- `outputs`: This is where ngen will place the output files. (required)\n- `restart`: For restart files (optional)\n\n```\nngen-run/\n│\n├── config/\n│\n├── forcings/\n│\n├── lakeout/\n|\n├── metadata/\n│\n├── outputs/\n│\n├── restart/\n```"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 11,
    "content": "### Configuration directory `ngen-run/config/`\n\nThis folder contains the NextGen realization file, which serves as the primary model configuration for the ngen framework. This file specifies which models to run (such as NoahOWP/CFE, LSTM, etc), run parameters like date and time, and hydrofabric specifications (like location, gage, catchment).\n\nBased on the models defined in the realization file, [BMI](https://bmi.csdms.io/en/stable/index.html) configuration files may be required. For those models that require per-catchment configuration files, a folder will hold these files for each model in `ngen-run/config/cat-config`. See the directory structure convention below.\n\n```\nngen-run/\n|\n├── config/\n|   │"
  },
  {
    "idurl": 165,
    "idtype": "table",
    "order": 12,
    "content": "|   ├── nextgen_09.gpkg\n|   |\n|   ├── realization.json\n|   |\n|   ├── ngen.yaml\n|   |\n|   ├── cat-config/\n|   │   |\n|   |   ├──PET/\n|   │   |\n|   |   ├──CFE/\n|   │   |\n|   |   ├──NOAH-OWP-M/"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 13,
    "content": "```\n\nNextGen requires a single geopackage file. This file is the [hydrofabric (Johnson, 2022)](https://mikejohnson51.github.io/hyAggregate/) (spatial data). An example geopackage can be found on [Lynker-Spatial's website](https://www.lynker-spatial.com/data?path=hydrofabric%2Fv2.2%2F). Tools to subset a geopackage into a smaller domain can be found at [Lynker's hfsubset](https://github.com/LynkerIntel/hfsubset)."
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 14,
    "content": "## Your Turn\n\nUsing the Data Preprocess tool, you should be able to create a run directory for your desired catchment that can be used with NGIAB. Try out both the GUI and the CLI, and experiment with different arguments and selection tools!"
  },
  {
    "idurl": 165,
    "idtype": "text",
    "order": 15,
    "content": "## Key Points\n\n- `ngen-run/` is the standard NextGen run directory, containing the realization files that define models, parameters, and run settings; forcing data; outputs; as well as the spatial hydrofabric.\n- The Data Preprocess tool simplifies preparing data for NextGen by offering a GUI and CLI for selecting catchments and date ranges, subsetting hydrofabric data, generating forcing files, and creating realization files."
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 1,
    "content": "# Model Execution\n\nLast updated on 2025-05-01"
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I execute a NextGen run?"
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Recognize methods to execute NextGen models\n- Execute a NextGen run using NGIAB"
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 5,
    "content": "## Model Execution using `guide.sh`\n\n`guide.sh` is used to execute pre-configured NextGen runs in NGIAB. These settings can be configured by users ahead of time using the [Data Preprocess tool](https://docs.ciroh.org/training-NGIAB-101/data-preparation.html). Execute the following commands:\n\n```bash\ncd NextGen\ngit clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\ncd NGIAB-CloudInfra\n./guide.sh\n```\n\n`guide.sh` will prompt you to enter input data pathways and allow you to select a computational mode (serial or parallel processing). After the run is complete, `guide.sh` will give you the option to evaluate model predictions and visualize results (discussed in the next two episodes)."
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 6,
    "content": "## Model Execution using Data Preprocess tool\n\nA secondary method for executing a NextGen run in NGIAB is by using the Data Preprocess tool's CLI. The `-a` argument in the command will automatically run NGIAB after preprocessing selected data. As this module is being updated constantly, check back on its [GitHub page](https://github.com/CIROH-UA/NGIAB_data_preprocess) for the latest updates on its functionality."
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 7,
    "content": "## Your Turn\n\nUse `guide.sh` to execute a NextGen run in NGIAB using your preprocessed data.\n\nExtra Credit: Use the Data Preprocess tool to automatically execute a NextGen run in NGIAB."
  },
  {
    "idurl": 166,
    "idtype": "text",
    "order": 8,
    "content": "## Key Points\n\n- To execute a NextGen run in NGIAB with full functionality, use `guide.sh` in the NGIAB container.\n- A NextGen run in NGIAB can also be automatically executed post-preprocessing using the Data Preprocess tool."
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 1,
    "content": "# Evaluation\n\nLast updated on 2025-04-25"
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I use Tools for Exploratory Evaluation in Hydrologic Research (TEEHR) to evaluate my NextGen models in NGIAB?"
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Explain how TEEHR interacts with NGIAB\n- Evaluate a NextGen run using TEEHR in NGIAB"
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 5,
    "content": "## Using Tools for Exploratory Evaluation in Hydrologic Research (TEEHR) with NGIAB\n\nA TEEHR evaluation run is executed by default upon execution of `guide.sh`. A separate `runTeehr.sh` script is also available in the NGIAB-CloudInfra repository."
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 6,
    "content": "## What is TEEHR?\n\nTEEHR is a Python-based package enabling analysis of hydrologic model performance [(RTI International, 2025)](https://github.com/RTIInternational/teehr). TEEHR also provides tools to fetch the U.S. Geological Survey (USGS) streamflow data and NWM gridded and point-based retrospective and near real-time forecast data. This functionality supports **comprehensive model evaluation and uncertainty analysis**. NGIAB leverages TEEHR as a tool for hydrological model evaluation [(CIROH, 2024)](https://github.com/CIROH-UA/ngiab-teehr). Researchers can explore a comprehensive range of metrics, including error statistics, skill scores, hydrologic signatures, and uncertainty quantification.\n\n> **Note: Gridded Data vs. Point Data**\n> \n> Gridded data consists of values at regularly spaced 2-dimensional cells that form a grid covering a region on Earth. Point data consists of values on specific 0-dimensional points. The NWM real-time forecast produces point-type stream routing and reservoir variables."
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 7,
    "content": "### TEEHR folder contents\n\nTEEHR consolidates data from the USGS and NWM, allowing side‐by‐side visual comparisons of observed and simulated over the model run intervals. Figure 1 shows the default comparison of the modeled outlet hydrograph and the corresponding time series from the NWM 3.0. While the default configuration produces this view, the TEEHR user documentation provides additional examples of capabilities for customized plotting functions.\n\n![Hydrograph comparison](https://docs.ciroh.org/training-NGIAB-101/fig/fig5-1.png)\n\nFigure 1: Comparison of the NextGen-based model (labeled \"ngen\"; blue line) and the NWM time series (labeled \"nwm30_retrospective\"; orange line) for the same location. The figure is automatically generated by the TEEHR-based analysis that accompanies the `guide.sh` script included with NGIAB and is named `timeseries_plot_streamflow_hourly_inst.html` in the `teehr` folder.\n\nIn addition to hydrograph visualization, TEEHR also calculates key performance metrics such as Kling-Gupta Efficiency (KGE), Nash-Sutcliffe Efficiency (NSE), and Relative Bias (RB) to quantify the accuracy of model predictions against observed data ([Gupta et al., 2009](https://doi.org/10.1016/j.jhydrol.2009.08.003); [Nash and Sutcliffe, 1970](https://doi.org/10.1016/0022-1694(70)90255-6)). Users do not need to understand all these metrics immediately, and can refer to the references above to learn more. These results are then stored in a standardized output directory. For instance, the `metrics.csv` file in the TEEHR folder structure contains aggregated statistics for each configuration.\n\nThrough this systematic approach, NGIAB and TEEHR together allow hydrologists and stakeholders to inspect, compare, and refine model predictions in an open-source environment."
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 8,
    "content": "## Your Turn\n\nGo ahead and execute the TEEHR run using `guide.sh` and explore your TEEHR folder!\n\n> **`guide.sh` Tips**\n> - The default TEEHR image is fine to use."
  },
  {
    "idurl": 167,
    "idtype": "text",
    "order": 9,
    "content": "## Key Points\n\n- Tools for Exploratory Evaluation in Hydrologic Research (TEEHR) is a Python-based package for hydrologic model evaluation.\n- NGIAB uses TEEHR to assess model performance, comparing predictions against USGS streamflow and NWM data and calculating performance metrics.\n- TEEHR runs automatically with the main `guide.sh` NGIAB script."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 1,
    "content": "# Visualization\n\nLast updated on 2025-05-27"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I visualize my NextGen runs?"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Explain how the Data Visualizer complements NGIAB.\n- Use the Data Visualizer to visualize results of a NextGen run in NGIAB."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 5,
    "content": "## Data Visualizer\n\nThe Data Visualizer component developed using the Tethys Platform [(Swain et al., 2015)](https://doi.org/10.1016/j.envsoft.2015.01.014) complements NGIAB by providing an environment for **geospatial and time series visualization of catchments and nexus points** (locations where objects in the hydrofabric like streams or water bodies connect). Through a web-based architecture, researchers can explore hydrological data in a spatiotemporal context [(CIROH, 2025)](https://github.com/CIROH-UA/ngiab-client). In addition to standard map-based displays, this component also **supports the visualization of the TEEHR output**, including tabular metrics and interactive time series plots.\n\n![NGIAB and DataStream Visualizer web interface](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-1.png)\n\nFigure 1: A map showing the geospatial visualization using the Data Visualizer within the Tethys framework for an entire study area (Provo River near Woodland, UT)."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 6,
    "content": "## Using the Data Visualizer with NGIAB"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 7,
    "content": "### `ViewOnTethys` Script\n\nLike TEEHR, the Data Visualizer can be activated upon execution of the main NGIAB guide script, `guide.sh`. A separate `viewOnTethys.sh` script is also available in the NGIAB-CloudInfra repository.\n\nOnce a run is complete, users can launch the Data Visualizer through their web browser when prompted by the guide script. Although TEEHR's outputs can be displayed within the Data Visualizer, this tool is primarily designed to provide a broad overview of model results. Users seeking TEEHR's more advanced analysis features can still access them outside the Data Visualizer.\n\nOne of the advantages of the `viewOnTethys.sh` script is that it allows the user to keep multiple outputs for the same hydrofabric. It prompts the user if they want to use the same output directory by renaming it and adding it to the collection of outputs or if they want to overwrite it.\n\n```bash\n  ⚠ ~/ngiab_visualizer is not empty.\n  → Keep (K) or Fresh start (F)? [K/F]: k\nℹ Reclaiming ownership of ~/ngiab_visualizer  (sudo may prompt)…\n  ⚠ Directory exists: ~/ngiab_visualizer/gage-10154200\n  → Overwrite (O) or Duplicate (D)? [O/D]: o\n  ✓ Overwritten ➜ ~/ngiab_visualizer/gage-10154200\nChecking for ~/ngiab_visualizer/ngiab_visualizer.json...\n```\n\nYou should be able to see multiple outputs through the UI:\n\n![NGIAB Visualizer dropdown for multiple outputs](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-2.png)\n\nFigure 2: NGIAB Visualizer dropdown for multiple outputs"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 8,
    "content": "### Visualizer Directory Organization\n\nThe Visualizer organizes data using a directory named `ngiab_visualizer`. This directory stores all the outputs generated by the user and includes a file called `ngiab_visualizer.json`. This file contains metadata that allows the Visualizer to locate and manage the outputs from different runs executed via the `./guide.sh` script. The metadata, specific to the Visualizer, is structured as a simple JSON file. It lists the outputs with details such as a label, the data path, and a unique identifier. The `./ViewOnTethys.sh` script handles the creation and management of this metadata.\n\n```json\n{\n  \"model_runs\": [\n    {\n      \"label\": \"gage-10154200\",\n      \"path\": \"/var/lib/tethys_persist/ngiab_preprocess_output/gage-10154200\",\n      \"date\": \"2025-05-23:13:51:51\",\n      \"id\": \"61026834-4235-4d39-8a8e-f076a8854148\",\n      \"subset\": \"\",\n      \"tags\": []\n    },\n    {\n      \"label\": \"gage-20454200\",\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer/gage-20454200\",\n      \"date\": \"2025-05-23:17:00:34\",\n      \"id\": \"68f6cf78-188c-4e86-b797-6c40ea36e0e6\",\n      \"subset\": \"\",\n      \"tags\": []\n    },\n    {\n      \"label\": \"gage-35054600\",\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer/gage-35054600\",\n      \"date\": \"2025-05-23:17:01:10\",\n      \"id\": \"6d0cb736-2dac-4ea0-a3a3-ad26cd45ef36\",\n      \"subset\": \"\",\n      \"tags\": []\n    }\n  ]\n}\n```\n\nThe path `/var/lib/tethys_persist/` belongs to the `$HOME` env variable of the container running the visualizer. When the user runs the `./ViewOnTethys.sh`, it mounts the directory from the host at `~/ngiab_visualizer` to `/var/lib/tethys_persist/ngiab_visualizer`.\n\nHowever, if the user wants more control, the user can copy their data directory to `~/ngiab_visualizer` on the host (not the container) while the container is **stopped**:\n\n```bash\nchown -R $USER: ~/ngiab_visualizer\ncp -R your/data/path ~/ngiab_visualizer\n```\n\nFinally the user can open the `~/ngiab_visualizer/ngiab_visualizer.json` on the host (not the container), and add the specific run to the visualizer:\n\n```json\n....\n    {\n      \"label\": \"gage-35054600\",\n      \"path\": \"/var/lib/tethys_persist/ngiab_visualizer<MY_SPECIFIC_OUTPUT_DIRECTORY_NAME>\",\n      \"date\": \"2025-05-23:17:01:10\",\n      \"id\": \"6d0cb736-2dac-4ea0-a3a3-ad26cd45ef36\",\n      \"subset\": \"\",\n      \"tags\": []\n    }\n```\n\nThe user can then run `./ViewOnTethys.sh` script to spin again the container or if the user wants more control and just define the env variables and running the container:\n\n```bash\nexport CONFIG_FILE=\"$HOME/.host_data_path.conf\"          \\\n       TETHYS_CONTAINER_NAME=\"tethys-ngen-portal\"        \\\n       TETHYS_REPO=\"awiciroh/tethys-ngiab\"               \\\n       TETHYS_TAG=\"latest\"                               \\\n       NGINX_PORT=80                                     \\\n       MODELS_RUNS_DIRECTORY=\"$HOME/ngiab_visualizer\"    \\\n       DATASTREAM_DIRECTORY=\"$HOME/.datastream_ngiab\"    \\\n       VISUALIZER_CONF=\"$MODELS_RUNS_DIRECTORY/ngiab_visualizer.json\" \\\n       TETHYS_PERSIST_PATH=\"/var/lib/tethys_persist\"     \\\n       SKIP_DB_SETUP=false                               \\\n       CSRF_TRUSTED_ORIGINS=\"[\\\"http://localhost:${NGINX_PORT}\\\",\\\"http://127.0.0.1:${NGINX_PORT}\\\"]\"\n```\n\n```bash\ndocker run --rm -d \\\n  -v \"$MODELS_RUNS_DIRECTORY:$TETHYS_PERSIST_PATH/ngiab_visualizer\" \\\n  -v \"$DATASTREAM_DIRECTORY:$TETHYS_PERSIST_PATH/.datastream_ngiab\" \\\n  -p \"$NGINX_PORT:$NGINX_PORT\" \\\n  --name \"$TETHYS_CONTAINER_NAME\" \\\n  -e MEDIA_ROOT=\"$TETHYS_PERSIST_PATH/media\" \\\n  -e MEDIA_URL=\"/media/\" \\\n  -e SKIP_DB_SETUP=\"$SKIP_DB_SETUP\" \\\n  -e DATASTREAM_CONF=\"$TETHYS_PERSIST_PATH/.datastream_ngiab\" \\\n  -e VISUALIZER_CONF=\"$TETHYS_PERSIST_PATH/ngiab_visualizer/ngiab_visualizer.json\" \\\n  -e NGINX_PORT=\"$NGINX_PORT\" \\\n  -e CSRF_TRUSTED_ORIGINS=\"$CSRF_TRUSTED_ORIGINS\" \\\n  \"${TETHYS_REPO}:${TETHYS_TAG}\"\n```\n\nYou should see something like this using `docker ps`:\n\n```bash\ndocker ps\n\nCONTAINER ID   IMAGE                          COMMAND                  CREATED          STATUS                             PORTS                                 NAMES\nb1818a03de9b   awiciroh/tethys-ngiab:latest   \"/usr/local/bin/_ent…\"   25 seconds ago   Up 24 seconds (health: starting)   0.0.0.0:80->80/tcp, [::]:80->80/tcp   tethys-ngen-portal\n```\n\nOnce it's healthy, you can access the visualizer at: `http://localhost:${NGINX_PORT}`\n\nThe `ViewOnTethys.sh` script automates updating `ngiab_visualizer.json` and copying your model output into `~/ngiab_visualizer`."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 9,
    "content": "### Key points\n- `ViewOnTethys.sh` automates adding model outputs to `ngiab_visualizer.json` and syncing data to `~/ngiab_visualizer`.\n- To customize your setup, set environment variables and run the `awiciroh/tethys-ngiab` Docker image manually."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 10,
    "content": "### NGIAB Visualizer UI\n\nThe following figures demonstrate several ways the Data Visualizer can be used to visualize model outputs, including geospatial visualization for nexus points, catchment-based visualization, and TEEHR time series representation (hydrographs).\n\n**Nexus** points can be visualized when the user selects the output that wants to visualize. Time series can be retrieved by clicking on any of the **Nexus** points, or by changing the select dropdown assigned to the Nexus.\n\n![NGIAB Visualizer time series visualization from Nexus points](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-3.png)\n\nFigure 3: NGIAB Visualizer time series visualization from Nexus points\n\n**Troute** variables time series can also be displayed using the **Troute** select dropdown.\n\n![NGIAB Visualizer time series visualization from Troute variables](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-4.png)\n\nFigure 4: NGIAB Visualizer time series visualization from Troute variables\n\n**Catchments** time series can be retrieved by clicking on any of the **Catchments** polygons, or by changing the select dropdown assigned to the Catchments.\n\n![NGIAB Visualizer time series visualization for Catchments](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-5.png)\n\nFigure 5: NGIAB Visualizer time series visualization for Catchments\n\n**TEEHR** evaluation can be visualized when the user hits a point that contains **TEEHR** evaluation output, the user can also look at a **Nexus** point on the dropdown assigned and enter the id of the **Nexus** points that contains **TEEHR** evaluation output.\n\n![NGIAB Visualizer with TEEHR comparison](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-6.png)\n\nFigure 6: A map showing the geospatial visualization using the Data Visualizer within the Tethys framework for a selected outlet nexus point as well as displaying a time series plot between observed (labeled \"USGS\"; blue line) and simulated (labeled \"ngen\"; orange line)\n\nSimilarly, a **TEEHR** evaluation metric can be visualized by going to the metrics tab:\n\n![NGIAB Visualizer performance metrics](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-7.png)\n\nFigure 7: NGIAB Visualizer performance metrics (KGE, NSE, and relative bias). The Visualizer can also show the performance of the NWM 3.0 compared to the observed time series.\n\n> **Note: Using Data Visualizer with SSH**\n> \n> To use the Data Visualizer through an Secure Shell (SSH) connection, you will have to set up port forwarding when connecting to the remote machine. Port forwarding will allow you to access a remotely hosted browser session on your local machine. See the instructions under \"Using NGIAB through an SSH connection\" in the [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html) in this training module."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 11,
    "content": "### NGIAB DataStream UI\n\nThe Visualizer also allows the user to download data as well from an [S3 bucket](https://datastream.ciroh.org/index.html) containing the output of the [NextGen DataStream](https://github.com/CIROH-UA/ngen-datastream). The `ViewOnTethys.sh` script will create a `~/.datastream_ngiab` directory in which it saves all the different outputs downloaded by the visualizer. It will also create a `~/.datastream_ngiab/datastream_ngiab.json` in which metadata will be saved to locate the downloaded output directories. It serves as a cache, so it allows the user to look first at the `~/.datastream_ngiab` before trying to download the data.\n\n```bash\nℹ Reclaiming ownership of /home/aquagio/.datastream_ngiab  (sudo may prompt)…\n  ℹ No existing Datastream cache found – a fresh download will be used.\n```\n\nThe `.datastream_ngiab.json` appends the different downloads with metadata that allows the user to know the file being downloaded. The `prefix` belongs to the path on the s3 bucket. The `label` is created with the following format: `ngen.<date>_<forecast_type>_<cycle>_<VPU>`\n\n```json\n{\n    \"datastream\": [\n        {\n            \"label\": \"ngen.20250522_medium_range_06_VPU_02\",\n            \"bucket\": \"ciroh-community-ngen-datastream\",\n            \"prefix\": \"v2.2/ngen.20250522/medium_range/06/VPU_02/ngen-run.tar.gz\",\n            \"path\": \"/var/lib/tethys_persist/.datastream_ngiab/ngen.20250522_medium_range_06_VPU_02\",\n            \"date\": \"2021-01-01:00:00:00\",\n            \"id\": \"15145d327f19426b890e4465160f963a\"\n        }\n    ]\n}\n```\n\n**Note** assuming only the first ensemble. If we are specific it will look like this: `ngen.<date>_<forecast_type>_<cycle>_<ensemble>_<VPU>`\n\nThis functionality allows the user to be able to quickly search the data they want from the [S3 bucket](https://datastream.ciroh.org/index.html) containing the output of the [NextGen DataStream](https://github.com/CIROH-UA/ngen-datastream). They can explore and download as needed.\n\n![NGIAB Visualizer Visualization of DataStream Data](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-8.png)\n\nFigure 8: NGIAB Visualizer Visualization of DataStream Data"
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 12,
    "content": "## Your Turn\n\nUse the `ViewOnTethys.sh` script to launch the Data Visualizer in Docker, then open `http://localhost:${NGINX_PORT}` in your browser. Explore at least three different visualization modes—such as Nexus time series, catchment hydrographs, and TEEHR performance metrics. Next, edit `~/ngiab_visualizer/ngiab_visualizer.json` to add a new model run (define its label and path), restart the visualizer, and confirm it appears in the dropdown menu."
  },
  {
    "idurl": 168,
    "idtype": "text",
    "order": 13,
    "content": "## Key Points\n\n- The Data Visualizer (built on the Tethys Platform) provides interactive geospatial maps and time series plots for NextGen model outputs in NGIAB.\n- It integrates seamlessly with NGIAB via `guide.sh` or `ViewOnTethys.sh`.\n- Model outputs reside under `~/ngiab_visualizer`, with metadata stored in `ngiab_visualizer.json`.\n- You can visualize Nexus points, catchment summaries, Troute variables, and TEEHR hydrographs and performance metrics."
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 1,
    "content": "# Calibration\n\nLast updated on 2025-06-10"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I calibrate parameters for a NextGen run?"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Create calibration configurations for a NextGen run\n- Run the `ngiab-cal` package"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 5,
    "content": "## Parameter Calibration using `ngiab-cal`\n\n`ngiab-cal` is a Python package that creates calibration configurations and copies calibrated parameters to a NextGen model configuration. It works with the NGIAB folder structure to ensure compatibility with the other suite of NGIAB tools.\n\nThe modeled time period is split into three periods: warmup, calibration, and validation. After the warmup period, the model parameters are adjusted to match observations in the calibration period, and the validation period is used to test the calibrated parameters."
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 6,
    "content": "## Using `ngiab-cal`\n\nTo use `ngiab-cal` without installation:\n\n```bash"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 7,
    "content": "# Run directly without installation\nuvx ngiab-cal --help\n```\n\nTo install `ngiab-cal` as a package:\n\n```bash"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 8,
    "content": "# Or install as a tool\nuv tool install ngiab-cal\nngiab-cal --help\n```\n\nA configuration file at `calibration/ngen_cal_conf.yaml` controls the calibration process. Users can edit the configuration file to meet their needs and preferences, such as number of iterations, acceptable parameter ranges, and time periods. The layout of the configuration file is as follows:\n\n```yaml\ngeneral:\n  strategy:\n    type: estimation\n    algorithm: dds         # Uses Dynamically Dimensioned Search algorithm\n  name: calib              # Don't modify this\n  log: true               # Enable logging\n  workdir: /ngen/ngen/data/calibration  # Don't modify this working directory in the Docker container\n  yaml_file: /ngen/ngen/data/calibration/ngen_cal_conf.yaml # Don't modify this either\n  iterations: 100         # Number of calibration iterations (customizable with -i flag)\n  restart: 0              # Start from beginning (0) or resume from iteration"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 9,
    "content": "# Model configurations\nCFE:\n  - name: b               # CFE parameter name\n    min: 2.0              # Minimum allowed value\n    max: 15.0             # Maximum allowed value\n    init: 4.05            # Initial value\n  - name: satpsi\n    min: 0.03\n    max: 0.955\n    init: 0.355\n  # Additional parameters...\n\neval_params:\n  objective: kge           # Kling-Gupta Efficiency as objective function\n  evaluation_start: \"...\"  # Start time for calibration period\n  evaluation_stop: \"...\"   # End time for calibration period\n  valid_start_time: \"...\"  # Start time including warmup\n  valid_end_time: \"...\"    # End time of simulation\n  # Additional time parameters...\n  basinID: 01646500        # USGS gage ID\n  site_name: \"USGS 01646500: \"  # Label for plots\n```\n\nCalibrating and saving parameters requires the three following commands, where `/path/to/ngiab/data/folder` is replaced with the appropriate filepath, and `USGS_GAGE_ID` is replaced with an appropriate USGS gage within your study area.\n\n```bash"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 10,
    "content": "# Create calibration configuration\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 11,
    "content": "# Create and run calibration (200 iterations)\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID --run -i 200"
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 12,
    "content": "# Force recreation of calibration configuration\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID -f\n```\n\nMore details about usage of `ngiab-cal` can be found on [its GitHub page](https://github.com/CIROH-UA/ngiab-cal)."
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 13,
    "content": "## Your Turn\n\nUse the `ngiab-cal` package to calibrate parameters for the input data that you used for your latest run.\n\nExtra Credit: execute a NextGen run again, and compare the performance between calibrated parameters and uncalibrated parameters."
  },
  {
    "idurl": 169,
    "idtype": "text",
    "order": 14,
    "content": "## Key Points\n\n- The `ngiab-cal` package is used to calibrate parameters for a NextGen model run.\n- `ngiab-cal` is a command-line tool controlled via a YAML configuration file that determines parameter ranges, time periods, and evaluation metrics."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 1,
    "content": "# Advanced Topics\n\nLast updated on 2025-05-01"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 2,
    "content": "## Overview"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 3,
    "content": "### Questions\n- How do I use NGIAB on an high-performance computing (HPC) system?\n- How do I use the Data Visualizer through an SSH connection?\n- How can I contribute to NGIAB?"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 4,
    "content": "### Objectives\n- Install and use NGIAB on an HPC\n- Use port forwarding to view NGIAB results\n- Explain the NGIAB community contribution process"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 5,
    "content": "## Using NGIAB on High-Performance Computing (HPC) Environments: General Info\n\nThe most up-to-date information on installing NGIAB on an HPC can be found on [CIROH'S NGIAB HPC GitHub page](https://github.com/CIROH-UA/NGIAB-HPCInfra). Other than a different installation process and the use of Singularity instead of Docker, the workflow is the same to execute a NextGen run in NGIAB. Tools like the Data Preprocessor, TEEHR, and the Data Visualizer are still available. The NGIAB-HPCInfra contains its own interactive `guide.sh` script, which allows users to specify input data pathways and run configurations (serial or parallel), as well as trigger the execution of TEEHR and the Data Visualizer."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 6,
    "content": "### Singularity\n\nNGIAB uses Singularity as its containerization platform for HPC environments. Singularity enables secure execution of containerized applications on multi-user HPC clusters. Key features of Singularity include:\n\n- Native HPC integration, which allows the execution of containerized applications within existing batch job schedulers such as SLURM (Simple Linux Utility for Resource Management) workload manager, PBS (Portable Batch System) and LSF (Load Sharing Facility)\n- Enforced security – it runs containers as non-root users, reducing security risks; and\n- Access to host file systems – it enables users to interact with datasets and computational resources without additional configuration directly."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 7,
    "content": "## Run NGIAB on Pantarhei HPC (Singularity Version)\n\nThis section explains how to run **NextGen In A Box (NGIAB)** using **Singularity** on the **Pantarhei HPC system** at the University of Alabama. To access Pantarhei, please follow the instructions on [CIROH's DocuHub page](https://docs.ciroh.org/docs/services/access/#accessing-on-premises-infrastructure)."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 8,
    "content": "### 1. Log Into Pantarhei\n\nOpen a terminal and connect to the login node:\n\n```bash\nssh <USERNAME>@pantarhei.ua.edu\n```\n\nReplace `<USERNAME>` with your actual Pantarhei username."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 9,
    "content": "### 2. Request a Compute Node (Do NOT run on login node)\n\nOn the login node, request an interactive session:\n\n```bash\nsrun --partition=normal --nodes=1 --ntasks=1 --time=02:00:00 --pty bash\n```\n\nUse the `normal` partition unless you require more time or special resources."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 10,
    "content": "### 3. Verify that Singularity is available:\n\n```bash\nsingularity --version\n```"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 11,
    "content": "### 4. Prepare Project Directory\n\nCreate the directory and move into it:\n\n```bash\nmkdir -p ~/NextGen/ngen-data\ncd ~/NextGen/ngen-data\n```"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 12,
    "content": "### 5. Download Sample Dataset\n\nPick one of the sample datasets to download and extract:"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 13,
    "content": "#### Option 1: AWI-009 (Provo River, UT)\n\n```bash\nwget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\ntar -xf AWI_16_10154200_009.tar.gz\n```\n\nOther options: AWI-007 or AWI-008 can be used similarly, see the [Installation and Setup episode](https://docs.ciroh.org/training-NGIAB-101/installation.html#step-2-download-sample-data)."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 14,
    "content": "### 6. Clone the NGIAB-HPCInfra Repository\n\n```bash\ncd ~/NextGen\ngit clone https://github.com/CIROH-UA/NGIAB-HPCInfra.git\ncd NGIAB-HPCInfra\n```\n\n> ✅ **Note:** Always run `guide.sh` from inside the `NGIAB-HPCInfra` folder."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 15,
    "content": "### 7. Run `guide.sh`\n\nMake the script executable if needed:\n\n```bash\nchmod +x guide.sh\n```\n\nThen run it:\n\n```bash\n./guide.sh\n```\n\nFollow the prompts:\n\n- When asked **\"Do you want to use the same path?\"**, type `n`\n- Then enter the **full absolute path** to your extracted dataset folder. Example:\n\n```bash\n/home/<username>/NextGen/ngen-data/AWI_16_10154200_009\n```\n\nThis folder must contain:\n```\nforcings/\nconfig/\noutputs/\n```\n\nThe script will:\n- Detect system architecture\n- Pull the correct Singularity image\n- Mount your dataset\n- Allow running in:\n  - Serial mode\n  - Parallel mode\n  - Interactive container shell"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 16,
    "content": "### Notes\n\n- Do not run the model or load modules on the login node.\n- All commands should be executed on a **compute node**.\n- If output files do not appear, double-check the input path and folder structure.\n- If `outputs/` does not exist, create an empty folder manually before running."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 17,
    "content": "## Using NGIAB through an SSH connection\n\nNGIAB's core functions work through an SSH connection without port forwarding. However, to use the Data Visualizer, you will have to set up port forwarding to view visualization results on your local machine's browser.\n\nTo do so, run the following command on your local machine:\n\n```bash\nssh -L 80:localhost:80 username@remote_host\n```\n\nReplace `username@remote_host` with your credentials.\n\nNow, you should be able to run NGIAB as usual through your SSH tunnel, and access Data Visualizer results in your local browser."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 18,
    "content": "## Community Contributions to NGIAB/NextGen\n\nThe most up-to-date guidelines on community contributions for each repository can be found on its respective GitHub page."
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 19,
    "content": "### General contribution guidance\n\n- You can use the issue tracker on GitHub to suggest feature requests, report bugs, or ask questions.\n- You can change the codebase through Git:\n  - Create a fork\n  - Clone the repository locally\n  - Keep the fork and clone up-to-date\n  - Create branches when you want to contribute\n  - Make changes to the code\n  - Commit to your local branch\n  - Push commits to your GitHub fork\n  - Create a pull request when the changes are ready to be incorporated"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 20,
    "content": "## Your Turn\n\nBased on your own interests and use cases, try out some of these options:\n\n- Install and use NGIAB on your HPC environment\n- Use NGIAB through an SSH connection\n- Contribute to NGIAB/NextGen!"
  },
  {
    "idurl": 170,
    "idtype": "text",
    "order": 21,
    "content": "## Key Points\n\n- NGIAB supports HPC environments through Singularity, not Docker, but the workflow mirrors the local Docker use.\n- Port forwarding is required to use the Data Visualizer through an SSH connection.\n- Community contribution guidelines are available in each repository's GitHub page."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 1,
    "content": "# Key Points"
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction\n\n- The Next Generation Water Resources Modeling Framework (NextGen) advances the National Water Model with flexible, modular, and regionally adaptive hydrologic modeling at national scale.\n- NextGen In A Box (NGIAB) packages the complex NextGen system into an open-source, containerized application for easier access and usability.\n- NGIAB uses Docker and Singularity for portability across local machines, cloud platforms, and HPC systems.\n- NGIAB's multi-layered architecture integrates hydrologic modeling tools, CI/CD pipelines, and supportive technologies and is complemented by a suite of extensions that allow for end-to-end workflows from data acquisition to visualization and evaluation.\n- NGIAB fosters an open ecosystem where researchers, developers, and practitioners actively contribute new models, extensions, and workflows."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 3,
    "content": "## Installation and Setup\n\n- NGIAB simplifies NextGen framework deployment through Docker.\n- Use `guide.sh` for interactive configuration and run execution.\n- Always confirm successful setup by executing provided sample runs."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 4,
    "content": "## Data Preparation\n\n- `ngen-run/` is the standard NextGen run directory, containing the realization files that define models, parameters, and run settings; forcing data; outputs; as well as the spatial hydrofabric.\n- The Data Preprocess tool simplifies preparing data for NextGen by offering a GUI and CLI for selecting catchments and date ranges, subsetting hydrofabric data, generating forcing files, and creating realization files."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 5,
    "content": "## Model Execution\n\n- To execute a NextGen run in NGIAB with full functionality, use `guide.sh` in the NGIAB container.\n- A NextGen run in NGIAB can also be automatically executed post-preprocessing using the Data Preprocess tool."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 6,
    "content": "## Evaluation\n\n- Tools for Exploratory Evaluation in Hydrologic Research (TEEHR) is a Python-based package for hydrologic model evaluation.\n- NGIAB uses TEEHR to assess model performance, comparing predictions against USGS streamflow and NWM data and calculating performance metrics.\n- TEEHR runs automatically with the main `guide.sh` NGIAB script."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 7,
    "content": "## Visualization\n\n- The Data Visualizer (built on the Tethys Platform) provides interactive geospatial maps and time series plots for NextGen model outputs in NGIAB.\n- It integrates seamlessly with NGIAB via `guide.sh` or `ViewOnTethys.sh`.\n- Model outputs reside under `~/ngiab_visualizer`, with metadata stored in `ngiab_visualizer.json`.\n- You can visualize Nexus points, catchment summaries, Troute variables, and TEEHR hydrographs and performance metrics."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 8,
    "content": "## Calibration\n\n- The `ngiab-cal` package is used to calibrate parameters for a NextGen model run.\n- `ngiab-cal` is a command-line tool controlled via a YAML configuration file that determines parameter ranges, time periods, and evaluation metrics."
  },
  {
    "idurl": 171,
    "idtype": "text",
    "order": 9,
    "content": "## Advanced Topics\n\n- NGIAB supports HPC environments through Singularity, not Docker, but the workflow mirrors the local Docker use.\n- Port forwarding is required to use the Data Visualizer through an SSH connection.\n- Community contribution guidelines are available in each repository's GitHub page."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 1,
    "content": "# Reference\n\nLast updated on 2025-04-22"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 2,
    "content": "## Glossary"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 3,
    "content": "### Modeling Terms"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 4,
    "content": "#### National Water Model (NWM)\nHydrologic model used operationally by NOAA's National Weather Service to provide hydrologic predictions for over 3.4 million rivers and streams across the United States"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 5,
    "content": "#### Next Generation Water Resources Modeling Framework (NextGen)\nAn advancement of the NWM, a standards-based and model-agnostic modular set of software tools that applies the Basic Model Interface"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 6,
    "content": "#### NextGen in a Box (NGIAB)\nAn open-source, containerized solution that encapsulates the NextGen framework and essential modeling components into a self-contained, reproducible application"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 7,
    "content": "#### Data Preprocess\nA tool to streamline data preparation for NextGen runs"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 8,
    "content": "#### Tools for Exploratory Evaluation in Hydrologic Research (TEEHR)\nA Python-based package enabling iterative and explorative analysis of hydrologic model performance"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 9,
    "content": "#### Data Visualizer\nA tool that provides a robust environment for geospatial and time series visualization of catchments and nexus points"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 10,
    "content": "#### Forcings\nExternal inputs that drive a model"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 11,
    "content": "#### t-route\nChannel routing model used in the NWM. t-route is used in NextGen."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 12,
    "content": "#### Channel routing\nPredicts the magnitude and shape of a hydrograph as water moves through waterways."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 13,
    "content": "#### ngen\nThe NextGen model engine"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 14,
    "content": "#### Realization file\nA file that configures models and the variables exchanged between them. For example, our realization files define which hydrologic model we are using, over what time period, and over which regions."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 15,
    "content": "#### Hydrofabric\nSee [the hydrofabric documentation](https://noaa-owp.github.io/hydrofabric/articles/01-intro-deep-dive.html)"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 16,
    "content": "#### Conceptual Functional Equivalent (CFE) Model\nA simplified conceptual rainfall-runoff model designed to be functionally equivalent to the NWM. This is an available model in NGIAB."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 17,
    "content": "#### Rainfall-runoff model\nDescribes how rainfall is converted into runoff in a catchment, producing a hydrograph in response to a rainfall event"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 18,
    "content": "#### NoahOWP\nVersion of the Noah-MP land surface model used by NOAA's Office of Water Prediction. This is an available model in NGIAB."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 19,
    "content": "#### Land surface model\nDescribes the exchange of water and energy between the land surface and the atmosphere"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 20,
    "content": "#### Long short-term memory (LSTM) model\nA type of recurrent neural network that excels at learning long-term dependencies in sequential data. This is an available model in NGIAB."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 21,
    "content": "#### Error statistics\nThe difference between the true value and the modeled variable"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 22,
    "content": "#### Skill score\nThe accuracy of a model relative to a reference approach, like random chance or persistence"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 23,
    "content": "#### Persistence\nA forecast that the future condition will be the same as the present condition"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 24,
    "content": "#### Uncertainty quantification\nEstimation of uncertainty and errors in models"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 25,
    "content": "### Computing Terms"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 26,
    "content": "#### Containerization\nEncapsulating applications, their dependencies, and runtime environments into a single, portable unit to address compatibility issues and hardware variation challenges"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 27,
    "content": "#### Docker\nA widely used containerization platform that provides a lightweight, portable, and scalable solution for software deployment"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 28,
    "content": "#### Singularity\nA containerization platform for HPC environments"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 29,
    "content": "#### Basic Model Interface (BMI)\nA standardized system designed to enhance interoperability between models and datasets"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 30,
    "content": "#### Tethys Platform\nPlatform for geospatial web app development"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 31,
    "content": "#### High-Performance Computing (HPC)\nSystems with clusters of parallel processors that process data and perform calculations at high speeds"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 32,
    "content": "#### Job scheduler (e.g. SLURM/PBS/LSF)\nA software that manages which tasks run where and when on an HPC system"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 33,
    "content": "#### Open-source\nSoftware or data that is publicly accessible and can be redistributed or modified"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 34,
    "content": "#### Secure Shell (SSH)\nNetwork protocol that provides secure remote access with encrypted connections"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 35,
    "content": "### Hydrologic Terms"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 36,
    "content": "#### Catchment\nArea from which precipitation collects and drains into an output point like a river or water body. Also known as a watershed"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 37,
    "content": "#### Streamflow\nThe flow of water in a channel, measured in units of volume/time (e.g. cubic meters per second (CMS), cubic feet per second (CFS)). Also known as discharge"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 38,
    "content": "#### Hydrologic model\nA computer-based replication of the natural water cycle"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 39,
    "content": "#### CONUS (Continental United States)\nRefers to the 48 contiguous states and D.C."
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 40,
    "content": "#### Surface runoff\nUnconfined flow of water over the ground surface"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 41,
    "content": "#### Hydrologic signature\nQuantitative metrics that define streamflow, such as maximum flow, baseflow, mean flow, or slope"
  },
  {
    "idurl": 172,
    "idtype": "text",
    "order": 42,
    "content": "#### Hydrograph\nGraph that depicts streamflow at a specific location over time"
  },
  {
    "idurl": 173,
    "idtype": "text",
    "order": 1,
    "content": "# Learner Profiles\n\nWe have lots of different kinds of learners and workshops about NGIAB. This page catalogs resources within this training module to help you to find what you need. For all users, the [Key Points summary page](https://docs.ciroh.org/training-NGIAB-101/key-points.html) and the [Glossary](https://docs.ciroh.org/reference.html) may be useful references."
  },
  {
    "idurl": 173,
    "idtype": "text",
    "order": 2,
    "content": "## General Users\n\nAre you using this training module as a self-teaching tool? Are you working at your own pace? Are you working off of a standard (non-HPC) computer? If so, following the episodes in order is right for you. The [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html) is optional and can be explored if you are interested."
  },
  {
    "idurl": 173,
    "idtype": "text",
    "order": 3,
    "content": "## HPC Users\n\nAre you planning on running NGIAB on an HPC? You can follow the episodes in order, but replace Episode 2: Installation and Setup with the HPC-specific instructions in the [Advanced Topics module](https://docs.ciroh.org/training-NGIAB-101/advanced-topics.html)."
  },
  {
    "idurl": 173,
    "idtype": "text",
    "order": 4,
    "content": "## DevCon 2025 NGIAB Workshop Attendee\n\nAre you joining us for the DevCon 2025 NGIAB workshop? We have prepared two DevCon-specific documents for you.\n\n- [DevCon 2025 Jetstream Instructions](https://docs.ciroh.org/training-NGIAB-101/devcon25js.html) will walk you through the specific steps we will take in this hands-on workshop.\n- [DevCon 2025 Troubleshooting](https://docs.ciroh.org/training-NGIAB-101/troubleshooting.html) contains useful reference material if you are stuck during the workshop.\n\n**These documents are key, since we will not be following the default episodes exactly as written.**\n\nAfter the workshop, feel free to explore the rest of this module to learn more about NGIAB!"
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 1,
    "content": "# DevCon 2025 Troubleshooting\n\nLast updated on 2025-05-23"
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 2,
    "content": "# NGIAB-CloudInfra – DevCon 2025 Workshop Setup Guide\n\nWelcome to the NGIAB Workshop!\n\nThis guide will help you troubleshoot problems."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 3,
    "content": "## Pre-Setup Requirements\n\nYou will be provided with a cloud instance (Jetstream) that already includes all the required tools (Docker and libraries). You will need the login credentials emailed to you before the workshop and a Virtual Network Computing (VNC) client installed."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 4,
    "content": "## Wi-Fi Access\n\nInstructions for connecting to UVM's Guest Wi-Fi are found on the [UVM IT page](https://www.uvm.edu/it/catalog/service/guest-access-guestnet)."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 5,
    "content": "## Troubleshooting Checklist\n\nIf you didn't get the expected output, check these:"
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 6,
    "content": "### Issues with VNC Client or Port Forwarding\n\nIf you are having issues with your VNC client, try using a different service, or port forward to your local web browser. Try both [RealVNC](https://www.realvnc.com/en/connect/download/viewer/) and [TigerVNC](https://tigervnc.org/). Try entering `localhost:5906`, and if that doesn't work, enter `5906`. Lastly, try port forwarding.\n\nIf you continue to have problems, make sure that the port you are using in your `ssh` command is not being used on your local machine. You can use the `netstat` command on all platforms to check which ports are being used."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 7,
    "content": "### No Browser on Web Desktop\n\nIf Firefox is not installed on the Jetstream image's web desktop, run this command:\n\n```bash\nsudo apt install firefox\n```"
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 8,
    "content": "### Invalid Data Preprocess Command\n\nMake sure that you are copying the command as one line, without any newline characters."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 9,
    "content": "### Are You on the Cloud Instance or Local Machine?\n\nSometimes people accidentally run commands on their local machine instead of the cloud instance. Here's how to check:\n\n```bash\nwhoami\n```\n\n✅ If you're on the **Jetstream cloud instance**, you'll see this:\n\n```\nexouser\n```\n\n❌ If it says something like `DESKTOP-ABC123`, `MacBook-Pro.local`, or anything else personal — **you're on your own computer**.\n\n**Fix:** Go back and follow the login instructions provided earlier to connect to your assigned instance before proceeding."
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 10,
    "content": "### Are You in the Right Directory?\n\nBefore running any script, always check your current folder:\n\n```bash\npwd\n```\n\nYou **should see something like**:\n\n```\n/home/exouser/NGIAB_demo/NGIAB-CloudInfra\n```\n\nIf not, move into the folder:\n\n```bash\ncd ~/NGIAB_demo/NGIAB-CloudInfra\n```"
  },
  {
    "idurl": 174,
    "idtype": "text",
    "order": 11,
    "content": "## Need Help?\n\n- Ask a facilitator during the session"
  },
  {
    "idurl": 175,
    "idtype": "text",
    "order": 1,
    "content": "# DevCon2025 Jetstream VM Instructions\n\nLast updated on 2025-05-23"
  },
  {
    "idurl": 175,
    "idtype": "text",
    "order": 2,
    "content": "# Running NGIAB on Jetstream VM for DevCon2025\n\n> **Note**: Before continuing, you will need the SSH credentials for your instance. These have been emailed to you."
  },
  {
    "idurl": 175,
    "idtype": "text",
    "order": 3,
    "content": "## Setting up an SSH session with SSH tunneling\n\nInput the following command into your Unix terminal or Windows command prompt:\n\n```bash\nssh -L [port]:localhost:[port] [username]@[ip.address]\n```\n\ne.g.:\n\n```bash\nssh -L 5906:localhost:5906 exouser@[ip.address]\n```\n\nThe -L flag is used to initiate SSH tunneling, which is necessary to view the NGIAB visualizer.\n\nYour selected port in the above command will differ depending on how you'd prefer to connect to the visualizer.\n\n- **[Recommended]** To open the visualizer via a Virtual Network Computing (VNC) client, use port `5906`.\n  - This approach requires a VNC client to be installed on your computer. [RealVNC](https://www.realvnc.com/en/connect/download/viewer/) and [TigerVNC](https://tigervnc.org/) are common choices.\n- To open the visualizer directly in your web browser, use a port such as `80` or `8080`.\n  - You may receive a message such as \"Could not request local forwarding.\" If this happens, you will need to select a different port.\n\nWhen logging in for the first time, you may be asked whether you'd like to trust the host. Type 'yes' to do so.\n\nAfter that, simply type in your password to gain access to your instance's terminal."
  },
  {
    "idurl": 175,
    "idtype": "text",
    "order": 4,
    "content": "## Task 1: Running NGIAB With Prepared Data\n\n1. Run the following command to preprocess data:\n\n```bash\nuvx --from ngiab_data_preprocess cli -i gage-10154200 -sfr --start 2017-09-01 --end 2018-09-01 --source aorc\n```\n\nUse the default output folder given.\n\n2. Run the following commands to clone the NGIAB-CloudInfra repo and run guide script:\n\n```bash\nmkdir NGIAB_demo\ncd NGIAB_demo\ngit clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\ncd NGIAB-CloudInfra\n./guide.sh\n```\n\n3. When prompted, enter the following input data directory path:\n\n```\n/home/exouser/ngiab_preprocess_output/gage-10154200\n```\n\n4. When prompted about which Docker image to use, select Option 1 (use existing Docker image).\n\n5. When prompted about which run option to use, select Option 2 (run NextGen in parallel mode).\n\n6. When prompted to redirect command output to `/dev/null`, select no. This allows you to see details about your NextGen run.\n\n7. After the NextGen run is complete, you will be asked \"Would you like to continue?\". Select Option 2 (exit), which will allow you to progress to the TEEHR evaluation.\n\n8. Run the TEEHR evaluation when prompted. When given the option to select an image tag, choose `x86`, and choose option 1 (use existing TEEHR image).\n\n9. After the TEEHR evaluation is complete, run the Tethys visualizer when prompted. When asked about the image, choose the default latest image. Note: the visualizer may take a while to load. When asked which port to use, use the default port (80).\n   - If you are opening the visualizer in a VNC client, first connect to `localhost:5906` using the password emailed to you. Then, open the link provided by the console output in the remote desktop's web browser.\n   - If you are opening the visualizer in your local web browser, simply enter the link provided by the console output.\n\n10. Enter `Ctrl+C` in the terminal to stop the visualizer when you are done."
  },
  {
    "idurl": 175,
    "idtype": "text",
    "order": 5,
    "content": "## Task 2: Running NGIAB with Your Own Data\n\n1. Use the Data Preprocess tool's graphical user interface (map app) to select your favorite catchments and time period that you would like to perform a NextGen run on. To be able to use TEEHR, you must select a region that contains USGS gages. You can show or hide USGS gages in the map app by toggling the \"Show/Hide gages\" button in the upper left.\n\n![Data Preprocess tool map view](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-4.png)\n\nFigure 1: Example view from the Data Preprocess tool. The highlighted region (light orange area; downstream-most basin in pink) represents the specific study basin, illustrating the river network (blue lines), sub-basins (orange), and surrounding USGS gaging stations (black dots).\n\n```bash\nuvx --from ngiab_data_preprocess map_app\n```\n\nTo view the map app in your VNC viewer or your local web browser, use one of the IP addresses listed after the map app is started.\n\n2. Copy the given command. Exit out of the Data Preprocess tool in the terminal using `Ctrl+C` and run the copied command to preprocess data. If you include the `--run` tag, the Data Preprocessor will automatically execute a NextGen run.\n\n3. If you included the `--run` tag, you will need to run the `runTeehr.sh` and `viewOnTethys.sh` scripts separately in order to use TEEHR and the Data Visualizer.\n\n```bash\nchmod +x runTeehr.sh viewOnTethys.sh\n./runTeehr.sh\n```\n\n```bash\n./viewOnTethys.sh\n```\n\nWhen you run `viewOnTethys.sh`, make sure to select a different port (we like 8081). Use your VNC viewer to access the visualization at the given link.\n\nIf you did not include the `--run` tag, you can run `guide.sh` as described in Task 1.\n\n```bash\n./guide.sh\n```\n\n4. Experiment with different basins or options as much as you'd like!"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 1,
    "content": "# Summary and Schedule\n\nThis is a new lesson built with [The Carpentries Workbench](https://carpentries.github.io/sandpaper-docs)."
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 2,
    "content": "## Workshop Schedule"
  },
  {
    "idurl": 176,
    "idtype": "table",
    "order": 3,
    "content": "| Duration | Episode | Questions |\n|----------|---------|-----------|\n| | Setup Instructions | Download files required for the lesson |\n| Duration: 00h 00m | 1. [Introduction](https://docs.ciroh.org/training-NGIAB-101/instructor/introduction.html) | What is the NextGen Framework?<br>What is NextGen in a Box (NGIAB)?<br>What is containerization?<br>Why should I use NGIAB? |\n| Duration: 00h 15m | 2. [Installation and Setup](https://docs.ciroh.org/training-NGIAB-101/instructor/installation.html) | How do I install and set up NGIAB?<br>What are the prerequisites for running NGIAB?<br>How do I verify my installation? |\n| Duration: 00h 27m | 3. [Data Preparation](https://docs.ciroh.org/training-NGIAB-101/instructor/data-preparation.html) | How should I prepare my run directory?<br>What is the Data Preprocess tool? |\n| Duration: 01h 17m | 4. [Model Execution](https://docs.ciroh.org/training-NGIAB-101/instructor/model-execution.html) | How do I execute a NextGen run? |\n| Duration: 01h 27m | 5. [Evaluation](https://docs.ciroh.org/training-NGIAB-101/instructor/evaluation.html) | How do I use Tools for Exploratory Evaluation in Hydrologic Research (TEEHR) to evaluate my NextGen models in NGIAB? |\n| Duration: 01h 42m | 6. [Visualization](https://docs.ciroh.org/training-NGIAB-101/instructor/visualization.html) | How do I visualize my NextGen runs? |\n| Duration: 01h 57m | 7. [Calibration](https://docs.ciroh.org/training-NGIAB-101/instructor/calibration.html) | How do I calibrate parameters for a NextGen run? |\n| Duration: 02h 22m | 8. [Advanced Topics](https://docs.ciroh.org/training-NGIAB-101/instructor/advanced-topics.html) | How do I use NGIAB on an high-performance computing (HPC) system?<br>How do I use the Data Visualizer through an SSH connection?<br>How can I contribute to NGIAB? |\n| Duration: 03h 27m | Finish | |"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 4,
    "content": "The actual schedule may vary slightly depending on the topics and exercises chosen by the instructor."
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 5,
    "content": "## Welcome\n\nWelcome to the NextGen In A Box (NGIAB) 101 training module!"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 6,
    "content": "### Source Code and Documentation\n\nThe source code for NextGen In A Box (NGIAB) is found at [our GitHub repository](https://github.com/CIROH-UA/NGIAB-CloudInfra). The documentation for NGIAB and its extensions are found on the [CIROH docs](https://docs.ciroh.org/docs/products/Community%20Hydrologic%20Modeling%20Framework/).\n\nTo dive right into NGIAB as quickly as possible, follow the quick start guide on our [GitHub repository](https://github.com/CIROH-UA/NGIAB-CloudInfra), or head straight to our second episode, Installation. If you are interested in taking your time and learning in-depth about the features of NGIAB, follow the episodes in this module.\n\nFor all users, the [Key Points summary page](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/key-points.html) and the [Glossary](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/reference.html) may be useful references. The [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/advanced-topics.html) is optional, but may contain useful content depending on your specific use case.\n\nNote: This module requires you to use Bash scripting in your command line and Git. All commands will be given in this module. However, if you are interested in learning more, you can refer to this [quick Bash tutorial by Ubuntu](https://ubuntu.com/tutorials/command-line-for-beginners#1-overview), [this basic Git tutorial in the Git documentation](https://git-scm.com/docs/gittutorial), and this [Git workflow tutorial by CIROH](https://github.com/AlabamaWaterInstitute/data_access_examples/blob/main/doc/GIT_USAGE.md)."
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 7,
    "content": "### DevCon 2025 NGIAB Workshop Attendees\n\nAre you joining us for the DevCon 2025 NGIAB workshop? We have prepared two DevCon-specific documents for you.\n\n- [DevCon 2025 Jetstream Instructions](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/devcon25js.html) will walk you through the specific steps we will take in this hands-on workshop.\n- [DevCon 2025 Troubleshooting](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/troubleshooting.html) contains useful reference material if you are stuck during the workshop.\n\n**These documents are key, since we will not be following the default episodes exactly as written.**\n\nAfter the workshop, feel free to explore the rest of this module to learn more about NGIAB!"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 8,
    "content": "### HPC Users\n\nAre you planning on running NGIAB on an HPC? You can follow the episodes in order, but replace Episode 2: Installation and Setup with the HPC-specific instructions in the [Advanced Topics module](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/advanced-topics.html)."
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 9,
    "content": "## System Requirements\n\nThe Installation episode will walk you through the steps to install Windows Subsystem for Linux (WSL), Docker, NGIAB, and retrieve sample data sets. This page summarizes system requirements."
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 10,
    "content": "### Windows\n- **Software:** Docker, Git, WSL, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 11,
    "content": "### MacOS\n- **Software:** Docker, Git, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 176,
    "idtype": "text",
    "order": 12,
    "content": "### Linux\n- **Software:** Docker, Git, Astral UV\n- **Recommended Minimum RAM:** 8 GB"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 1,
    "content": "# Instructor Notes"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 2,
    "content": "## DevCon 2025 NGIAB Workshop Instructors\n\nWe have prepared two DevCon-specific documents for you. [DevCon 2025 Jetstream Instructions](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/devcon25js.html) will walk learners through the specific steps we will take in this hands-on workshop. [DevCon 2025 Troubleshooting](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/troubleshooting.html) contains useful reference material for learners if they are stuck during the workshop."
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 3,
    "content": "## Other Instructors\n\nFeel free to skip around this module based on learner backgrounds and use cases. The [Introduction episode](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/introduction.html) contains heavy text-based exposition that may not be engaging in a hands-on workshop environment. The [Advanced Topics episode](https://docs.ciroh.org/training-NGIAB-101/training-NGIAB-101/advanced-topics.html) is optional for most learners, but you may want to highlight specific sections depending on your audience."
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 4,
    "content": "## Episode Notes"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 5,
    "content": "### Introduction\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 6,
    "content": "### Installation and Setup\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 7,
    "content": "### Data Preparation\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 8,
    "content": "### Model Execution\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 9,
    "content": "### Evaluation\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 10,
    "content": "### Visualization\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 11,
    "content": "### Calibration\n(No specific notes provided)"
  },
  {
    "idurl": 177,
    "idtype": "text",
    "order": 12,
    "content": "### Advanced Topics\n(No specific notes provided)"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 1,
    "content": "# Extract All Images\n\nThis page contains all images/figures used in the NGIAB 101 training module, organized by episode."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 2,
    "content": "## Introduction"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 3,
    "content": "### Figure 1\n![NGIAB Containerization Architecture](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-1.png)\n\n**Figure 1:** Architecture of the NGIAB, highlighting its core modeling foundation, CI/CD pipelines, containerized tools and supporting technologies.\n\n*Description:* A concentric circle diagram titled \"NGIAB Containerization Architecture.\" It consists of four nested layers representing different components. At the center is \"1. Core Framework\" in gray, symbolizing the core hydrological modeling framework. Surrounding it is \"2. CI/CD Pipeline\" in green, representing tools that facilitate rapid development and reliability. The next layer is \"3. NGIAB Containerization\" in blue, indicating simplified access to modeling tools. The outermost layer is \"4. Technologies & Methods\" in dark blue, representing supportive tools and practices. Labels on the left of the diagram describe the increasing level of support from the core outward."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 4,
    "content": "### Figure 2\n![NGIAB model execution process](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-2.png)\n\n**Figure 2:** Workflow of data acquisition, model execution, evaluation, and results visualization.\n\n*Description:* A flowchart diagram showing the NGIAB model execution process. The central box labeled \"NGIAB Model Execution\" is connected to three components. To the left is a yellow-green box labeled \"Data Preprocess,\" with three subcomponents listed: \"GPKG Sub-setting,\" \"Realization,\" and \"Forcing.\" To the right, two boxes are connected to the center: a purple box labeled \"TEEHR Evaluation\" and a green box labeled \"Data Visualizer.\" Dashed arrows indicate the flow of data between preprocessing, model execution, and evaluation/visualization."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 5,
    "content": "### Figure 3\n![Provo River network and basin boundaries](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-4.png)\n\n**Figure 3:** Map showing the drainage basin used as our demonstration case, the Provo River near Woodland, UT (Gage-10154200). This view shows the NGIAB interactive preprocessing tool. The highlighted region (light orange area; downstream-most basin in pink) represents the specific study basin, illustrating the river network (blue lines), sub-basins (orange), and surrounding USGS gaging stations (black dots)."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 6,
    "content": "### Figure 4\n![NextGen in a Box Visualizer web interface](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-5.png)\n\n**Figure 4:** Map showing the geospatial visualization using the Data Visualizer for a selected outlet point as well as displaying a time series plot between observed (labeled \"USGS\"; blue line) and simulated (labeled \"ngen\"; orange line) with the performance metrics (Kling-Gupta Efficiency (KGE), Nash-Sutcliffe Efficiency (NSE), and relative bias). These metrics assess how closely simulated results match observed data. The Visualizer can also show the performance of the NWM 3.0 compared to the observed time series."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 7,
    "content": "## Data Preparation"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 8,
    "content": "### Figure 1\n![USGS National Map](https://docs.ciroh.org/training-NGIAB-101/fig/fig3-1.png)\n\n**Figure 1:** Map showing an example drainage basin. View from the USGS National Map.\n\n*Description:* A screenshot of the USGS National Map centered on the Provo River network in Utah, showing streamflow and watershed data. A blue map marker identifies a Monitoring Location. A red dot marks an Active Monitoring Location farther downstream. The Upstream Basin is shaded in grey, while Upstream Flowlines and Downstream Flowlines are highlighted in dark and light blue, respectively. A scale bar in the bottom right shows distances of 5 kilometers and 3 miles. A map legend in the lower right corner explains the color codes for flowlines and monitoring locations."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 9,
    "content": "### Figure 2\n![Data Preprocess tool view](https://docs.ciroh.org/training-NGIAB-101/fig/fig1-4.png)\n\n**Figure 2:** Map showing an example drainage basin. View from the Data Preprocess tool. The highlighted region (light orange area; downstream-most basin in pink) represents the specific study basin, illustrating the river network (blue lines), sub-basins (orange), and surrounding USGS gaging stations (black dots)."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 10,
    "content": "## Evaluation"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 11,
    "content": "### Figure 1\n![Hydrograph comparison](https://docs.ciroh.org/training-NGIAB-101/fig/fig5-1.png)\n\n**Figure 1:** Comparison of the NextGen-based model (labeled \"ngen\"; blue line) and the NWM time series (labeled \"nwm30_retrospective\"; orange line) for the same location. The figure is automatically generated by the TEEHR-based analysis that accompanies the `guide.sh` script included with NGIAB and is named `timeseries_plot_streamflow_hourly_inst.html` in the `teehr` folder.\n\n*Description:* A hydrograph spanning years 2017-2022. The x-axis is labeled \"Datetime\", and the y-axis is labeled \"streamflow_hourly inst [m^3/s]\". The blue line represents the NextGen run (labeled \"ngen\"), and the orange line represents the NWM 3.0 time series (labeled \"nwm30_retrospective\"). A legend is in the upper-right corner explaining the colors of these lines."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 12,
    "content": "## Visualization"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 13,
    "content": "### Figure 1\n![NGIAB and DataStream Visualizer](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-1.png)\n\n**Figure 1:** A map showing the geospatial visualization using the Data Visualizer within the Tethys framework for an entire study area (Provo River near Woodland, UT)."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 14,
    "content": "### Figure 2\n![Multiple outputs dropdown](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-2.png)\n\n**Figure 2:** NGIAB Visualizer dropdown for multiple outputs"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 15,
    "content": "### Figure 3\n![Nexus points time series](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-3.png)\n\n**Figure 3:** NGIAB Visualizer time series visualization from Nexus points"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 16,
    "content": "### Figure 4\n![Troute variables](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-4.png)\n\n**Figure 4:** NGIAB Visualizer time series visualization from Troute variables"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 17,
    "content": "### Figure 5\n![Catchments time series](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-5.png)\n\n**Figure 5:** NGIAB Visualizer time series visualization for Catchments"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 18,
    "content": "### Figure 6\n![TEEHR comparison visualization](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-6.png)\n\n**Figure 6:** A map showing the geospatial visualization using the Data Visualizer within the Tethys framework for a selected outlet nexus point as well as displaying a time series plot between observed (labeled \"USGS\"; blue line) and simulated (labeled \"ngen\"; orange line)"
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 19,
    "content": "### Figure 7\n![Performance metrics table](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-7.png)\n\n**Figure 7:** NGIAB Visualizer performance metrics (KGE, NSE, and relative bias). The Visualizer can also show the performance of the NWM 3.0 compared to the observed time series."
  },
  {
    "idurl": 178,
    "idtype": "text",
    "order": 20,
    "content": "### Figure 8\n![DataStream visualization](https://docs.ciroh.org/training-NGIAB-101/fig/fig6-8.png)\n\n**Figure 8:** NGIAB Visualizer Visualization of DataStream Data"
  }
]