[
    {
      "metadata": {
        "idurl": 1,
        "title": "ciroh-portal",
        "source": "https://github.com/CIROH-UA/ciroh-portal",
        "section_header": "# ciroh-portal"
      },
      "content": "# Project Title: **ciroh-portal**  \n\n## Project Objective  \nThe **CIROH Portal** is a static website built using **Docusaurus**, designed to serve as a documentation or informational hub. Its main goal is to provide a modern, easily maintainable platform for hosting CIROH-related content and resources. It simplifies content creation, organization, and deployment for developers and contributors maintaining the CIROH ecosystem.\n\n## Core Functionalities  \n- Uses **Docusaurus** for generating a modern, responsive static website.  \n- Provides **live local development** with hot-reloading for rapid content updates.  \n- Supports **automated build and deployment** to GitHub Pages or other static hosting services.  \n- Allows **SSH and non-SSH deployment options** for flexibility in publishing.\n\n## Technical Stack  \n- **Language:** JavaScript  \n- **Framework:** [Docusaurus](https://docusaurus.io/)  \n- **Package Manager:** Yarn  \n- **Deployment Options:** GitHub Pages, SSH-based deployment\n\n## Setup and Usage  \n1. **Install dependencies:** Run `yarn`.  \n2. **Local development:** Use `yarn start` to launch a development server with live reload.  \n3. **Build static site:** Execute `yarn build` to generate static files in the `build` directory.  \n4. **Deploy:**  \n   - With SSH: `USE_SSH=true yarn deploy`  \n   - Without SSH: `GIT_USER=<Your GitHub username> yarn deploy`\n\n## Project Context & Domain  \n- **Domain:** Web development / Documentation portal.  \n- **Affiliation:** CIROH (Consortium of Universities for the Advancement of Hydrologic Science, Inc.).  \n- **Purpose:** Serve as a centralized online portal for CIROH’s documentation, community resources, or related content.\n\n## Input / Output  \n**Input:**  \n- Markdown documentation files and Docusaurus configuration settings.  \n**Output:**  \n- A fully built static website generated in the `build` directory, ready for deployment to web hosting platforms.",
      "summary": "The ciroh-portal repository implements CIROH’s documentation hub as a Docusaurus-based static site, optimizing for maintainability, contributor workflow, and rapid publishing. Authors write content in Markdown, iterate locally with hot reload, and deploy via GitHub Pages or SSH-based automation. The stack uses JavaScript with Yarn and a modern doc toolchain that supports modular pages, versioned docs, and consistent styling. Operationally, the portal centralizes CIROH technical content and community resources, lowering onboarding friction and improving discoverability for research artifacts, APIs, and workflows. This repository is the anchor for CIROH’s developer experience, providing a stable, scalable front door for documentation while integrating clean CI/CD pathways for continuous updates.",
      "keywords": ["Docusaurus", "documentation", "static site", "CI/CD", "GitHub Pages", "Yarn", "developer portal", "Markdown", "technical docs", "CIROH"],
      "entities": ["CIROH", "Docusaurus", "GitHub Pages", "Yarn"]
    },
    {
      "metadata": {
        "idurl": 2,
        "title": "api-nwm-gcp",
        "source": "https://github.com/CIROH-UA/api-nwm-gcp",
        "section_header": "# api-nwm-gcp"
      },
      "content": "# Project Title: **api-nwm-gcp**  \n\n## Project Objective  \nThe **api-nwm-gcp** repository provides a REST API backed by National Water Model (NWM) data, developed on Google Cloud Platform (GCP). It enables users to access hydrologic and geospatial data through cloud-based infrastructure and deployment.\n\n## Core Functionalities  \n- REST API for accessing National Water Model (NWM) data.  \n- Cloud infrastructure setup using Terraform and Google Cloud SDK.  \n- Automated deployment using Cloud Build and Cloud Run.  \n- API management with Google API Gateway.  \n- Example API endpoints for geometry, analysis-assimilation data, and forecast data.  \n- Includes infrastructure configuration for service accounts, permissions, and Docker image storage in Artifact Registry.\n\n## Technical Stack  \n- **Cloud Services:** Google Cloud Platform (Cloud Run, API Gateway, Cloud Build, Artifact Registry).  \n- **Infrastructure Management:** Terraform.  \n- **Languages/Tools:** Docker, gcloud CLI, terraform CLI.  \n- **Deployment Configuration:** cloudbuild.yaml, deployment.tfvars.\n\n## Setup and Usage  \n### Infrastructure Setup  \n1. Authenticate with gcloud:  \n   ```bash\n   gcloud auth application-default login\n   ```  \n2. Update variables in `deployment.tfvars` with project-specific details.  \n3. Initialize Terraform:  \n   ```bash\n   terraform init\n   ```  \n4. (Optional) Preview deployment plan:  \n   ```bash\n   terraform plan -var-file=\"deployment.tfvars\"\n   ```  \n5. Deploy infrastructure:  \n   ```bash\n   terraform apply -var-file=\"deployment.tfvars\"\n   ```  \n6. (Optional) Destroy infrastructure:  \n   ```bash\n   terraform destroy -var-file=\"deployment.tfvars\"\n   ```\n\n### Deployment  \n#### Initial Docker Build and Cloud Run Deploy  \n1. Ensure Cloud Build service account has **Cloud Run Admin** and **Service Account User** roles enabled.  \n2. Submit a Cloud Build job:  \n   ```bash\n   gcloud builds submit --config cloudbuild.yaml\n   ```  \n3. Modify Cloud Run parameters in `cloudbuild.yaml` if necessary.\n\n#### Deploy API Gateway  \n1. Create the API:  \n   ```bash\n   gcloud api-gateway apis create API_ID --project=PROJECT_ID\n   ```  \n2. Create the API config:  \n   ```bash\n   gcloud api-gateway api-configs create CONFIG_ID --api=API_ID --openapi-spec=API_DEFINITION --project=PROJECT_ID --backend-auth-service-account=SERVICE_ACCOUNT_EMAIL\n   ```  \n3. Deploy an API config to a gateway:  \n   ```bash\n   gcloud api-gateway gateways create GATEWAY_ID --api=API_ID --api-config=CONFIG_ID --location=GCP_REGION --project=PROJECT_ID\n   ```\n\n### Example Use  \n#### Geometry  \n```bash\ncurl -H \"x-api-key: ${API_KEY}\" \"${NWM_API}/geometry?lon=-121.76&lat=37.70\"\n```  \n#### Analysis Assimilation Data  \n```bash\ncurl -H \"x-api-key: ${API_KEY}\" \"${NWM_API}/analysis-assim?start_time=2018-09-17&end_time=2023-05-01&comids=15059811&output_format=csv\"\n```  \n#### Forecast Data  \n```bash\ncurl -H \"x-api-key: ${API_KEY}\" \"${NWM_API}/forecast?forecast_type=long_range&reference_time=2023-05-01&ensemble=0&comids=15059811&output_format=csv\"\n```  \n\n## Project Context & Domain  \n- **Domain:** Hydrology / Cloud Infrastructure / API Development.  \n- **Affiliation:** Not specified.  \n- **Purpose:** Provide cloud-based access to National Water Model data through a REST API deployed on Google Cloud Platform.\n\n## Input / Output  \n**Input:**  \n- API parameters such as longitude/latitude, forecast type, time range, ensemble ID, and COMIDs.  \n**Output:**  \n- JSON or CSV-formatted responses containing geometry, analysis-assimilation, or forecast data from the National Water Model.",
      "summary": "api-nwm-gcp exposes National Water Model (NWM) geometry, analysis-assimilation, and forecast data via a REST API deployed on Google Cloud Platform. The repository codifies cloud-native operations with Cloud Run, API Gateway, and Cloud Build, while Terraform manages infrastructure as code and artifact storage in Artifact Registry. It documents end-to-end deployment steps and provides cURL examples for core endpoints, enabling reproducible provisioning and programmatic data access. This project operationalizes hydrologic data delivery for downstream GIS, modeling, and decision support systems, emphasizing automation, security, and scalability in a standardized DevOps workflow.",
      "keywords": ["National Water Model", "NWM", "REST API", "Google Cloud", "Cloud Run", "API Gateway", "Cloud Build", "Terraform", "hydrology", "DevOps"],
      "entities": ["CIROH", "Google Cloud Platform", "National Water Model", "Terraform", "Artifact Registry"]
    },
    {
      "metadata": {
        "idurl": 3,
        "title": "awi-ciroh-image",
        "source": "https://github.com/CIROH-UA/awi-ciroh-image",
        "section_header": "# awi-ciroh-image"
      },
      "content": "# Project Title: **awi-ciroh-image**  \n\n## Project Objective  \nThe **awi-ciroh-image** repository provides a **template for creating hub user images** for 2i2c JupyterHub environments. It allows users to build and test Docker images automatically using GitHub Actions and `repo2docker`, which can then be published to **Quay.io**. The project enables easy replication of computational environments for research or educational use within JupyterHub.\n\n## Core Functionalities  \n- Acts as a **template repository** for hub user images.  \n- Uses **GitHub Actions** to automatically build and push images to **Quay.io**.  \n- Supports **Binder testing** for pull requests to validate builds interactively.  \n- Allows environment customization via **environment.yml** for package management.  \n- Compatible with the 2i2c JupyterHub image-building workflow.\n\n## Technical Stack  \n- **Image builder:** [repo2docker](https://repo2docker.readthedocs.io/)  \n- **Automation:** GitHub Actions  \n- **Container registry:** Quay.io  \n- **Environment specification:** Conda (`environment.yml`)  \n- **Programming languages:** Not specified.  \n- **Dependencies:** Not specified.\n\n## Setup and Usage  \nInstructions provided in the README:  \n1. Use the repository as a **template** to create a new hub image repository.  \n2. The image is **automatically built** and **pushed to Quay.io** using GitHub Actions.  \n3. Pull requests build temporary images for testing through **Binder**.  \n4. The base documentation for using this workflow can be found in the [2i2c hub user image template guide](https://docs.2i2c.org/en/latest/admin/howto/environment/hub-user-image-template-guide.html).  \nFurther details on setup, local testing, or Hub configuration are **not specified**.\n\n## Project Context & Domain  \n- **Domain:** JupyterHub environment management / Reproducible computing.  \n- **Affiliation:** Not specified.  \n- **Purpose:** Provide a standard template for building and deploying hub user images within the 2i2c JupyterHub infrastructure.\n\n## Input / Output  \n**Input:**  \n- Configuration files (e.g., `environment.yml`).  \n- GitHub repository actions for image building.  \n**Output:**  \n- Built Docker images automatically published to **Quay.io**.  \n- Binder test builds for pull request validation.",
      "summary": "awi-ciroh-image is a template for producing reproducible user environments for 2i2c JupyterHub. It automates image builds with GitHub Actions and repo2docker and publishes to Quay.io, enabling consistent, shareable computational stacks across research cohorts and classrooms. Binder integration validates PRs through ephemeral, interactive test builds. Environment definition via Conda (environment.yml) supports transparent dependency control. This pattern operationalizes environment-as-code for CIROH hubs, reducing setup friction and ensuring consistent execution contexts for notebooks, tutorials, and model workflows.",
      "keywords": ["JupyterHub", "2i2c", "repo2docker", "GitHub Actions", "Quay.io", "Binder", "Conda", "reproducible computing", "images", "CI/CD"],
      "entities": ["CIROH", "2i2c", "Quay.io", "Binder", "GitHub Actions"]
    },
    {
      "metadata": {
        "idurl": 4,
        "title": "bmi-burn-lstm",
        "source": "https://github.com/CIROH-UA/bmi-burn-lstm",   
        "section_header": "# bmi-burn-lstm"
      },
      "content": "# Project Title: **bmi-burn-lstm**  \n\n## Project Objective  \n**bmi-burn-lstm** is a Rust implementation of a **Basic Model Interface (BMI)** adapter for LSTM-based streamflow prediction. It provides a drop-in replacement for Python BMI LSTM implementations found in repositories such as [CIROH-UA/lstm](https://github.com/CIROH-UA/lstm), [NOAA-OWP/lstm](https://github.com/NOAA-OWP/lstm), and [jmframe/lstm](https://github.com/jmframe/lstm).\n\n## Core Functionalities  \n- Native Rust implementation using the [Burn](https://burn.dev/) deep learning framework.  \n- Compatible with existing PyTorch model weights from `jmframe/lstm` and **NextGen-In-A-Box**.  \n- Drop-in replacement for Python BMI LSTM adapter.  \n- Supports ensemble model configurations.  \n- Built on [bmi-rs](https://github.com/aaraney/bmi-rs) Rust BMI bindings.  \n- Outputs match those of the Python BMI LSTM implementation.\n\n## Technical Stack  \n- **Language:** Rust (2024 edition).  \n- **Deep Learning Framework:** Burn.  \n- **Bindings:** bmi-rs (Rust BMI bindings).  \n- **Dependencies:**  \n  - Rust 2024 edition.  \n  - Local clone of [bmi-rs](https://github.com/aaraney/bmi-rs).  \n  - [Astral UV](https://docs.astral.sh/uv/) installed and available on system path.  \n- **Development Status:** Active development.\n\n## Setup and Usage  \n1. **Install Dependencies:**  \n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   curl https://sh.rustup.rs -sSf | bash -s -- -y && . \"$HOME/.cargo/env\"\n   ```  \n2. **Clone Required Repositories:**  \n   ```bash\n   git clone https://github.com/aaraney/bmi-rs\n   git clone https://github.com/ciroh-ua/bmi-burn-lstm\n   ```  \n3. **Build Project:**  \n   ```bash\n   cd bmi-burn-lstm\n   cargo build --release\n   ```  \n   The compiled shared object is located at:  \n   `target/release/libbmi_burn_lstm.so`\n\n## Model Compatibility  \n- Trained models from `jmframe/lstm`.  \n- Models included in **NextGen-In-A-Box**.  \n- Any PyTorch LSTM model following the **neuralhydrology** architecture (`1 layer LSTM + 1 linear layer`).  \n- Automatic weight conversion from PyTorch to Burn format on first run.\n\n## NextGen Realization Example  \nExample configuration for integrating this model with NextGen:  \n```json\n{ \"global\": { \"formulations\": [ { \"name\": \"bmi_multi\", \"params\": { \"name\": \"bmi_multi\", \"model_type_name\": \"lstm\", \"forcing_file\": \"\", \"init_config\": \"\", \"allow_exceed_end_time\": true, \"main_output_variable\": \"land_surface_water__runoff_depth\", \"modules\": [ { \"name\": \"bmi_c\", \"params\": { \"name\": \"bmi_c\", \"model_type_name\": \"bmi_rust\", \"init_config\": \"./config/cat_config/lstm/{{id}}.yml\", \"allow_exceed_end_time\": true, \"main_output_variable\": \"land_surface_water__runoff_depth\", \"uses_forcing_file\": false, \"registration_function\": \"register_bmi_lstm\", \"library_file\": \"/dmod/shared_libs/libbmi_burn_lstm.so\" } } ] } } ], \"forcing\": { \"path\": \"./forcings/forcings.nc\", \"provider\": \"NetCDF\", \"enable_cache\": false } }, \"time\": { \"start_time\": \"2010-01-01 00:00:00\", \"end_time\": \"2011-01-01 00:00:00\", \"output_interval\": 3600 }, \"routing\": { \"t_route_config_file_with_path\": \"./config/troute.yaml\" }, \"remotes_enabled\": false, \"output_root\": \"./outputs/ngen\" }\n```\n\n## NGIAB Patch Example  \nExample Dockerfile for integrating the adapter into NextGen-In-A-Box:  \n```Dockerfile\nFROM awiciroh/ciroh-ngen-image AS build\nRUN dnf install -y gcc clang git\nRUN curl https://sh.rustup.rs -sSf | bash -s -- -y\nRUN echo 'source $HOME/.cargo/env' >> $HOME/.bashrc\nWORKDIR /build\nRUN git clone https://github.com/aaraney/bmi-rs\nRUN git clone https://github.com/ciroh-ua/bmi-burn-lstm\nWORKDIR /build/bmi-burn-lstm\nRUN cargo build --release\n\nFROM awiciroh/ciroh-ngen-image AS final\nCOPY --from=build /build/bmi-burn-lstm/target/release/libbmi_burn_lstm.so /dmod/shared_libs/libbmi_burn_lstm.so\n```\nBuild command:  \n```bash\ndocker build -t ngiab -f Dockerfile .\n```\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Streamflow prediction / Model integration.  \n- **Affiliation:** Not specified.  \n- **Purpose:** Provide a faster Rust-based BMI adapter for LSTM hydrologic models.\n\n## Input / Output  \n**Input:**  \n- Trained PyTorch LSTM model weights.  \n- NextGen configuration files for model integration.  \n- Forcing and initialization data defined in configuration.  \n**Output:**  \n- Rust-based shared object file (`libbmi_burn_lstm.so`).  \n- Model outputs consistent with the Python BMI LSTM implementation.\n\n## License  \nLicense information: Not specified.",
      "summary": "bmi-burn-lstm provides a Rust BMI adapter for LSTM-based streamflow prediction, enabling performance-oriented deployments while maintaining parity with existing Python BMI implementations. Built on Burn and bmi-rs, it supports ensemble configurations and automatic conversion of PyTorch weights that follow the neuralhydrology architecture. The repo includes clear build instructions, NextGen realization JSON, and an NGIAB-compatible Docker patch to integrate the shared library into established hydrologic workflows. This adapter advances interoperability and execution speed for operational or research settings that require BMI-compliant LSTM models.",
      "keywords": ["BMI", "LSTM", "Rust", "Burn", "streamflow prediction", "NextGen", "NGIAB", "neuralhydrology", "PyTorch weights", "hydrology"],
      "entities": ["CIROH", "NextGen", "NGIAB", "Burn", "bmi-rs"]
    },
    {
      "metadata": {
        "idurl": 5,
        "title": "CAMELS_data_sample",
        "source": "https://github.com/CIROH-UA/CAMELS_data_sample",
        "section_header": "# CAMELS_data_sample"
      },
      "content": "# Project Title: **CAMELS_data_sample**  \n\n## Project Objective  \nThe **CAMELS_data_sample** repository provides sample data for learning to use the CAMELS dataset with various modeling platforms. It is designed solely for educational and demonstration purposes and is not intended for research or operational use.\n\n## Core Functionalities  \n- Includes **sample basins** to replicate the original CAMELS 2014 directory structure.  \n- Provides **streamflow** and **forcing files** organized under directories numbered 01–18, with one representative basin per directory.  \n- Contains **sample years 1993–2013** to reduce file size and allow GitHub hosting.  \n- Offers a simplified dataset for testing neural hydrology or similar modeling workflows.\n\n## Technical Stack  \n- **Languages:** Not specified.  \n- **Dependencies:** Not specified.  \n- **Data Source:** Full CAMELS dataset available from [UCAR](https://ral.ucar.edu/solutions/products/camels).\n\n## Setup and Usage  \n- The repository is pre-structured for immediate use with modeling platforms compatible with CAMELS data.  \n- To access complete datasets, users should retrieve the full CAMELS data from UCAR.\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Data Modeling / Machine Learning.  \n- **Affiliation:** Not specified.  \n- **Purpose:** Provide a small, easy-to-access version of the CAMELS dataset for testing and learning workflows.\n\n## Input / Output  \n**Input:** Not specified.  \n**Output:** Sample CAMELS-format data files representing basins and years for hydrological modeling.\n\n## Citation Guidelines  \nDo **not** cite this repository. Users should cite the full CAMELS dataset or CARAVAN instead.",
      "summary": "CAMELS_data_sample offers a compact, educational subset of the CAMELS dataset that mirrors the 2014 directory structure and includes representative basins with streamflow and forcing data for 1993–2013. Designed for tutorials and quick-start workflows, it enables testing of neural hydrology pipelines and other modeling frameworks without the storage and bandwidth demands of the full dataset. This repo is intentionally non-research-grade and directs users to obtain authoritative data from UCAR for production analyses. Its value is in teaching format conventions, directory layouts, and minimal viable inputs that support reproducible hydrologic model exercises.",
      "keywords": ["CAMELS", "sample data", "hydrology", "streamflow", "forcing data", "neural hydrology", "education", "machine learning", "tutorials"],
      "entities": ["CIROH", "UCAR", "CAMELS"]
    },
    {
      "metadata": {
        "idurl": 6,
        "title": "cfe_v1.0",
        "source": "https://github.com/CIROH-UA/cfe_v1.0",
        "section_header": "# cfe_v1.0"
      },
      "content": "# Project Title: **cfe_v1.0**  \n\n## Project Objective  \nThe **Conceptual Functional Equivalent (CFE)** model is a simplified conceptual hydrologic model written by Fred Ogden. It is designed to be functionally equivalent to the National Water Model (NWM). The repository includes both the original author’s code and a BMI-enabled, expanded version of the model for broader integration and testing.\n\n## Core Functionalities  \n- Implements a conceptual model functionally equivalent to the National Water Model.  \n- Provides both standalone and framework-integrated operation modes.  \n- Includes example configurations demonstrating model coupling and data exchange:  \n  - **Example 1:** Standalone mode reading local forcing data.  \n  - **Example 2:** Coupled to AORC (provides forcing data via BMI).  \n  - **Example 3:** Coupled to AORC and PET modules for evapotranspiration.  \n  - **Example 4:** Same as Example 3 but with rootzone-based evapotranspiration.  \n  - **Example 5:** NextGen framework mode, coupled to PET module.  \n- Offers detailed configuration options for model parameters.  \n- Includes unit tests and example datasets for validation.  \n- Supports community contributions through GitHub pull requests.\n\n## Technical Stack  \n- **Languages:** C.  \n- **Frameworks:** BMI (Basic Model Interface).  \n- **Dependencies:** Forcing data (AORC), PET module, and configuration files.\n\n## Setup and Usage  \n- Detailed build and run instructions are provided in the `INSTALL.md` guide.  \n- Configuration parameters and model setup details are documented in the `configs/README.md` file.  \n- Unit and example test cases are available in the `test/` directory.  \n- The model supports both standalone and coupled framework execution modes.\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Hydrologic Modeling.  \n- **Affiliation:** NOAA Office of Water Prediction (OWP).  \n- **Purpose:** Provide a simplified, functionally equivalent representation of the National Water Model for research and operational evaluation.\n\n## Input / Output  \n**Input:**  \n- Forcing data (e.g., from AORC).  \n- Model configuration files specifying parameters and settings.  \n**Output:**  \n- Modeled hydrologic outputs generated by the CFE model.",
      "summary": "cfe_v1.0 contains the Conceptual Functional Equivalent (CFE) model, a BMI-enabled, C-based conceptual hydrologic model intended to be functionally equivalent to the National Water Model. The repository demonstrates standalone execution and coupled integrations (AORC forcing, PET modules, NextGen mode), with documented configurations, tests, and example datasets. It supports parameter configuration and reproducible builds, aligning with OWP’s modeling ecosystem. CFE’s purpose is to provide a tractable, interoperable surrogate for NWM behavior that is easier to test, embed, and evaluate in research or pre-operational contexts.",
      "keywords": ["CFE", "National Water Model", "NWM", "BMI", "AORC", "PET", "NextGen", "conceptual model", "hydrologic modeling"],
      "entities": ["CIROH", "NOAA OWP", "CFE", "National Water Model"]
    },
    {
      "metadata": {
        "idurl": 7,
        "title": "ciroh_pyngiab",
        "source": "https://github.com/CIROH-UA/ciroh_pyngiab",
        "section_header": "# ciroh_pyngiab"
      },
      "content": "# Project Title: **ciroh_pyngiab**  \n\n## Project Objective  \nThe **ciroh_pyngiab** repository provides tools and configurations to run the **NextGen In A Box (NGIAB)** hydrologic modeling workflow entirely within a **Python or Jupyter environment**. It integrates NGIAB capabilities—such as data preprocessing and model execution—into JupyterHub-compatible environments, enabling researchers to perform end-to-end hydrologic simulations using the NextGen framework.\n\n## Core Functionalities  \n- **JupyterHub-compatible NGIAB:**  \n  - Provides a version of NGIAB that can run on JupyterHub platforms (e.g., 2i2c), adapted from the standard NGIAB built on `rockylinux9`.  \n  - Builds upon the `pangeo/pangeo-notebook` image to ensure compatibility with CIROH’s 2i2c infrastructure.  \n  - Includes scripts such as `Dockerfile` and `docker_run.sh` for local image creation and deployment.  \n- **Python Wrapper Libraries:**  \n  - Wraps NGIAB’s data preprocessing and model execution shell scripts for direct use from Python.  \n  - Enables execution of the NextGen model framework directly from a Jupyter Notebook or Python script without terminal commands.  \n- **Integrated Tools:**  \n  - Preinstalled utilities include [`ngiab_data_preprocess`](https://github.com/CIROH-UA/NGIAB_data_preprocess) for data subsetting, forcing generation, and realization creation.  \n  - Contains example tests, unit test files, and CI/CD workflow configurations.  \n- **Sample Data:**  \n  - Example datasets are included under `/tests/` in the container image.  \n  - Users may also follow the NGIAB “Quick Start Guide” to manually download additional data.\n\n## Technical Stack  \n- **Languages:** Python, Shell  \n- **Containerization:** Docker  \n- **Base Image:** `pangeo/pangeo-notebook:2024.04.08`  \n- **Dependencies:** NGIAB, NextGen, ngiab_data_preprocess  \n- **Environment:** Compatible with JupyterHub and local Docker environments  \n- **Installation:** Available via pip using `pip install git+https://github.com/fbaig/ciroh_pyngiab.git`\n\n## Setup and Usage  \n### 1. Access Preconfigured JupyterHub Environments  \n- **Production:** [CIROH 2i2c JupyterHub](http://staging.ciroh.awi.2i2c.cloud/)  \n- **Staging/Dev:** [CIROH 2i2c JupyterHub (Staging)](http://staging.ciroh.awi.2i2c.cloud/)  \n- **Local Deployment:**  \n  - Build local image with `Dockerfile` or `docker_run.sh`.  \n  - Access via `http://127.0.0.1:8888/` after running:  \n    ```bash\n    docker run -it -v \"${PWD}\":/shared/ -p 8888:8888 quay.io/fbaig25/ngiab-2i2c:latest jupyter lab --ip 0.0.0.0 /shared\n    ```  \n- **Upcoming:** Support for [I-GUIDE JupyterHub](https://jupyter.iguide.illinois.edu/).\n\n### 2. Python Wrapper Example  \nExecute NGIAB via Python:  \n```python\nfrom pyngiab import PyNGIAB\n\ndata_dir = './AWI_16_2863657_007'\ntest_ngiab = PyNGIAB(data_dir)\ntest_ngiab.run()\n```\nFor serial mode:  \n```python\ntest_ngiab_serial = PyNGIAB(data_dir, serial_execution_mode=True)\ntest_ngiab_serial.run()\n```\n\n### 3. Data Preprocessing Example  \nRun data subsetting, forcing generation, and realization creation:  \n```bash\npython -m ngiab_data_cli -i cat-5173 -a --start 2022-01-01 --end 2022-02-28\n```\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Hydrologic modeling / Computational workflows.  \n- **Affiliation:** Cooperative Institute for Research to Operations in Hydrology (**CIROH**), The University of Alabama.  \n- **Purpose:** Streamline end-to-end hydrologic modeling within Jupyter environments, combining NGIAB and NextGen frameworks for research and development.\n\n## Input / Output  \n**Input:**  \n- Catchment ID, date ranges, configuration files, and hydrologic datasets.  \n- Preprocessing utilities for data subsetting and forcing generation.  \n**Output:**  \n- NextGen model execution results and generated realization files.  \n- Processed hydrologic data and analysis-ready outputs.",
      "summary": "ciroh_pyngiab brings NGIAB hydrologic workflows into Python and Jupyter, enabling end-to-end model runs on JupyterHub (2i2c) or locally with Docker. It wraps NGIAB shell tooling in Python, integrates data preprocessing (ngiab_data_preprocess), and ships with examples, tests, and CI/CD configs. Based on pangeo/pangeo-notebook, it aligns with CIROH’s hub infrastructure and supports interactive execution of NextGen realizations, from subsetting and forcing generation to model runs and result inspection. This repository reduces operational friction for researchers by combining containerized environments, notebook UX, and standardized pipelines.",
      "keywords": ["NGIAB", "NextGen", "Jupyter", "JupyterHub", "pangeo-notebook", "Python wrapper", "Docker", "data preprocessing", "hydrologic modeling"],
      "entities": ["CIROH", "NextGen", "NGIAB", "2i2c", "pangeo-notebook", "The University of Alabama"]
    },
    {
      "metadata": {
         "idurl": 8,
         "title": "CIROH-open-source-project-template",
         "source": "https://github.com/CIROH-UA/CIROH-open-source-project-template",
         "section_header": "# CIROH-open-source-project-template"
      },
      "content": "# Project Title: **CIROH-open-source-project-template**  \n\n## Project Objective  \nThe **CIROH Open Source Project Template** provides a standardized structure and set of instructions for creating new open-source projects. It helps developers initialize a repository with consistent documentation, licensing, and configuration files for CIROH-affiliated software.\n\n## Core Functionalities  \n- Provides an installation script to copy all template files into a new project.  \n- Includes guidance for updating README files and open-source checklists.  \n- Supplies TERMS, LICENSE, and CONTRIBUTING templates.  \n- Defines structure for project documentation, configuration, and dependencies.  \n- Encourages best practices for maintaining clear and up-to-date documentation.\n\n## Technical Stack  \n- **Languages/Tools:** Shell script (Bash).  \n- **Dependencies:** None explicitly listed.\n\n## Setup and Usage  \nTo install the full template into a new repository, run:  \n```bash\nbash -c \"$(curl -s https://raw.githubusercontent.com/CIROH-UA/CIROH-open-source-project-template/main/open_source_template.sh)\"\n```  \nDevelopers should then:  \n1. Update the README with project-specific details.  \n2. Complete the open-source checklist.  \n3. Adjust the TERMS and LICENSE sections as needed.  \n4. Maintain the README and other documentation regularly.\n\n## Project Context & Domain  \n- **Domain:** Open Source Software Development / Project Management.  \n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology).  \n- **Purpose:** Standardize CIROH open-source repositories with consistent documentation and licensing structure.\n\n## Input / Output  \n**Input:**  \n- None (template installation and customization).  \n\n**Output:**  \n- A fully initialized project repository containing template documentation, configuration files, and licensing information.",
      "summary": "The CIROH-open-source-project-template repository standardizes the creation of CIROH-affiliated open-source projects by providing a turnkey scaffold with consistent documentation, licensing, and configuration assets. A Bash installer script bootstraps a new repository by copying template files and instructing maintainers to complete a README, an open-source readiness checklist, and TERMS/LICENSE/CONTRIBUTING documents. The template enforces best practices in governance and documentation hygiene, clarifies dependency and configuration conventions, and streamlines early-stage setup so teams can focus on code and reproducibility. This project helps align CIROH software with open-source norms across repositories, improving maintainability, onboarding, and compliance while reducing friction for new contributors and ensuring a predictable structure for downstream users and integrators.",
      "keywords": ["open source template", "repository scaffold", "README", "LICENSE", "TERMS", "CONTRIBUTING", "governance", "documentation", "Bash installer", "CIROH"],
      "entities": ["CIROH", "GitHub", "Open Source"]
    },
    {
        "metadata": {
          "idurl": 9,
          "title": "ciroh-ua_website",
          "source": "https://github.com/CIROH-UA/ciroh-ua_website",
          "section_header": "# ciroh-ua_website"
        },
        "content": "# Project Title: **ciroh-ua_website**  \n\n## Project Objective  \nThe **CIROH DocuHub** serves as a centralized documentation and communication portal for CIROH projects, built using [Docusaurus](https://docusaurus.io/).  \nIt provides a structured platform to host technical documentation, blog content, and product information related to CIROH and its NextGen initiatives.  \nThe project aims to enhance collaboration, transparency, and accessibility across CIROH’s distributed teams and contributors.\n\n## Core Functionalities  \n- **Static site generation:** Built using Docusaurus for efficient, responsive documentation websites.  \n- **Multi-environment deployment:**  \n  - **Production:** [https://docs.ciroh.org/](https://docs.ciroh.org/)  \n  - **Staging:** [https://docs.ciroh.org/docuhub-staging/](https://docs.ciroh.org/docuhub-staging/) (for testing and validation).  \n- **Contributor workflows:**  \n  - Edit content directly via GitHub (“Edit page” button).  \n  - Submit pull requests for review and merge.  \n  - Contribute to the Products tab by sharing repository links.  \n  - Add blog posts following official guidelines.  \n- **Local development support:** Step-by-step setup guide for cloning, installing dependencies, and running the development server.  \n- **Support channels:** Direct contact via CIROH IT Admin and Arpita Patel for contributor assistance.\n\n## Technical Stack  \n- **Framework:** [Docusaurus](https://docusaurus.io/)  \n- **Language:** JavaScript  \n- **Runtime:** Node.js (LTS version)  \n- **Package Manager:** npm  \n- **Hosting:** docs.ciroh.org (production) and docs.ciroh.org/docuhub-staging (staging)  \n- **Version Control:** GitHub  \n- **Continuous Integration:** GitHub Actions (used for PR validation and local build testing).\n\n## Setup and Usage  \n1. **Clone Repository:**  \n   ```bash\n   git clone https://github.com/CIROH-UA/ciroh-ua_website.git\n   cd ciroh-ua_website\n   ```  \n2. **Install Node.js (LTS):** Obtain from [nodejs.org](https://nodejs.org/en).  \n3. **Install Dependencies:**  \n   ```bash\n   npm install\n   ```  \n4. **Run Locally:**  \n   ```bash\n   npm run start\n   ```  \n   Opens the local server at http://localhost:3000  \n5. **Build for Production (optional):**  \n   ```bash\n   npm run build\n   ```  \n   Generates static files in the `build/` directory.  \n6. **Validate PR Locally:**  \n   Download the build folder from GitHub Actions PR validation, unzip, then run:  \n   ```bash\n   npx http-server\n   ```\n\n## Project Context & Domain  \n- **Domain:** Web development / Documentation management.  \n- **Affiliation:** Cooperative Institute for Research to Operations in Hydrology (**CIROH**), hosted by **The University of Alabama**.  \n- **Purpose:** Facilitate the sharing and maintenance of CIROH’s documentation, blogs, and project resources through a unified web portal.\n\n## Input / Output  \n**Input:**  \n- Markdown-based documentation files, configuration files, and static assets.  \n**Output:**  \n- A compiled static documentation website served via docs.ciroh.org and its staging environment.",
        "summary": "The ciroh-ua_website repository, also known as the CIROH DocuHub, centralizes documentation, blogs, and project resources for CIROH and its NextGen initiatives. Built on Docusaurus and powered by Node.js, it supports multi-environment deployment (production and staging) via GitHub Actions for CI/CD validation. Contributors can edit pages directly through GitHub, submit pull requests, and add blog posts or product listings, streamlining collaboration across distributed CIROH teams. Local development uses npm for dependency management and live previews. Hosted at docs.ciroh.org, this platform enhances transparency, consistency, and knowledge sharing across CIROH’s research and operational communities.",
        "keywords": ["Docusaurus", "documentation portal", "Node.js", "npm", "GitHub Actions", "static site", "NextGen", "CI/CD", "knowledge management", "web development"],
        "entities": ["CIROH", "The University of Alabama", "Docusaurus", "NextGen", "docs.ciroh.org"]
    },
    {
        "metadata": {
          "idurl": 10,
          "title": "community_hf_patcher",
          "source": "https://github.com/CIROH-UA/community_hf_patcher",
          "section_header": "# community_hf_patcher"
        },
        "content": "# Project Title: **community_hf_patcher**  \n\n## Project Objective  \nThe **community_hf_patcher** repository provides scripts and Docker configurations to apply patches to the **CIROH Community Hydrofabric**, a temporary fork of Lynker Spatial's **NextGen Hydrofabric v2.2**.  \nThese patches improve compatibility with CIROH projects, including the **NextGen In A Box** ecosystem. The repository generates patched hydrofabric datasets that can be published to the **Community Hydrofabric bucket** on AWS S3.\n\n## Core Functionalities  \n- Applies patches to the CIROH Community Hydrofabric.  \n- Publishes processed hydrofabric data to AWS S3.  \n- Corrects gage-to-flowpath mappings (~4500 gages).  \n- Reformats hydrolocation tables to be GPKG compliant for GIS compatibility.  \n- Adds database indices for faster query performance.  \n- Subsets the hydrofabric into individual VPUs.  \n- Uses Docker for building, processing, and compressing hydrofabric data.\n\n## Technical Stack  \n- **Languages/Tools:** Python, Shell.  \n- **Containerization:** Docker (multi-stage builds).  \n- **Data Sources:** Lynker Spatial’s NextGen Hydrofabric v2.2 (ODbL licensed).  \n- **Storage:** AWS S3 for output publication.  \n- **Dependencies:** Docker, AWS CLI (for upload).\n\n## Setup and Usage  \n### Prerequisites  \n- Docker  \n- AWS CLI (if uploading to S3)\n\n### Running the Patcher  \nRun the build process using:  \n```bash\n./generate_hydrofabric.sh\n```  \nTo upload outputs to an S3 bucket, uncomment AWS commands in the shell scripts.\n\n### Repository Structure  \n- **`generate_hydrofabric.sh`** – Main entrypoint for building and running the Docker container, extracting outputs, and optionally uploading results.  \n- **`generate_vpus.sh`** – Secondary script that subsets the hydrofabric into individual VPUs.  \n- **`Dockerfile`** – Defines stages for downloading, processing, and compressing hydrofabric data.  \n- **`scripts/`** – Contains Python and shell scripts for data processing tasks.  \n  - **`scripts/formatting`** – Handles data formatting and storage structure.  \n  - **`scripts/hydro`** – Modifies hydrologic data affecting simulation outputs.\n\n### Workflow Overview  \n1. **Build and Run the Docker Container:**  \n   - Downloads raw hydrofabric data.  \n   - Adds indices and updates gages.  \n   - Converts hydrolocations and subsets VPUs.  \n   - Compresses processed outputs.  \n2. **Extract Processed Files:**  \n   - `conus_nextgen.gpkg`: Complete patched hydrofabric.  \n   - `conus_nextgen.tar.gz`: Compressed version of the above.  \n   - `gage_replacements.csv`: Catalog of gage corrections.  \n   - `VPU/`: Folder with individual VPU hydrofabrics.  \n   - `VPU/compressed/`: Folder with compressed tarballs of each VPU hydrofabric.  \n3. **Upload to S3 (Optional):**  \n   Uncomment upload commands in the scripts and ensure valid AWS credentials are configured.\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Geospatial Data Processing.  \n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology).  \n- **Purpose:** Apply compatibility patches to the NextGen Hydrofabric v2.2 for CIROH projects and publish updated versions to AWS S3.\n\n## Input / Output  \n**Input:**  \n- Lynker Spatial’s NextGen Hydrofabric v2.2 source data.  \n- Configuration and patch scripts.  \n**Output:**  \n- Patched hydrofabric files (`.gpkg`, `.tar.gz`).  \n- Updated gage mapping CSV.  \n- Subset VPU hydrofabric folders (optional).  \n- Uploaded datasets in the Community Hydrofabric S3 bucket (optional).",
        "summary": "The community_hf_patcher repository automates the process of patching and publishing the CIROH Community Hydrofabric—a modified version of Lynker Spatial’s NextGen Hydrofabric v2.2. Using Python, Shell, and Docker-based workflows, it applies geospatial corrections to gage-to-flowpath mappings, enforces GPKG compliance, and builds VPU-level subsets for efficient modeling. The system generates complete, indexed hydrofabric datasets that can be compressed and uploaded to CIROH’s AWS S3 bucket for distribution. By streamlining data correction and deployment, this repository ensures consistency, compatibility, and reproducibility across CIROH’s NextGen hydrologic modeling ecosystem, improving accessibility for downstream simulation and analysis workflows.",
        "keywords": ["hydrofabric", "NextGen", "CIROH", "AWS S3", "Docker", "Python", "Shell scripting", "hydrology", "geospatial processing", "gage mapping"],
        "entities": ["CIROH", "NextGen Hydrofabric", "Lynker Spatial", "AWS S3", "Community Hydrofabric", "Docker"]
    },
    {
        "metadata": {
          "idurl": 11,
          "title": "Community-Streamflow-Evaluation-System (CSES)",
          "source": "https://github.com/CIROH-UA/Community-Streamflow-Evaluation-System",
          "section_header": "# Community-Streamflow-Evaluation-System (CSES)"
        },
        "content": "# Project Title: **Community-Streamflow-Evaluation-System (CSES)**  \n\n## Project Objective  \nThe **Community-Streamflow-Evaluation-System (CSES)** is a Python-based tool designed to evaluate hydrological model performance using a standardized NHDPlus data model. It enables users to assess modeled streamflow against over 5,000 in situ USGS monitoring sites through interactive visualization and analysis tools.\n\n## Core Functionalities  \n- Evaluates modeled streamflow performance using NHDPlus data and colocated USGS/NWIS monitoring sites.  \n- Provides three primary analysis methods:  \n  - **State-based LULC (Land Use Land Cover) Evaluation** using StreamStats.  \n  - **HUC-level (Hydrologic Unit Code) Evaluation** for watershed-scale analysis.  \n  - **USGS Station-based (Reach) Evaluation** for site-specific comparisons.  \n- Allows fast access to observed and predicted data from the Alabama Water Institute-hosted AWS S3 datasets.  \n- Supports interactive maps and performance visualizations through the `holoviews` package.  \n- Designed for evaluating National Water Model (NWM) retrospective data (v2.1, with v3.0 coming soon).\n\n## Technical Stack  \n- **Language:** Python (Version 3.9.12)  \n- **Dependencies:** Listed in `requirements.txt`  \n- **Visualization Tools:** holoviews  \n- **Data Source:** AWS S3-hosted NWM v2.1 retrospective and USGS/NWIS observations\n\n## Setup and Usage  \n1. Create a Python virtual environment.  \n2. Install dependencies using the included `requirements.txt` file.  \n3. Refer to the `Getting Started.md` guide for setup instructions.  \n4. Choose one of the three main evaluation classes for analysis:  \n   - `Eval_State()` for state-level StreamStats-based analysis.  \n   - `HUC_Eval()` for hydrologic unit evaluation.  \n   - `Reach_Eval()` for individual USGS site evaluation.  \n5. Input parameters include start date, end date, and model version (NWM v2.1).\n\n## Project Context & Domain  \n- **Domain:** Hydrology / Streamflow Modeling / Data Evaluation  \n- **Affiliation:** Alabama Water Institute, University of Alabama  \n- **Purpose:** Provide a scalable, standardized framework to assess and improve hydrological model performance across diverse watersheds and land use types.\n\n## Input / Output  \n**Input:**  \n- NWM modeled streamflow data (v2.1 retrospective)  \n- USGS/NWIS in situ streamflow observations  \n- User-defined parameters (start date, end date, HUCs, or USGS site IDs)  \n**Output:**  \n- Statistical performance metrics and comparative plots of modeled vs. observed streamflow.  \n- Interactive maps visualizing spatial distribution of model accuracy and performance.\n\n## License  \nLicensed under the GitHub license specified in the repository.",
        "summary": "The Community-Streamflow-Evaluation-System (CSES) is a Python-based framework developed by the Alabama Water Institute to evaluate hydrologic model performance using standardized NHDPlus data and USGS/NWIS observations. It supports state-level, HUC-level, and site-specific evaluations through classes like Eval_State, HUC_Eval, and Reach_Eval, providing spatial and statistical comparisons of modeled versus observed streamflow. Designed for use with National Water Model (NWM) retrospective datasets, CSES integrates AWS S3-hosted data and interactive visualizations via holoviews to generate accuracy maps and performance metrics. The system promotes consistent, scalable streamflow validation for hydrologic research and model development across U.S. watersheds.",
        "keywords": ["streamflow evaluation", "hydrology", "NHDPlus", "USGS", "National Water Model", "AWS S3", "holoviews", "Python", "Alabama Water Institute", "data visualization"],
        "entities": ["Community-Streamflow-Evaluation-System", "CSES", "USGS", "NWIS", "NHDPlus", "National Water Model", "Alabama Water Institute", "University of Alabama"]
    },
    {
        "metadata": {
          "idurl": 12,
          "title": "Conferences",
          "source": "https://github.com/CIROH-UA/Conferences",
          "section_header": "# Conferences"
        },
        "content": "# Project Title: **Conferences**  \\n\\n## Project Objective  \\nThe **Conferences** repository serves as a collection space for **conference presentations and posters** produced by the **Alabama Water Institute (AWI)** and the **Cooperative Institute for Research to Operations in Hydrology (CIROH)**.  \\nIts purpose is to organize, preserve, and share presentation materials that highlight CIROH’s ongoing research and contributions to hydrologic science and water prediction.\\n\\n## Core Functionalities  \\n- Provides a repository for uploading **PDF versions** of slides, posters, and presentations.  \\n- Represents CIROH and AWI’s participation in scientific and professional conferences.  \\n- Acts as a centralized archive for communication and outreach materials related to water prediction and hydrology research.  \\n\\n## Technical Stack  \\n- **File Format:** PDF (for uploaded slides and posters).  \\n- **Platform:** GitHub (for storage and sharing).  \\n- **Technologies Used:** Not specified.  \\n\\n## Setup and Usage  \\nNo setup or installation instructions are provided. Contributors are expected to upload presentation materials in **PDF format** directly to the repository.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Scientific communication / Research dissemination.  \\n- **Affiliation:**  \\n  - **Alabama Water Institute (AWI)**  \\n  - **Cooperative Institute for Research to Operations in Hydrology (CIROH)**  \\n  - **NOAA–The University of Alabama Partnership**  \\n- **Purpose:** Share and document research outputs—specifically conference materials—that advance CIROH’s mission to improve water prediction, hydrologic forecasting, and community resilience.  \\n\\n## Input / Output  \\n**Input:**  \\n- Conference presentation slides and posters in PDF format.  \\n\\n**Output:**  \\n- Archived presentation materials representing CIROH and AWI research activities.",
        "summary": "The Conferences repository serves as an organized archive for conference presentations and posters created by the Alabama Water Institute (AWI) and the Cooperative Institute for Research to Operations in Hydrology (CIROH). Hosted on GitHub, it centralizes CIROH and AWI’s scientific communication outputs, including PDFs of slides and posters presented at professional and academic conferences. This repository enhances visibility and accessibility of hydrology and water prediction research, preserving institutional contributions tied to NOAA–University of Alabama collaborations. It functions primarily as a dissemination and documentation resource rather than executable software, supporting CIROH’s outreach, transparency, and knowledge-sharing mission.",
        "keywords": ["CIROH", "AWI", "NOAA", "conference presentations", "scientific communication", "hydrology", "water prediction", "research dissemination", "PDF archive", "outreach"],
        "entities": ["Cooperative Institute for Research to Operations in Hydrology", "Alabama Water Institute", "NOAA", "The University of Alabama"]
    },
    {
        "metadata": {
          "idurl": 13,
          "title": "data_access_example",
          "source": "https://github.com/CIROH-UA/data_access_example",
          "section_header": "# data_access_example"
        },
        "content": "# Project Title: **data_access_example**  \\n\\n## Project Objective  \\nThe **data_access_example** repository provides a Python example for efficiently accessing data from the National Water Model (NWM) across various configurations. It demonstrates how to generate JSON header URLs for specific NWM datasets, retrieve data in parallel, and process and visualize the results.\\n\\n## Core Functionalities  \\n- Generates JSON header URLs for NWM data access.  \\n- Retrieves NWM data in parallel for improved efficiency.  \\n- Processes and plots time series of streamflow data.  \\n- Includes a Jupyter Notebook (`DataAccessExample.ipynb`) that guides users through the workflow.  \\n- Provides a Python module (`urlgennwm.py`) with helper functions for generating NWM URLs.  \\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Libraries:** numpy, xarray, fsspec, ujson, matplotlib, psutil, concurrent.futures, multiprocessing, joblib, tqdm  \\n- **Environment:** Jupyter Notebook  \\n\\n## Setup and Usage  \\n1. Clone the repository:  \\n   ```bash\\n   git clone https://github.com/CIROH-UA/data_access_example.git\\n   ```  \\n2. Navigate to the project directory.  \\n3. Launch the Jupyter Notebook:  \\n   ```bash\\n   jupyter notebook\\n   ```  \\n4. Modify parameters (`start_date`, `end_date`, `fcst_cycle`, `lead_time`, `varinput`, `geoinput`, `runinput`) within the notebook to customize data retrieval.  \\n5. Execute the notebook to fetch, process, and visualize NWM data.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Data Access / Modeling  \\n- **Affiliation:** Not specified.  \\n- **Purpose:** Demonstrate practical methods for accessing and visualizing NWM data efficiently using Python.  \\n\\n## Input / Output  \\n**Input:**  \\n- User-defined parameters for data retrieval (e.g., start date, forecast cycle, variable type, and geographic region).  \\n\\n**Output:**  \\n- Generated JSON header URLs.  \\n- Processed NWM data and time series plots of streamflow values.  \\n\\n## License  \\nThis project is licensed under the MIT License.",
        "summary": "data_access_example provides a Python-based demonstration of how to efficiently access, process, and visualize National Water Model (NWM) data. It features a Jupyter Notebook workflow and modular code that generates JSON header URLs, retrieves data in parallel, and produces time series plots of streamflow outputs. Using libraries like xarray, fsspec, and joblib, the repository highlights scalable techniques for querying large hydrologic datasets stored remotely. By combining multiprocessing and visualization tools, it serves as an educational and practical example for researchers developing workflows for NWM data retrieval and analysis.",
        "keywords": ["National Water Model", "NWM", "data access", "Python", "xarray", "fsspec", "parallel processing", "Jupyter Notebook", "streamflow", "hydrology"],
        "entities": ["National Water Model", "CIROH", "NOAA"]
    },
    {
        "metadata": {
          "idurl": 14,
          "title": "datastreamcli",
          "source": "https://github.com/CIROH-UA/datastreamcli",
          "section_header": "# datastreamcli"
        },
        "content": "# Project Title: **datastreamcli**  \\n\\n## Project Objective  \\n**DataStreamCLI** is a standalone command-line tool that automates the complete workflow from **preprocessing input data** for [NextGen](https://github.com/NOAA-OWP/ngen) to executing the **NextGen simulation** through [NextGen In a Box (NGIAB)](https://github.com/CIROH-UA/NGIAB-CloudInfra).  \\nIt serves as the workflow tooling for the [NextGen Research DataStream](https://github.com/CIROH-UA/ngen-datastream), enabling users to run NextGen efficiently, reproducibly, and with integrated support for **hfsubset**, **NGIAB**, and **TEEHR**.\\n\\n## Core Functionalities  \\n- **Automated workflow:** Handles the full NextGen workflow—from data preprocessing to simulation execution.  \\n- **Integration with external tools:** Supports tools such as `hfsubset`, NGIAB, and TEEHR.  \\n- **Model configuration automation:** Generates configurations for available NextGen models.  \\n- **Interactive setup:** Includes an interactive script (`datastream_guide`) to guide users through setup and command formation.  \\n- **Documentation suite:** Provides detailed documentation covering available models, datastream options, standard directory structures, and internal workflow breakdowns.  \\n- **Status monitoring:** Includes a `STATUS.md` file for tracking tool and integration availability.  \\n- **Reproducibility:** Ensures consistent simulation runs via standardized inputs and configurations.  \\n\\n## Technical Stack  \\n- **Programming Language:** Not specified.  \\n- **Frameworks / Libraries:** Not specified.  \\n- **Integrated Tools:** NextGen, NGIAB, hfsubset, and TEEHR.  \\n- **License:** GNU General Public License v3.0 or later.  \\n\\n## Setup and Usage  \\n1. **Installation:**  \\n   - Follow the [Installation Guide](https://github.com/CIROH-UA/datastreamcli/blob/main/INSTALL.md) to prepare the environment.  \\n\\n2. **Interactive Guide:**  \\n   - Run the [DataStreamCLI guide](https://github.com/CIROH-UA/datastreamcli/blob/main/scripts/datastream_guide) to explore repository functions and form commands interactively.  \\n\\n3. **Example Workflow:**  \\n   - Obtain a hydrofabric file (e.g., using `hfsubset`).  \\n   - Run a 24-hour NextGen simulation:  \\n     ```bash\\n     ./scripts/datastream -s 202006200100                          -e 202006210000                          -C NWM_RETRO_V3                          -d $(pwd)/data/datastream_test                          -g $(pwd)/palisade.gpkg                          -R $(pwd)/configs/ngen/realization_sloth_nom_cfe_pet_troute.json                          -n 4\\n     ```\\n   - Outputs will be located at:  \\n     ```\\n     $(pwd)/data/datastream_test/ngen-run/outputs\\n     ```\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Environmental Modeling / Workflow Automation.  \\n- **Affiliation:** Not specified.  \\n- **Purpose:** Simplify, standardize, and automate NextGen model runs using reproducible, CLI-based workflows.  \\n\\n## Input / Output  \\n**Input:**  \\n- Hydrofabric files (e.g., `.gpkg`).  \\n- NextGen configuration JSONs and run parameters.  \\n- Command-line arguments defining time domain and model setup.  \\n\\n**Output:**  \\n- NextGen simulation outputs generated in the specified data directory.",
        "summary": "datastreamcli is a command-line automation tool that orchestrates the full workflow for running NextGen hydrologic simulations. It handles preprocessing, configuration generation, and model execution through NextGen In a Box (NGIAB), with built-in support for hfsubset and TEEHR. The tool standardizes inputs and directory structures, offers an interactive guide for command generation, and tracks workflow component availability. Designed for reproducibility, DataStreamCLI serves as a foundational component of CIROH’s NextGen Research DataStream, ensuring consistent, transparent, and efficient hydrologic modeling operations across diverse environments.",
        "keywords": ["NextGen", "NGIAB", "hfsubset", "TEEHR", "workflow automation", "CLI", "hydrology", "simulation", "data preprocessing", "reproducibility"],
        "entities": ["CIROH", "NextGen", "NextGen In a Box", "TEEHR", "hfsubset", "NOAA Office of Water Prediction"]
    },
    {
        "metadata": {
          "idurl": 15,
          "title": "deep_bucket_lab",
          "source": "https://github.com/CIROH-UA/deep_bucket_lab",
          "section_header": "# deep_bucket_lab"
        },
        "content": "# Project Title: **deep_bucket_lab**  \\n\\n## Project Objective  \\n**DeepBucketLab** is an interactive, hands-on educational tool designed to introduce students in civil engineering and Earth science disciplines to neural network-based modeling. The project emphasizes hydrological processes, aiming to fill the educational gap in formal neural network training for hydrology at upper-division and graduate levels.\\n\\n## Core Functionalities  \\n- Interactive learning through hands-on modeling exercises.  \\n- Focused on understanding hydrological processes using modern neural network models.  \\n- Enables practical training and evaluation of neural network models for hydrological applications.  \\n- Fully accessible via Binder for browser-based execution without local setup.  \\n- Includes advanced code modules for data generation, modeling, validation, and visualization.  \\n- Provides structured directories for modular experimentation, configuration, and reproducibility.  \\n\\n## Technical Stack  \\n- **Languages:** Python  \\n- **Frameworks/Libraries:** Not specified  \\n- **Environment Management:** Conda (environment.yml)  \\n- **Execution Interfaces:** Jupyter Notebook, Command Line  \\n\\n## Setup and Usage  \\n### Quick Start (via Binder)  \\n- Click the Binder badge in the README to launch the interactive notebook directly in your browser.  \\n\\n### Local Installation  \\n1. Install Anaconda or Miniconda.  \\n2. Create and activate the environment:  \\n   ```bash\\n   conda env create -f environment.yml  \\n   conda activate deep_bucket_env\\n   ```  \\n3. Launch the lab notebook:  \\n   ```bash\\n   jupyter notebook deep_bucket_lab.ipynb\\n   ```  \\n\\n### Advanced Usage  \\n- Run Jupyter notebook with graphics:  \\n  ```bash\\n  ./run/run_deep_bucket_lab_with_graphics.ipynb\\n  ```  \\n- Execute from command line:  \\n  ```bash\\n  python3 ./run/run_deep_bucket_lab.py\\n  ```  \\n- Conduct experiment (e.g., noise sensitivity):  \\n  ```bash\\n  python3 ./experiments/noise_iteration.py\\n  ```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Education  \\n- **Affiliation:** Not specified  \\n- **Purpose:** Educational tool for teaching and understanding deep learning applications in hydrological modeling.  \\n\\n## Input / Output  \\n**Input:** Synthetic hydrological data generated by `data_generation.py` or user-provided inputs.  \\n**Output:** Model performance statistics, validation results, and visualizations from trained neural networks.  \\n\\n## Citation  \\n**Suggested citation:**  \\nJ. M. Frame, L. Hernandez Rodriguez, and M. Bassiouni, *\"DeepBucketLab - A Playground for Understanding Deep Learning for Hydrologic Process Representations,\"* DOI: 10.5072/zenodo.7349",
        "summary": "deep_bucket_lab is an interactive educational platform for learning neural network-based hydrologic modeling. Developed to bridge the gap between hydrology and deep learning education, it offers Jupyter Notebook labs, modular Python scripts, and Binder access for browser-based use without installation. The tool generates synthetic hydrological data, trains and validates neural networks, and visualizes performance metrics to help users understand model behavior. Built with reproducibility and experimentation in mind, DeepBucketLab provides structured exercises for exploring process representation, sensitivity testing, and data-driven modeling of hydrologic systems.",
        "keywords": ["hydrology", "deep learning", "education", "neural networks", "machine learning", "Binder", "Jupyter Notebook", "Conda", "data generation", "model validation"],
        "entities": ["DeepBucketLab", "CIROH", "Jupyter", "Binder", "University of Alabama", "Zenodo"]
    },
    {
        "metadata": {
        "idurl": 16,
        "title": "DEVCON_SNOW_ML",
        "source": "https://github.com/CIROH-UA/DEVCON_SNOW_ML",
        "section_header": "# DEVCON_SNOW_ML"
        },
        "content": "# Project Title: **DEVCON_SNOW_ML**  \\n\\n## Project Objective  \\nThe **DEVCON_SNOW_ML** repository provides a machine learning framework for estimating Snow-Water-Equivalent (SWE) across the western United States. The project aims to improve operational, near-real-time SWE estimation to update the snowpack state within the National Water Model (NWM) as part of the CIROH initiative.\\n\\n## Core Functionalities  \\n- Develops and optimizes ML algorithms for SWE estimation using regionalized modeling.  \\n- Utilizes twenty-three sub-models tailored to unique topographical and hydroclimatic zones across the western U.S.  \\n- Integrates data from nearly 700 snow telemetry (SNOTEL) and California Data Exchange Center (CDEC) sites.  \\n- Combines observation data with lidar-derived terrain features for 1 km x 1 km SWE predictions.  \\n- Provides a modular ML framework enabling experimentation with different algorithms:  \\n  - Deep neural networks (core model).  \\n  - Long Short-Term Memory (LSTM) networks.  \\n  - Tree-based and regression algorithms (e.g., Ordinary Least Squares).  \\n- Incorporates evaluation metrics using the **Standardized Snow Water Equivalent Evaluation Tool (SSWEET)**.  \\n- Offers a Jupyter workbook for demonstration, training, and evaluation of SWE prediction algorithms.  \\n\\n## Technical Stack  \\n- **Languages:** Python.  \\n- **Frameworks/Libraries:** Not specified.  \\n- **Data Sources:**  \\n  - NASA Airborne Snow Observatory (ASO) LiDAR-derived SWE.  \\n  - Copernicus 90m Digital Elevation Model (DEM).  \\n  - NRCS Snow Telemetry (SNOTEL) station SWE observations.  \\n  - CDEC monitoring data.  \\n  - VIIRS fraction snow-covered area (fSCA).  \\n- **Infrastructure:** AWS S3 and Box for cloud-hosted data access.  \\n\\n## Setup and Usage  \\n1. Refer to the `Getting Started` guide for virtual environment setup using the provided `environment.yml` file.  \\n2. Run the example Jupyter Notebook to reproduce the core SWE modeling workflow.  \\n3. Users are encouraged to modify and test different ML architectures to refine prediction accuracy, particularly in elevation-sensitive regions.  \\n4. Model performance evaluation is conducted using SSWEET.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Snow Modeling / Machine Learning.  \\n- **Affiliation:** Cooperative Institute for Research to Operations in Hydrology (CIROH), Alabama Water Institute (University of Alabama), and University of Utah.  \\n- **Purpose:** Enhance snowpack characterization and forecasting by improving the efficiency, scalability, and accuracy of machine learning models for SWE prediction.  \\n\\n## Input / Output  \\n**Input:**  \\n- Pre-processed training datasets (~7,000 grid points) including snow telemetry, lidar, DEM, and satellite-derived features.  \\n- Model configuration and training parameters defined within Jupyter notebooks.  \\n\\n**Output:**  \\n- High-resolution (1 km) SWE predictions.  \\n- Evaluation metrics (RMSE, R²) for algorithm performance.  \\n- Comparative performance analyses of multiple ML architectures.",
        "summary": "DEVCON_SNOW_ML provides a machine learning framework for estimating Snow Water Equivalent (SWE) across the western United States. Using regionalized models, it integrates data from nearly 700 SNOTEL and CDEC sites, lidar-derived terrain features, and satellite inputs to produce 1 km SWE predictions. Built with modular architecture, it supports deep neural networks, LSTMs, and regression models, evaluated through the SSWEET tool. Developed by CIROH and partners at the University of Alabama and University of Utah, the project advances near-real-time snowpack characterization for improving National Water Model forecasts.",
        "keywords": ["SWE", "machine learning", "hydrology", "snow modeling", "LSTM", "neural networks", "SSWEET", "SNOTEL", "CDEC", "NWM"],
        "entities": ["CIROH", "Alabama Water Institute", "University of Utah", "NASA Airborne Snow Observatory", "NRCS", "CDEC", "National Water Model"]
    },
    {
        "metadata": {
        "idurl": 17,
        "title": "forcingprocessor",
        "source": "https://github.com/CIROH-UA/forcingprocessor",
        "section_header": "# forcingprocessor"
        },
        "content": "# Project Title: **forcingprocessor**  \\n\\n## Project Objective  \\nThe **Forcing Processor** converts **National Water Model (NWM)** gridded forcing data into **NextGen**-compatible catchment-averaged time series. It serves as the **pre-processing tool** for the [NextGen Research DataStream](https://github.com/CIROH-UA/ngen-datastream), ensuring hydrologic input data can be efficiently transformed and used in the Next Generation National Water Model framework.  \\nIt is primarily intended for hydrologic researchers and practitioners working with NWM or NextGen data who need to generate or manage forcing datasets in standardized formats.\\n\\n## Core Functionalities  \\n- **Data Conversion:** Transforms NWM gridded forcings (netCDFs) into catchment-averaged NextGen time series.  \\n- **Automated Weight Calculation:** Uses [exactextract](https://github.com/isciences/exactextract) to compute polygonal cell coverage and weights for catchment averaging if not provided in the geopackage.  \\n- **Flexible Input Support:** Accepts local file paths, S3 URIs, or URLs for geopackages and forcing data.  \\n- **Parallel Processing:** Supports multi-core execution with configurable process counts (`nprocs`).  \\n- **Visualization:** Optionally generates side-by-side **GIFs** comparing NWM and NextGen forcings for selected variables.  \\n- **Configurable Outputs:** Supports multiple formats (`tar`, `parquet`, `csv`, `netcdf`) and local or cloud storage options.  \\n- **File Name Generation:** Provides a helper tool (`nwm_filenames_generator.py`) to automatically create lists of NWM forcing filenames.  \\n- **Weights Reuse:** Saves computed weights as a parquet file for reuse in future runs.  \\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Key Libraries/Tools:**  \\n  - `exactextract` (for spatial averaging)  \\n  - `hfsubset` (for geopackage creation)  \\n  - `nwmurl` (for URL generation of NWM forcing files)  \\n- **Data Formats:** netCDF, GeoPackage (`.gpkg`), Parquet, CSV, TAR  \\n- **Parallelization:** Multi-processing support via configurable cores  \\n\\n## Setup and Usage  \\n- **Installation:**  \\n  ```bash\\n  pip install -e .\\n  ```  \\n- **Execution:**  \\n  Run the processor using a JSON configuration file:  \\n  ```bash\\n  python src/forcingprocessor/processor.py ./configs/conf.json\\n  ```  \\n- **Configuration:**  \\n  The `conf.json` file defines four main sections:  \\n  1. **Forcing** — Paths to NWM filename list and geopackage file.  \\n  2. **Storage** — Output location and file types.  \\n  3. **Run** — Processing parameters (verbosity, stats collection, parallelization).  \\n  4. **Plot** — Visualization settings for generating GIFs.  \\n- **Example Generator Usage:**  \\n  ```bash\\n  python nwm_filenames_generator.py conf_nwm_files.json\\n  ```  \\n- **Weights Calculation (if needed):**  \\n  ```bash\\n  python3 src/forcingprocessor/weights_hf2ds.py   --outname ./weights.parquet   --input_file ./nextgen_VPU_03W.gpkg\\n  ```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Hydrologic modeling and data preprocessing  \\n- **Affiliation:** Developed by CIROH (Cooperative Institute for Research to Operations in Hydrology) under the **University of Alabama** in coordination with **NOAA**.  \\n- **Purpose:** Supports the preprocessing pipeline for the **NextGen Research DataStream**, standardizing NWM input data for model-ready use.\\n\\n## Input / Output  \\n**Input:**  \\n- Text file listing NWM forcing filenames (local paths, URLs, or S3 URIs).  \\n- GeoPackage (`.gpkg`) defining spatial domain with optional `forcing-weights` layer.  \\n- Optional configuration for weights calculation or plotting parameters.  \\n\\n**Output:**  \\n- Catchment-averaged NextGen forcing datasets in chosen formats (`tar`, `parquet`, `csv`, `netcdf`).  \\n- Optional GIF visualizations comparing NWM and NextGen forcings.  \\n- Reusable weights parquet file written to metadata.",
        "summary": "forcingprocessor converts National Water Model (NWM) gridded forcing data into catchment-averaged time series compatible with the NextGen framework. It automates spatial averaging, supports local and cloud-based data inputs, and provides parallelized processing with optional visualization of NWM versus NextGen forcings. The tool underpins CIROH’s NextGen Research DataStream by standardizing hydrologic input datasets for model-ready use, improving data accessibility, reproducibility, and integration with NextGen workflows.",
        "keywords": ["NextGen", "forcing data", "NWM", "exactextract", "hfsubset", "Python", "netCDF", "catchment averaging", "data preprocessing", "hydrology"],
        "entities": ["CIROH", "NOAA", "University of Alabama", "NextGen", "National Water Model"]
    },
    {
        "metadata": {
        "idurl": 18,
        "title": "GEE_Workshop",
        "source": "https://github.com/CIROH-UA/GEE_Workshop",
        "section_header": "# GEE_Workshop"
        },
        "content": "# Project Title: **GEE_Workshop**  \\n\\n## Project Objective  \\nThe **GEE_Workshop** repository provides instructional materials and code examples for working with Google Earth Engine (GEE) using Python and JavaScript. It demonstrates how to process, analyze, and visualize geospatial data for environmental and Earth science applications.\\n\\n## Core Functionalities  \\n- Contains example scripts and notebooks for Google Earth Engine operations.  \\n- Demonstrates data retrieval, processing, and visualization using GEE.  \\n- Supports both JavaScript and Python implementations.  \\n- Includes hands-on exercises for workshop participants.  \\n\\n## Technical Stack  \\n- **Languages:** Python, JavaScript  \\n- **Platform:** Google Earth Engine  \\n- **Environment:** Jupyter Notebook (for Python-based examples)  \\n\\n## Setup and Usage  \\n1. Clone the repository.  \\n2. Follow workshop instructions or open provided Jupyter notebooks.  \\n3. Run scripts using the GEE Code Editor or Python API.  \\n4. Modify and execute code to explore geospatial datasets and visualization techniques.  \\n\\n## Project Context & Domain  \\n- **Domain:** Remote Sensing / Geospatial Analysis / Environmental Modeling  \\n- **Affiliation:** Not specified.  \\n- **Purpose:** Provide an educational workshop on Google Earth Engine for geospatial data analysis.  \\n\\n## Input / Output  \\n**Input:**  \\n- Geospatial datasets accessed through Google Earth Engine.  \\n\\n**Output:**  \\n- Processed geospatial analyses and visualizations.  \\n\\n## License  \\nNot specified.",
        "summary": "GEE_Workshop offers tutorials and example scripts for using Google Earth Engine (GEE) in Python and JavaScript. It teaches participants to retrieve, process, and visualize environmental and geospatial data using both the GEE Code Editor and Python API. The repository includes hands-on exercises and Jupyter Notebooks designed for training and research in remote sensing and Earth system analysis.",
        "keywords": ["Google Earth Engine", "remote sensing", "geospatial analysis", "Python", "JavaScript", "education", "Earth science", "visualization", "Jupyter"],
        "entities": ["Google Earth Engine", "CIROH", "University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 19,
        "title": "hf_pmtiles",
        "source": "https://github.com/CIROH-UA/hf_pmtiles",
        "section_header": "# hf_pmtiles"
        },
        "content": "# Project Title: **hf_pmtiles**  \\n\\n## Project Objective  \\nThe **hf_pmtiles** repository provides a Docker-based workflow to convert GeoPackage files into Protomaps PMTiles format. It utilizes the Lynker Spatial Hydrofabric dataset under the OBDL license. The repository aims to streamline the generation of PMTiles for hydrologic or geospatial visualization and distribution.\\n\\n## Core Functionalities  \\n- Converts GeoPackages into Protomaps PMTiles using a Dockerized process.  \\n- Provides an example style and usage guide in the `map` folder.  \\n- Enables serving of generated tiles using compatible tools such as Martin Tiles or the PMTiles utility.  \\n\\n## Technical Stack  \\n- **Languages/Tools:** Docker.  \\n- **Data Sources:** Lynker Spatial Hydrofabric under OBDL license.  \\n- **Tile Format:** Protomaps PMTiles.  \\n\\n## Setup and Usage  \\n1. **Build the Docker image:**  \\n   ```bash\\n   docker build -t pmtiles .\\n   ```  \\n2. **Wait for processing:** The build process downloads and processes the required data.  \\n3. **Run the Docker container:**  \\n   ```bash\\n   docker run --name hfmap --rm -it pmtiles /bin/bash\\n   ```  \\n4. **Copy the PMTiles file from the container:**  \\n   ```bash\\n   docker cp hfmap:/mbtiles/merged/merged.pmtiles .\\n   ```  \\n5. **Serve the tiles locally:** Use a tile-serving tool like [Martin Tiles](https://martin.maplibre.org/) or the PMTiles command-line utility.  \\n\\n## Project Context & Domain  \\n- **Domain:** Geospatial Data Processing / Hydrology.  \\n- **Affiliation:** Lynker Spatial.  \\n- **Purpose:** Simplify the conversion of GeoPackages to PMTiles for efficient mapping and visualization workflows.  \\n\\n## Input / Output  \\n**Input:**  \\n- GeoPackage (.gpkg) files containing spatial or hydrologic data.  \\n\\n**Output:**  \\n- Protomaps PMTiles (.pmtiles) files suitable for web-based tile serving or visualization.",
        "summary": "hf_pmtiles provides a containerized workflow for converting GeoPackage hydrologic datasets into Protomaps PMTiles format. Built on Docker, it streamlines tile generation from the Lynker Spatial Hydrofabric, enabling efficient web visualization via tools such as Martin Tiles or PMTiles utilities. This repository supports reproducible geospatial data conversion for hydrology-focused mapping and public data distribution.",
        "keywords": ["PMTiles", "Docker", "GeoPackage", "Hydrofabric", "Lynker Spatial", "mapping", "visualization", "geospatial data", "tile generation", "hydrology"],
        "entities": ["Lynker Spatial", "Protomaps", "Hydrofabric", "Martin Tiles", "CIROH"]
    },
    {
        "metadata": {
        "idurl": 20,
        "title": "hydrotools",
        "source": "https://github.com/CIROH-UA/hydrotools",
        "section_header": "# hydrotools"
        },
        "content": "# Project Title: **hydrotools**  \\n\\n## Project Objective  \\n**OWPHydroTools** is a Python library developed by NOAA’s Office of Water Prediction (OWP) to support hydrologic data scientists. It simplifies retrieving, evaluating, and exporting hydrologic and environmental data using familiar data structures such as `pandas.DataFrame`, `geopandas.GeoDataFrame`, and `numpy.array`. The goal is to make hydrologic data analysis more accessible, consistent, and reproducible.\\n\\n## Core Functionalities  \\n- Provides modular tools (“subpackages”) for specific hydrologic data operations:  \\n  - **events:** Methods for event-based evaluations of hydrometric time series.  \\n  - **nwm_client:** Retrieval of National Water Model (NWM) data from sources such as Google Cloud Platform and NOMADS.  \\n  - **metrics:** Computation of common hydrologic evaluation metrics.  \\n  - **nwis_client:** Easy-to-use interface for retrieving data from the USGS NWIS Instantaneous Values Web Service.  \\n  - **svi_client:** Programmatic access to CDC’s Social Vulnerability Index (SVI).  \\n  - **_restclient:** Generic REST client with built-in caching for GET requests.  \\n- Adheres to NOAA-OWP naming standards for consistent data labeling.  \\n- Allows lightweight installation of only the required subpackages.  \\n- Supports efficient handling of large datasets through canonical labeling and categorical data types.  \\n- Ensures UTC-based, time-zone-naive timestamps across all methods.  \\n\\n## Technical Stack  \\n- **Languages:** Python (>= 3.8).  \\n- **Libraries:** pandas, geopandas, numpy.  \\n- **Data Access:** REST APIs (USGS NWIS, NWM, CDC SVI).  \\n- **Testing Framework:** pytest (as used in GitHub workflows).  \\n\\n## Setup and Usage  \\n### Installation  \\nIt is recommended to install HydroTools within a virtual environment. Example setup:  \\n```bash\\n# Create and activate environment\\npython3 -m venv venv\\nsource venv/bin/activate\\npython3 -m pip install --upgrade pip\\n\\n# Install all tools\\npython3 -m pip install hydrotools\\n\\n# Or install a single subpackage (e.g., NWIS Client)\\npython3 -m pip install hydrotools.nwis_client\\n```  \\n\\n### Data Structure Notes  \\n- Canonical columns (shared across subpackages) include standardized labels such as `value`, `value_time`, `variable_name`, `measurement_unit`, and `series`.  \\n- Non-canonical labels (subpackage-specific) include fields like `usgs_site_code`, `nwm_feature_id`, `nws_lid`, and event boundaries (`start`, `end`).  \\n- Categorical data types (`pandas.Categorical`) are used for efficiency but may require conversion or adjustments for certain operations or geospatial outputs.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Data Science.  \\n- **Affiliation:** NOAA Office of Water Prediction (OWP).  \\n- **Purpose:** Facilitate reproducible, standards-compliant hydrologic data retrieval, analysis, and evaluation.  \\n\\n## Input / Output  \\n**Input:**  \\n- Hydrologic and environmental data from external APIs and web services (e.g., NWIS, NWM, CDC SVI).  \\n\\n**Output:**  \\n- Processed datasets returned as `pandas.DataFrame`, `geopandas.GeoDataFrame`, or `numpy.array` objects suitable for analysis and visualization.",
        "summary": "hydrotools is a Python library from NOAA’s Office of Water Prediction designed for hydrologic data retrieval, analysis, and evaluation. It standardizes workflows through modular subpackages that access National Water Model (NWM) outputs, USGS NWIS data, and CDC SVI indices, returning data in pandas and geopandas formats. With consistent naming conventions and efficient data handling, HydroTools enables reproducible hydrologic analytics, supports event-based evaluations, and simplifies access to environmental datasets for research and operations.",
        "keywords": ["hydrology", "NOAA", "data retrieval", "Python", "National Water Model", "USGS", "NWIS", "CDC SVI", "data analysis", "OWP"],
        "entities": ["NOAA Office of Water Prediction", "National Water Model", "USGS", "CDC", "HydroTools"]
    },
    {
        "metadata": {
        "idurl": 21,
        "title": "lstm",
        "source": "https://github.com/CIROH-UA/lstm",
        "section_header": "# lstm"
        },
        "content": "# Project Title: **LSTM (Basic Model Interface for Streamflow Prediction)**  \\n\\n## Project Objective  \\nThis project provides a **Long Short-Term Memory (LSTM)** deep learning model designed for use within the **Next Generation Water Resources Modeling Framework (NextGen)**.  \\nThe goal is to enable **streamflow prediction** using trained LSTM networks through a standardized **Basic Model Interface (BMI)**, improving model interoperability and accuracy in hydrologic forecasting.  \\nIt allows integration with NextGen and supports model testing, training, and validation for multiple basins using open hydrologic datasets.\\n\\n## Core Functionalities  \\n- **BMI Integration:** Wraps an LSTM model with a Basic Model Interface (BMI) to standardize how models interact within NextGen.  \\n- **NeuralHydrology adaptation:** Uses a modified version of NeuralHydrology’s `CudaLSTM` for forward passes (`nextgen_cuda_lstm.py`).  \\n- **Flexible configurations:** Supports multiple trained LSTM models with varying input attributes and forcings.  \\n- **Example datasets:** Includes forcing and observation data for four USGS gauges (e.g., Falling River, Marsh Creek, Brokenstraw Creek, Narraguagus River).  \\n- **Training & testing setup:** Offers examples for model setup, configuration, and execution using Python or Jupyter Notebooks.  \\n- **Unit tests:** Implements automated tests for BMI model control, variable getters/setters, and grid functions.  \\n- **Environment setup:** Uses `environment.yml` for dependency management via Anaconda or standard Python environments.  \\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Frameworks & Tools:**  \\n  - [PyTorch](https://pytorch.org/) (for model training/inference)  \\n  - [NeuralHydrology](https://neuralhydrology.readthedocs.io/) (adapted model structure)  \\n  - [BMI](https://bmi.readthedocs.io/en/latest/) (interface standard)  \\n  - [Anaconda](https://www.anaconda.com/) (environment management)  \\n- **Dependencies:** Defined in `environment.yml` (includes `xarray==0.14.0` and `llvm-openmp==10.0.0`).  \\n- **Data Formats:** NetCDF (`.nc`), YAML (`.yml`), and PyTorch model weights (`.pt`).  \\n\\n## Setup and Usage  \\n### Environment Setup  \\n1. Create a conda environment:  \\n   ```bash\\n   conda env create -f environment.yml\\n   conda activate bmi_lstm\\n   ```  \\n2. Install the LSTM library:  \\n   ```bash\\n   pip install lstm\\n   ```  \\n\\n### Running the Model  \\n1. Configure the LSTM run by editing or creating a YAML config file in `bmi_config_files/`.  \\n2. Run the LSTM BMI module:  \\n   ```bash\\n   python -m lstm\\n   ```  \\n3. Example workflow:  \\n   - Initialize the model: `model.initialize()`  \\n   - Run one or multiple timesteps: `model.update()` or `model.update_until(model.iend)`  \\n   - Finalize: `model.finalize()`  \\n\\n### Unit Testing  \\nRun the BMI test suite:  \\n```bash\\npython ./lstm/run_bmi_unit_test.py\\n```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Streamflow prediction.  \\n- **Affiliation:** Developed for integration with the **NOAA Office of Water Prediction’s NextGen framework**.  \\n- **Purpose:** Advance hydrologic modeling by coupling deep learning models with standardized interfaces for operational and research applications.  \\n\\n## Input / Output  \\n**Input:**  \\n- Atmospheric forcing data (e.g., NLDAS-based NetCDF files).  \\n- Catchment static attributes from the CAMELS dataset.  \\n- Configuration files (`.yml`) defining parameters, forcings, and runtime periods.  \\n- Trained model weights (`.pt` files).  \\n\\n**Output:**  \\n- Simulated streamflow predictions for target basins.  \\n- Comparison datasets for model evaluation.  \\n- Logs and unit test results verifying BMI functionality.",
        "summary": "The LSTM repository provides a Basic Model Interface (BMI) implementation for deep learning–based streamflow prediction within NOAA’s NextGen framework. Built on PyTorch and NeuralHydrology, it standardizes how trained LSTM models interact with hydrologic systems through configurable YAML files. The project includes example basins, test data, and automated BMI validation routines, enabling reproducible deep learning hydrologic modeling across diverse catchments and integration with NextGen workflows.",
        "keywords": ["LSTM", "BMI", "NextGen", "NeuralHydrology", "PyTorch", "streamflow prediction", "hydrology", "machine learning", "CAMELS", "NOAA"],
        "entities": ["NextGen", "NOAA Office of Water Prediction", "NeuralHydrology", "PyTorch", "CAMELS"]
    },
    {
        "metadata": {
        "idurl": 22,
        "title": "neuralhydrology",
        "source": "https://github.com/CIROH-UA/neuralhydrology",
        "section_header": "# neuralhydrology"
        },
        "content": "# Project Title: **neuralhydrology**  \\n\\n## Project Objective  \\nThe **neuralhydrology** project is a Python library for training neural networks with a strong focus on hydrological applications. Its goal is to provide a modular and flexible framework for hydrology-related deep learning research, allowing users to easily integrate new datasets, architectures, and training components through configuration files without modifying the core code.\\n\\n## Core Functionalities  \\n- Enables training of neural networks for hydrological modeling.  \\n- Provides modular design for easy integration of new datasets, model architectures, and training components.  \\n- Supports configuration-based experimentation to train models without code changes.  \\n- Built on the PyTorch framework for flexibility in deep learning research.  \\n- Includes documentation and research blog for user support and updates.  \\n- Open-source and continuously updated by the AI for Earth Science group at Johannes Kepler University, Linz, Austria.  \\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Framework:** PyTorch  \\n\\n## Setup and Usage  \\n- Users can train neural networks using configuration files without editing source code.  \\n- Documentation and installation guides are available at [neuralhydrology.readthedocs.io](https://neuralhydrology.readthedocs.io).  \\n- Additional examples and research insights are published on the [project blog](https://neuralhydrology.github.io).  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Deep Learning  \\n- **Affiliation:** AI for Earth Science group, Institute for Machine Learning, Johannes Kepler University (Linz, Austria)  \\n- **Purpose:** Support hydrology research by providing a modular and reproducible deep learning framework.  \\n\\n## Input / Output  \\n**Input:** Configuration files and hydrological datasets for model training.  \\n**Output:** Trained neural network models and performance metrics.  \\n\\n## Citation  \\nIf using **NeuralHydrology** in research, cite the following paper:  \\n\\nKratzert, F., Gauch, M., Nearing, G., & Klotz, D. (2022). *NeuralHydrology — A Python library for Deep Learning research in hydrology.* Journal of Open Source Software, 7(71), 4050. DOI: [10.21105/joss.04050](https://doi.org/10.21105/joss.04050)  \\n\\n## Contact  \\n- **Discussions:** [GitHub Discussions](https://github.com/neuralhydrology/neuralhydrology/discussions)  \\n- **Issues:** [GitHub Issues](https://github.com/neuralhydrology/neuralhydrology/issues)  \\n- **Email:** neuralhydrology@googlegroups.com",
        "summary": "neuralhydrology is a modular deep learning framework built on PyTorch for hydrologic research. It allows configuration-based training of neural networks for streamflow and hydrologic variable prediction without modifying source code. Developed by Johannes Kepler University’s AI for Earth Science group, it supports dataset customization, architecture experimentation, and reproducibility, making it a foundational library for machine learning–driven hydrology modeling.",
        "keywords": ["neural networks", "hydrology", "deep learning", "PyTorch", "machine learning", "streamflow", "configuration-based training", "reproducibility"],
        "entities": ["NeuralHydrology", "Johannes Kepler University", "AI for Earth Science"]
    },
    {
        "metadata": {
        "idurl": 23,
        "title": "ngen",
        "source": "https://github.com/CIROH-UA/ngen",
        "section_header": "# ngen"
        },
        "content": "# Project Title: **ngen**  \\n\\n## Project Objective  \\nThe **Next Gen Water Modeling Framework Prototype** (ngen) is designed to modernize and improve hydrologic modeling by addressing limitations in traditional models when applied across varying spatial and temporal scales. It adopts a **data-centric approach**, organizing hydrologic data first and mapping appropriate solutions to that data. This enables more flexible, scalable, and modular simulation of hydrologic processes.  \\nThe framework serves hydrologic researchers, developers, and water resource modelers seeking a more transparent and extensible modeling system capable of handling complex interactions across catchments.\\n\\n## Core Functionalities  \\n- **Data-centric modeling architecture:** Prioritizes organizing hydrologic data before assigning modeling algorithms.  \\n- **Component encapsulation:** Models are organized into catchments and nexuses connected by well-defined APIs to simulate water movement.  \\n- **Polymorphic and modular design:** Enables flexible model configurations and mixing of multiple formulations within one simulation.  \\n- **Recursive abstraction:** Allows nesting of high-resolution or “complex” realizations within catchments for multi-scale representation.  \\n- **Distributed processing:** Supports partitioned and parallel model execution.  \\n- **Testing framework:** Includes automated testing using **Google Test** for unit and integration validation.  \\n- **Debugging and development tools:** Supports debugging via **CMake**, `gdb`, `lldb`, or **GitPod** (browser-based VSCode environment).  \\n- **Extensive documentation and examples:** Includes structural diagrams, dependency listings, installation instructions, and usage examples for both serial and distributed runs.  \\n\\n## Technical Stack  \\n- **Language:** C++ (minimum standard: C++14)  \\n- **Build System:** CMake  \\n- **Testing Framework:** Google Test  \\n- **Optional Development Tools:** GitPod, gdb/lldb for debugging  \\n- **Dependencies:** Listed in `doc/DEPENDENCIES.md`  \\n- **Version:** 0.1.0 (initial development release)  \\n\\n## Setup and Usage  \\n- **Installation:** Follow instructions in `INSTALL.md` to compile and configure.  \\n- **Basic usage:**  \\n  - Run the ngen executable with paths to input GeoJSONs and configuration files:  \\n    ```bash\\n    ngen ./data/catchment_data.geojson \"\" ./data/nexus_data.geojson \"\" ./data/refactored_example_realization_config.json\\n    ```  \\n  - Subset catchments/nexuses using comma-separated IDs or use `\"all\"` for full runs.  \\n  - Use the `--info` flag to view compile-time configuration details.  \\n- **Testing:** Build and execute tests with CMake targets (`test_all`, `test_unit`, or `test_integration`).  \\n- **Debugging:** Enable `-g` compile flag and debug via `gdb`, `lldb`, or GitPod with VSCode extensions.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Computational modeling.  \\n- **Affiliation:** Developed under the **NOAA Office of Water Prediction (OWP)**.  \\n- **Purpose:** Advance the Next Generation Water Modeling Framework by improving modularity, scalability, and data-driven model integration for research and operational hydrology.\\n\\n## Input / Output  \\n**Input:**  \\n- Catchment and nexus GeoJSON files (hydrofabric).  \\n- JSON realization and partition configuration files.  \\n- Optional distributed processing parameters.  \\n\\n**Output:**  \\n- Hydrologic simulation results (format not specified).  \\n- Diagnostic and log files generated during execution.",
        "summary": "ngen (Next Gen Water Modeling Framework) is NOAA’s modular C++ hydrologic modeling framework designed to unify and modernize water resource simulations. Using a data-centric, component-based architecture, it organizes hydrologic domains into catchments and nexuses for flexible configuration and distributed execution. Supporting Google Test, CMake, and parallelization, ngen provides a scalable foundation for research and operational hydrologic modeling within the Office of Water Prediction’s NextGen initiative.",
        "keywords": ["NextGen", "hydrology", "C++", "modular modeling", "distributed computing", "hydrologic simulation", "NOAA", "OWP", "catchment modeling"],
        "entities": ["NOAA Office of Water Prediction", "NextGen", "CMake", "Google Test"]
    },
    {
        "metadata": {
        "idurl": 24,
        "title": "ngen-cal",
        "source": "https://github.com/CIROH-UA/ngen-cal",
        "section_header": "# ngen-cal"
        },
        "content": "# Project Title: **ngen-cal**  \\n\\n## Project Objective  \\n**ngen-cal** provides supporting code and workflows for automated calibration of [NGen](https://github.com/noaa-owp/ngen) Formulations using **Dynamic Dimensioned Search (DDS)**. It enables the generation of NGen parameter formulation permutations and executes them through the NGen framework driver.\\n\\n## Core Functionalities  \\n- Automated calibration of NGen Formulations using DDS.  \\n- Python-based workflow for generating parameter permutations.  \\n- Executes runs through the NGen framework driver.  \\n- Includes unit testing using pytest.  \\n- Provides issue tracking and contribution guidelines for user support.  \\n\\n## Technical Stack  \\n- **Language:** Python.  \\n- **Testing Framework:** pytest.  \\n- **Dependencies:** Listed in `requirements.txt`.  \\n- **Status:** Pre-release development (see CHANGELOG.md).  \\n\\n## Setup and Usage  \\n- **Installation:** `TODO` (INSTALL.md referenced but not completed).  \\n- **Configuration:** `TODO`.  \\n- **Usage:** `TODO`.  \\n\\n### Testing Instructions  \\n1. Install dependencies:  \\n   ```bash\\n   pip install -r python/requirements.txt\\n   ```  \\n   Or create and activate a virtual environment:  \\n   ```bash\\n   mkdir venv\\n   virtualenv ./venv\\n   source ./venv/bin/activate\\n   pip install -r python/requirements.txt\\n   ```  \\n2. Run tests using pytest:  \\n   ```bash\\n   pytest --log-cli-level 0 python/ngen-cal/test/\\n   ```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Model Calibration.  \\n- **Affiliation:** Not specified.  \\n- **Purpose:** Support automated calibration workflows for NGen Formulations using DDS.  \\n\\n## Input / Output  \\n**Input:**  \\n- NGen parameter formulations.  \\n- Python dependencies listed in `requirements.txt`.  \\n\\n**Output:**  \\n- Calibration results from DDS-based workflow runs.",
        "summary": "ngen-cal provides a Python-based system for automatically calibrating NextGen model formulations using the Dynamic Dimensioned Search (DDS) algorithm. It generates parameter permutations, executes runs through the NGen driver, and includes pytest-based validation for testing. The repository supports reproducible calibration workflows and aims to streamline parameter optimization within the NextGen hydrologic modeling framework.",
        "keywords": ["NextGen", "DDS", "model calibration", "hydrology", "automation", "Python", "parameter optimization", "pytest"],
        "entities": ["NextGen", "DDS", "CIROH"]
    },
    {
        "metadata": {
        "idurl": 25,
        "title": "ngen-datastream",
        "source": "https://github.com/CIROH-UA/ngen-datastream",
        "section_header": "# ngen-datastream"
        },
        "content": "# Project Title: **ngen-datastream**  \\n\\n## Project Objective  \\nThe **NextGen Research DataStream** aims to make hydrologic simulations more transparent, collaborative, and reproducible. It provides an open-source, community-editable configuration of the [NextGen Framework](https://github.com/NOAA-OWP/ngen), enabling researchers and hydrologists to contribute to improving streamflow predictions. By publicly sharing forcings, outputs, and configurations, the project allows regional experts to iteratively refine and enhance hydrologic modeling capabilities hosted in the AWS cloud.\\n\\n## Core Functionalities  \\n- Provides an **array of daily NextGen-based hydrologic simulations** hosted on AWS.  \\n- Offers a **community-editable configuration** to improve streamflow predictions collaboratively.  \\n- Includes **DataStreamCLI**, a backend tool that:  \\n  - Automates data collection and input formatting for NextGen runs.  \\n  - Orchestrates NextGen execution through *NextGen In a Box (NGIAB)*.  \\n  - Manages outputs efficiently and reproducibly.  \\n  - Integrates with tools such as **hfsubset**, **NextGenIn A Box**, and TEEHR. \n- Supplies an interactive CLI guide to help users understand and configure runs. \n- Provides infrastructure documentation, including AWS architecture and current project status. \n- Hosts open discussions and contribution guides to foster community collaboration. \n\n## Technical Stack \n- Programming Languages: Not specified. \n- Frameworks/Libraries: \n - NextGen In A Box (NGIAB) \n - hfsubset \n - TEEHR \n- Infrastructure: AWS Cloud, Terraform (for Infrastructure as Code). \n\n## Setup and Usage \nUsers install DataStreamCLI by following the installation guide. The interactive guide script helps users explore the repository and construct commands for simulations. \nBasic usage involves: \n1. Obtaining a hydrofabric file (e.g., via hfsubset). \n2. Running DataStreamCLI with parameters specifying start/end times, configuration, and hydrofabric input. \n3. Viewing simulation outputs in the designated ngen-run/outputs directory. \n\n## Project Context & Domain \n- Domain: Hydrology / Environmental Modeling. \n- Institutions: Developed under the Center for Hydrometeorology and Remote Sensing (CIROH) and linked to NOAA’s NextGen Framework. \n- Hosting: AWS Cloud infrastructure. \n- Purpose: Enhance accessibility, collaboration, and reproducibility in hydrologic modeling and streamflow prediction. \n\n## Input / Output \nInput: \n- Hydrofabric geopackage file (e.g., watershed boundaries, flowlines, network, forcing weights). \n- NextGen model configuration JSON files. \n- Time domain parameters for simulation. \n\nOutput: \n- NextGen simulation results stored in a local directory structure (ngen-run/outputs). \n- Data includes processed hydrologic model outputs for streamflow and related variables.",
        "summary": "ngen-datastream powers CIROH’s collaborative hydrologic modeling ecosystem by orchestrating reproducible NextGen simulations on AWS. It integrates the DataStreamCLI tool to automate input preparation, execution, and output management, while providing an open configuration that enables researchers to iteratively improve streamflow predictions. Through community contributions and transparent AWS-hosted workflows, ngen-datastream promotes shared development and operational research for the NextGen framework.",
        "keywords": ["NextGen", "DataStreamCLI", "hydrologic modeling", "AWS", "hfsubset", "NGIAB", "TEEHR", "collaboration", "CIROH", "streamflow"],
        "entities": ["CIROH", "NextGen", "AWS", "NOAA", "NextGen In a Box", "TEEHR", "hfsubset"]
    },
    {
        "metadata": {
        "idurl": 26,
        "title": "NGIAB_data_preprocess",
        "source": "https://github.com/CIROH-UA/NGIAB_data_preprocess",
        "section_header": "# NGIAB_data_preprocess"
        },
        "content": "# Project Title: **NGIAB_data_preprocess**  \\n\\n## Project Objective  \\nTools to prepare data for running a [NextGen](https://github.com/NOAA-OWP/ngen)-based simulation using NGIAB by letting users select a catchment on an interactive map, choose a date range, and generate a runnable package with a few clicks. Intended for hydrology practitioners who want to subset hydrofabric data, compute forcings, and build NGIAB-ready configurations efficiently.\\n\\n## Core Functionalities  \\n- **Interactive map workflow:** Select a catchment/point of interest, pick a time period, and execute steps to subset, create forcings, and create a realization.  \\n- **Hydrofabric subsetting:** Delineates the upstream network for the chosen feature and outputs a geopackage (`.gpkg`).  \\n- **Forcings generation:** Computes weighted-mean forcings from **NWM retrospective v3** or **AORC 1 km** gridded datasets (weights via *exactextract* and computed with NumPy).  \\n- **Configuration creation:** Produces NGIAB-ready configs (e.g., `realization.json`, `troute.yaml`, and per-catchment settings).  \\n- **Optional non-interactive run:** Can automatically run NGIAB’s Docker distribution on the prepared folder.  \\n- **CLI & Map apps:** Run via `ngiab-prep` CLI or `map_app` with UV/pipx support; includes combined flags for end-to-end prep.\\n\\n## Technical Stack  \\n- **Language:** Python (implied by UV/pip installation and module invocation).  \\n- **Key Libraries/Tools:** NumPy; *exactextract*; Docker (NGIAB distribution); Astral **UV**; `pipx`/`uvx`.  \\n- **Ecosystem/Models:** NextGen; NGIAB; optional TEEHR for evaluation (via NGIAB).  \\n- **OS Support:** Officially supported on macOS and Ubuntu; Windows via WSL.\\n\\n## Setup and Usage  \\n- **Run without install:** Use `uvx` to invoke the CLI (`uvx ngiab-prep`) or the map app (`uvx --from ngiab-data-preprocess map_app`).  \\n- **Install with UV:** Create a venv (`uv venv`), install (`uv pip install ngiab_data_preprocess`), then run `uv run cli --help` or `uv run map_app`.  \\n- **Legacy pip (virtual env recommended):** `pip install 'ngiab_data_preprocess'` then `python -m map_app`.  \\n- **Typical workflow (map app):** 1) Create subset GPKG → 2) Create Forcing from Zarrs → 3) Create Realization; outputs saved under `~/ngiab_preprocess_output/<your-input-feature>/`.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Hydrologic modeling.  \\n- **Funding & Affiliation:** Funded by **NOAA** and awarded to **CIROH** via NOAA Cooperative Agreement with **The University of Alabama** (NA22NWS4320003).  \\n- **Data Sources:** Hydrofabric v2.2; NWM retrospective v3 forcings; AORC 1 km gridded data.\\n\\n## Input / Output  \\n**Input:** Selected feature (catchment/gage/lat-lon), date range, Hydrofabric v2.2 geometries & attributes, and choice of forcing source (NWM v3 or AORC).  \\n**Output:** Subset geopackage (`.gpkg`), computed forcings, and NGIAB configuration files (`realization.json`, `troute.yaml`, per-catchment configs); optional NGIAB run artifacts in the output folder.\\n\\n## Known Limitations  \\n- Outputs only a **single default realization** (NOAH + CFE + SLoTH).  \\n- **Calibration:** Downloads calibrated parameters when available; falls back to defaults if not.  \\n- **Evaluation & Visualization:** Use NGIAB interactively (via `guide.sh`) for TEEHR evaluation and visualization.",
        "summary": "NGIAB_data_preprocess streamlines the preparation of hydrologic model inputs for NextGen simulations by enabling users to subset hydrofabric data, compute weighted forcings, and create NGIAB-ready configurations via an interactive map or command line. Developed under NOAA–CIROH funding, it supports NWM v3 and AORC datasets, producing standardized geopackages, forcings, and realization files. The tool can optionally execute NGIAB runs automatically and integrates with TEEHR for model evaluation.",
        "keywords": ["NGIAB", "NextGen", "hydrofabric", "data preprocessing", "NWM", "AORC", "Python", "CLI", "NOAA", "CIROH"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "NextGen", "NGIAB", "TEEHR"]
    },
    {
        "metadata": {
        "idurl": 27,
        "title": "ngiab-cal",
        "source": "https://github.com/CIROH-UA/ngiab-cal",
        "section_header": "# ngiab-cal"
        },
        "content": "# Project Title: **ngiab-cal**  \\n\\n## Project Objective  \\n**ngiab-cal** is a Python command-line interface (CLI) tool that simplifies hydrologic model calibration for **NextGen In A Box (NGIAB)** workflows. It automates the creation of calibration directories and configuration files necessary to run a modified version of **ngen-cal**, streamlining calibration setup, execution, and parameter management.\\n\\n## Core Functionalities  \\n- Automates calibration configuration creation.  \\n- Runs calibration processes using Docker.  \\n- Copies calibrated parameters back to model configurations.  \\n- Generates configuration files compatible with the `ngiab_cal` branch of **ngen-cal**.  \\n- Supports warmup, calibration, and validation workflows.  \\n- Provides both basic and advanced command-line options for calibration setup.  \\n- Downloads USGS streamflow data for calibration.  \\n- Creates visual plots and validation results after calibration.  \\n\\n## Technical Stack  \\n- **Language:** Python 3.10+  \\n- **Dependencies:** Docker, Internet connection for USGS data download.  \\n- **Containerization:** Uses Docker image `joshcu/ngiab-cal:demo`.  \\n- **Supported Models:** CFE (Conceptual Functional Equivalent), Noah-OWP-Modular.  \\n- **Calibration Algorithm:** Dynamically Dimensioned Search (DDS).  \\n- **Configuration Files:** YAML-based calibration settings (`ngen_cal_conf.yaml`).  \\n\\n## Setup and Usage  \\n### Installation Options  \\n- **Using `uvx` (recommended):**  \\n  ```bash\\n  uvx ngiab-cal --help\\n  uv tool install ngiab-cal\\n  ```  \\n- **Using `pipx`:**  \\n  ```bash\\n  pipx install ngiab-cal\\n  ngiab-cal --help\\n  ```  \\n- **Using pip:**  \\n  ```bash\\n  pip install ngiab-cal\\n  python -m ngiab_cal --help\\n  ```  \\n\\n### Basic Usage  \\n```bash\\nngiab-cal /path/to/ngiab/data/folder -g USGS_GAGE_ID --run -i 200\\n```  \\nThe tool will create `calibration/ngen_cal_conf.yaml`, run DDS calibration, and store results in `./calibration/Output/Calibration_Run/`.\\n\\n### Example Workflow: Calibrating CAMELS Basins  \\nAutomated script example for calibrating multiple CAMELS basins, integrating data preparation and calibration through `ngiab-cal` and `ngiab_data_preprocess`.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Model Calibration / Water Resources Modeling.  \\n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology).  \\n- **Purpose:** Simplify and automate NGIAB model calibration through a Python CLI integrating Docker-based ngen-cal execution.\\n\\n## Input / Output  \\n**Input:** NGIAB directory path, USGS gage ID, calibration templates, and optional tuning parameters.  \\n**Output:** YAML calibration file, DDS run results, updated configurations, and validation plots.",
        "summary": "ngiab-cal automates the calibration of NGIAB hydrologic models using Dockerized execution of a modified ngen-cal framework. It creates and manages configuration files, runs the Dynamic Dimensioned Search (DDS) algorithm, downloads USGS streamflow data, and integrates results back into NGIAB configurations. This tool enables reproducible, parameter-optimized model tuning across multiple catchments and basins under CIROH’s NextGen ecosystem.",
        "keywords": ["NGIAB", "NextGen", "DDS", "model calibration", "hydrology", "USGS", "Python", "Docker", "CFE", "Noah-OWP-Modular"],
        "entities": ["CIROH", "NOAA", "NextGen", "USGS", "ngen-cal"]
    },
    {
        "metadata": {
        "idurl": 28,
        "title": "ngiab-client",
        "source": "https://github.com/CIROH-UA/ngiab-client",
        "section_header": "# ngiab-client"
        },
        "content": "# Project Title: **ngiab-client**  \\n\\n## Project Objective  \\nThe **NGIAB Output and Datastream Visualizer** is a **Tethys + React web application** designed to visualize outputs from the **NextGen In A Box (NGIAB)** hydrologic modeling system. Developed under the **NOAA–CIROH Cooperative Agreement** with The University of Alabama, it provides an interactive platform for geospatial, temporal, and performance analysis of NextGen model results.\\n\\n## Core Functionalities  \\n- **Geospatial Visualization:** Displays catchments, nexus points, and river reaches.  \\n- **Time Series Analysis:** Interactive plots for catchments, nexus points, and `troute` variables.  \\n- **TEEHR Output Visualization:** Plots metrics such as Kling-Gupta Efficiency (KGE), Nash–Sutcliffe Efficiency (NSE), and relative bias.  \\n- **S3 Datastream Integration:** Downloads and visualizes model outputs stored in the NextGen Datastream.  \\n- **Multi-run Management:** Supports multiple NGIAB runs via `ngiab_visualizer.json`.  \\n- **Dockerized Deployment:** Runs in containers for consistency across environments.  \\n- **Frontend–Backend Integration:** Combines a React frontend with a Tethys (Django) backend for dynamic interaction.  \\n\\n## Technical Stack  \\n- **Frontend:** React + Webpack  \\n- **Backend:** Tethys Platform (Django-based)  \\n- **Languages:** Python, JavaScript  \\n- **Containerization:** Docker  \\n- **Package Managers:** npm (JS), PDM (Python)  \\n- **Database:** SQLite  \\n- **Testing:** Jest, pytest, flake8, eslint  \\n- **Supported Node:** v20  \\n\\n## Setup and Usage  \\n- **Assisted Launch:** Use `ViewOnTethys.sh` to start automatically from NGIAB-CloudInfra.  \\n- **Manual Launch:** Configure environment variables, edit `ngiab_visualizer.json`, and run Docker container manually.  \\n- **Development Mode:** Run Tethys and Webpack servers concurrently with Node and PDM.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Visualization / Model Evaluation.  \\n- **Affiliation:** NOAA, CIROH, and The University of Alabama.  \\n- **Purpose:** Provide an interactive, web-based environment for exploring, comparing, and evaluating NextGen simulation results.\\n\\n## Input / Output  \\n**Input:** NGIAB outputs (TEEHR metrics, model data, and metadata).  \\n**Output:** Interactive geospatial and time-series dashboards accessible via browser.",
        "summary": "ngiab-client is a Tethys + React web app for visualizing NGIAB (NextGen In A Box) model outputs. It supports geospatial and temporal visualization, integrates TEEHR performance metrics, and connects directly to the NextGen Datastream on AWS S3. Combining a Django backend with a React frontend, the tool provides scientists and researchers with an accessible interface to explore model performance and hydrologic simulation results.",
        "keywords": ["NGIAB", "NextGen", "TEEHR", "Tethys", "React", "visualization", "Docker", "Python", "JavaScript", "AWS"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "Tethys Platform", "NextGen", "TEEHR"]
    },
    {
        "metadata": {
        "idurl": 29,
        "title": "NGIAB-CloudInfra",
        "source": "https://github.com/CIROH-UA/NGIAB-CloudInfra",
        "section_header": "# NGIAB-CloudInfra"
        },
        "content": "# Project Title: **NGIAB-CloudInfra**  \\n\\n## Project Objective  \\nProvide a containerized, user-friendly way to run the **NextGen** National Water Resources Modeling Framework locally. The project focuses on simplifying setup and execution, giving users control over inputs, configurations, and runs while supporting open research, evaluation, and visualization workflows.\\n\\n## Core Functionalities  \\n- **Run NextGen locally:** Experiment with the framework on a personal machine.  \\n- **Control over inputs:** Choose regions/basins and adjust input data as needed.  \\n- **Simplified setup:** Deploy using Docker containers and provided guide scripts.  \\n- **Open research:** Encourage transparency with open-source tooling.  \\n- **Evaluation tools:** Integrated **TEEHR** capabilities for model evaluation.  \\n- **Visualization:** Built-in support to visualize outputs via the **Tethys Platform**.  \\n- **Guide scripts & docs:** `guide.sh`, `runTeehr.sh`, `viewOnTethys.sh` and a comprehensive `docs/` tree to navigate usage and options.\\n\\n## Technical Stack  \\n- **Containerization:** Docker  \\n- **Evaluation/Visualization:** TEEHR; Tethys Platform  \\n- **Orchestrations/CI:** GitHub Actions workflows (for image build/push)  \\n- **Models/Ecosystem:** NextGen framework  \\n\\n## Setup and Usage  \\n- **Recommended entrypoints:** Use the guide scripts (`guide.sh`, `runTeehr.sh`, `viewOnTethys.sh`); pass `-h` to see flags.  \\n- **Documentation:** See the `docs/` folder for details; broader docs mirrored at docs.ciroh.org/products/ngiab.  \\n- **Container image:** Releases are published to Docker Hub (`awiciroh/ciroh-ngen-image`).\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Water resources modeling.  \\n- **Funding & Affiliation:** Sponsored by **NOAA**, awarded to **CIROH** through the NOAA Cooperative Agreement with **The University of Alabama** (NA22NWS4320003).  \\n- **Related resources:** Case-study assets and setup details are available via the linked HydroShare resource for the Provo River NGIAB case study.\\n\\n## Input / Output  \\n**Input:** User-selected regions/basins and corresponding NextGen input datasets/configurations (high-level control via scripts and container).  \\n**Output:** NextGen model outputs suitable for **TEEHR** evaluation and **Tethys** visualization (plots, metrics, and geospatial visualizations).",
        "summary": "NGIAB-CloudInfra provides a Docker-based infrastructure for running NextGen simulations locally with integrated support for TEEHR evaluation and Tethys visualization. Developed under NOAA–CIROH, it simplifies hydrologic modeling setup using guide scripts and open-source workflows. This containerized environment allows transparent, reproducible research in water resource modeling with compatibility for CI/CD and community sharing through docs.ciroh.org.",
        "keywords": ["NGIAB", "NextGen", "TEEHR", "Docker", "Tethys", "CI/CD", "hydrology", "local modeling", "cloud infrastructure"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "NextGen", "TEEHR", "Tethys Platform"]
    },
    {
        "metadata": {
        "idurl": 30,
        "title": "NGIAB-HPCInfra",
        "source": "https://github.com/CIROH-UA/NGIAB-HPCInfra",
        "section_header": "# NGIAB-HPCInfra"
        },
        "content": "# Project Title: **NGIAB-HPCInfra**  \\n\\n## Project Objective  \\nThe **NextGen In A Box (NGIAB)** project provides a **containerized and user-friendly solution** for running the **NextGen National Water Resources Modeling Framework** locally or on high-performance computing (HPC) systems. This repository enables users to execute NextGen simulations using **Singularity containers**, offering flexibility to control inputs, configurations, and execution in both serial and parallel modes.\\n\\n## Core Functionalities  \\n- Run NextGen locally or on HPC systems using Singularity containers.  \\n- Automatically detect system architecture (ARM or x86) and pull the appropriate container image.  \\n- Download and run the latest Singularity NextGen image from Docker Hub.  \\n- Allow users to specify and attach input data directories for simulations.  \\n- Support Serial, Parallel, and Interactive Shell execution modes.  \\n- Provide commands to download pre-packaged sample datasets (AWI-007, AWI-008, AWI-009).  \\n- Include a `guide.sh` script for model execution guidance.  \\n- Provide a Singularity definition file (`singularity_ngen.def`) and sample execution script (`HelloNGEN.sh`).\\n\\n## Technical Stack  \\n- **Languages:** Shell (Bash)  \\n- **Containerization:** SingularityCE  \\n- **Supported Architectures:** ARM and x86  \\n- **Dependencies:** SingularityCE, wget, tar, git  \\n- **Data Sources:** Pre-packaged datasets available via AWS S3 (e.g., AWI-007, AWI-008, AWI-009).\\n\\n## Setup and Usage  \\n1. Access a compute node (e.g., via SLURM):  \\n   ```bash\\n   srun --partition=<partition_name> --nodes=1 --ntasks=1 --time=<hh:mm:ss> --pty bash\\n   ```  \\n2. Install or verify SingularityCE: `singularity --version`.  \\n3. Prepare data:  \\n   ```bash\\n   mkdir -p NextGen/ngen-data\\n   cd NextGen/ngen-data\\n   wget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\\n   tar -xf AWI_16_10154200_009.tar.gz\\n   ```  \\n4. Run the model:  \\n   ```bash\\n   git clone https://github.com/CIROH-UA/NGIAB-HPCInfra.git\\n   cd NGIAB-HPCInfra\\n   ./guide.sh\\n   ```  \\n5. Choose mode: Serial, Parallel, or Interactive Shell.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / HPC Simulation / Water Resources Modeling.  \\n- **Affiliation:** CIROH and The University of Alabama.  \\n- **Funding:** NOAA under Cooperative Agreement NA22NWS4320003.  \\n- **Purpose:** Enable scalable NextGen model execution on HPC systems using Singularity containers.\\n\\n## Input / Output  \\n**Input:** Hydrologic datasets (e.g., `.tar.gz`), configuration and realization files, and simulation parameters.  \\n**Output:** NextGen simulation results generated within HPC-compatible storage directories.",
        "summary": "NGIAB-HPCInfra enables execution of NextGen hydrologic simulations on HPC systems using Singularity containers. Supporting both ARM and x86 architectures, it downloads ready-to-run container images and data packages, providing scripts for guided execution in serial or parallel modes. Developed under NOAA–CIROH, it extends NGIAB’s portability to high-performance clusters, making scalable, reproducible water modeling accessible to researchers and institutions.",
        "keywords": ["NGIAB", "NextGen", "HPC", "Singularity", "containerization", "hydrology", "AWS", "parallel computing", "NOAA", "CIROH"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "NextGen", "SingularityCE"]
    },
    {
        "metadata": {
        "idurl": 31,
        "title": "ngiab-teehr",
        "source": "https://github.com/CIROH-UA/ngiab-teehr",
        "section_header": "# ngiab-teehr"
        },
        "content": "# Project Title: **ngiab-teehr**  \\n\\n## Project Objective  \\nThe **ngiab-teehr** repository provides functionality for coupling [TEEHR](https://rtiinternational.github.io/teehr/) with [NextGen-In-A-Box (NGIAB)](https://github.com/jameshalgren/NGIAB-CloudInfra) simulation output. The repository enables users to perform TEEHR evaluations on NGIAB-generated data.\\n\\n## Core Functionalities  \\n- Integrates TEEHR evaluations with NGIAB simulation outputs.  \\n- Provides an example script (`example_guide.sh`) for running evaluations.  \\n- Includes workflow scripts such as `scripts/teehr_ngen.py` and `scripts/utils.py`.  \\n- Supports customization of metrics calculated by TEEHR.  \\n- Uses GitHub Actions to automatically build and push Docker images to the AWI CIROH registry.  \\n- Allows local Docker builds and pushes.  \\n\\n## Technical Stack  \\n- **Languages/Tools:** Python, Shell.  \\n- **Containerization:** Docker.  \\n- **Automation:** GitHub Actions.  \\n\\n## Setup and Usage  \\n### Running TEEHR Evaluation Example  \\nThe included script `example_guide.sh` demonstrates how to run a TEEHR evaluation using NGIAB output.\\n\\n### Building and Pushing the Docker Image  \\n1. Create a branch from `main`.  \\n2. Edit `scripts/teehr_ngen.py` and/or `scripts/utils.py` as needed.  \\n3. Update the `Changelog` file.  \\n4. Submit a pull request and merge it.  \\n5. Tag and push the new version:  \\n   ```bash\\n   git checkout main\\n   git pull\\n   git tag -a v0.x.x -m \"version 0.x.x\"\\n   git push origin v0.x.x\\n   ```  \\n   This triggers a GitHub Action to build and push the Docker image with the new tag and the `latest` tag to the AWI CIROH registry.  \\n\\n### Local Build and Push  \\n```bash\\ndocker build -t awiciroh/ngiab-teehr:<tag name> .\\ndocker push awiciroh/ngiab-teehr:<tag name>\\n```  \\n\\n### Image Tag Usage  \\nSpecify the Docker image tag in the `guide.sh` script to run customized evaluations.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Model Evaluation / Data Processing.  \\n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology), NOAA, The University of Alabama.  \\n- **Purpose:** Facilitate TEEHR-based evaluation of NGIAB simulation outputs through an automated Docker-based workflow.  \\n\\n## Input / Output  \\n**Input:**  \\n- NGIAB simulation output data.  \\n- TEEHR evaluation configurations and scripts.  \\n\\n**Output:**  \\n- Evaluation results generated by TEEHR.  \\n- Docker images containing updated workflows.",
        "summary": "ngiab-teehr integrates TEEHR evaluation workflows with NGIAB simulation outputs to assess hydrologic model performance. It automates evaluation execution and Docker image management via GitHub Actions, supporting both cloud and local builds. By coupling TEEHR metrics with NextGen In A Box results, it streamlines reproducible performance analysis and provides customizable evaluation scripts for hydrologic research and operational benchmarking.",
        "keywords": ["TEEHR", "NGIAB", "NextGen", "Docker", "model evaluation", "GitHub Actions", "Python", "hydrology"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "TEEHR", "NextGen In A Box"]
    },
    {
        "metadata": {
        "idurl": 32,
        "title": "ngiab-website",
        "source": "https://github.com/CIROH-UA/ngiab-website",
        "section_header": "# ngiab-website"
        },
        "content": "# Project Title: **ngiab-website**  \\n\\n## Project Objective  \\nThis project is a web application built using React and Vite. It provides a frontend interface designed for deployment and optimized performance. The README describes how to run, build, and preview the app locally.\\n\\n## Core Functionalities  \\n- **Development Mode:** Run the app locally with `npm start` at [http://localhost:3000](http://localhost:3000).  \\n- **Production Build:** Create an optimized production build with `npm run build`.  \\n- **Local Preview:** Preview the production build using `npm run serve`.  \\n- **Automatic Reload:** The app reloads when code changes are detected.  \\n- **Error Reporting:** Displays linting errors in the console during development.\\n\\n## Technical Stack  \\n- **Languages/Frameworks:** React 19, Vite 6.  \\n- **Libraries/Tools:** React Router 7, Styled Components, Tailwind CSS, Axios.  \\n- **Build System:** Vite.  \\n- **Documentation References:** React, Vite, React Router, and Styled Components documentation.\\n\\n## Setup and Usage  \\n1. Install dependencies using npm.  \\n2. Run `npm start` to launch the development server.  \\n3. Build the project for production using `npm run build`.  \\n4. Preview the production build with `npm run serve`.\\n\\n## Project Context & Domain  \\n- **Domain:** Web Development / Frontend Interface.  \\n- **Affiliation:** Not specified.  \\n- **Purpose:** Provide a React-based web interface optimized for development and deployment using Vite.\\n\\n## Input / Output  \\n**Input:** Source code written in React, styled with Tailwind CSS and Styled Components.  \\n**Output:** Production-ready web application compiled into the `dist` directory.",
        "summary": "ngiab-website is a lightweight React + Vite frontend designed for efficient development, testing, and deployment. Using modern libraries like Tailwind CSS and Styled Components, it provides an optimized, hot-reloading environment for building responsive web interfaces, likely serving as a frontend for NGIAB or related CIROH applications.",
        "keywords": ["React", "Vite", "Tailwind CSS", "Styled Components", "web development", "frontend", "JavaScript", "CIROH"],
        "entities": ["React", "Vite", "CIROH"]
    },
    {
        "metadata": {
        "idurl": 33,
        "title": "NWM_ML",
        "source": "https://github.com/CIROH-UA/NWM_ML",
        "section_header": "# NWM_ML"
        },
        "content": "# Project Title: **NWM_ML**  \\n\\n## Project Objective  \\nThe **NWM-ML** project extends the NOAA National Water Model (NWM) using machine learning to improve streamflow predictions affected by water resource infrastructure across the Western United States. The project aims to enhance the NWM's 30-day operational forecasts into season-to-season water supply forecasting tools to support environmental management, agriculture, and urban water use planning.\\n\\n## Core Functionalities  \\n- Couples the National Water Model with machine learning workflows for improved streamflow prediction.  \\n- Integrates multiple data sources, including hydrological, climatological, and land cover datasets.  \\n- Trains models for application in the Great Salt Lake (GSL) basin with plans to expand to the Upper Colorado River Basin.  \\n- Demonstrates a proof-of-concept integrated water resources management tool linking NWM with lake and wetland modules.  \\n- Utilizes the physically based NWM v2.1 retrospective dataset (with future integration of NWM v3.0).  \\n- Incorporates seasonal cycle representation through sine and cosine functions to model annual hydrological variability.\\n\\n## Technical Stack  \\n- **Base Model:** NOAA National Water Model (NWM).  \\n- **Machine Learning Framework:** Not specified.  \\n- **Data Sources:** StreamStats, NWM v2.1, US Bureau of Reclamation, NRCS SNOTEL, NLDAS, USGS, Copernicus DEM.\\n\\n## Setup and Usage  \\n- Training and testing use NWM retrospective data and observed streamflow records.  \\n- The workflow integrates catchment-level attributes and seasonal indices for improved streamflow correction.  \\n- Initial tests focus on the Great Salt Lake basin, expanding later to the Upper Colorado River Basin.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Water Resources Management.  \\n- **Affiliation:** NOAA, CIROH, The University of Alabama.  \\n- **Purpose:** Enhance NWM forecasts by incorporating ML-driven corrections that account for human and climatic influences.\\n\\n## Input / Output  \\n**Input:** Retrospective NWM data, hydrologic catchment attributes, climate variables, and observed streamflow.  \\n**Output:** ML-corrected streamflow predictions and performance metrics.\\n\\n## Funding Acknowledgement  \\nFunded by NOAA and awarded to CIROH under Cooperative Agreement NA22NWS4320003.",
        "summary": "NWM_ML integrates machine learning techniques into the NOAA National Water Model to improve streamflow prediction and forecasting. Focusing on the Great Salt Lake basin, it combines hydrologic, climate, and land cover datasets with retrospective NWM outputs to train models that better represent seasonal variability and anthropogenic influences. The project advances data-driven water resource forecasting across the western United States.",
        "keywords": ["National Water Model", "machine learning", "streamflow", "forecasting", "hydrology", "NOAA", "CIROH", "SNOTEL", "climate modeling"],
        "entities": ["NOAA", "CIROH", "The University of Alabama", "National Water Model", "USGS", "NRCS", "NLDAS"]
    },
    {
        "metadata": {
        "idurl": 34,
        "title": "nwmurl",
        "source": "https://github.com/CIROH-UA/nwmurl",
        "section_header": "# nwmurl"
        },
        "content": "# Project Title: **nwmurl**  \\n\\n## Project Objective  \\n**nwmurl** is a Python library developed by CIROH (2023). It provides utility functions designed to subset and generate National Water Model (NWM) data URLs. The tool simplifies the process of accessing NWM data for purposes such as analysis, modeling, and visualization.\\n\\n## Core Functionalities  \\n- Generates NWM data URLs for operational and retrospective datasets.  \\n- Allows customization of data retrieval parameters including date ranges, forecast cycles, variables, regions, and configurations.  \\n- Supports multiple data sources (NOAA and CIROH-hosted).  \\n- Exports generated URLs to text files (optional).  \\n- Includes examples for both operational and retrospective workflows.\\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Installation:** `pip install nwmurl`  \\n- **Functions:** `generate_urls_operational()` and `generate_urls_retro()` for distinct data modes.\\n\\n## Setup and Usage  \\nExample usage for generating URLs for operational datasets:  \\n```python\\nimport nwmurl\\nfile_list = nwmurl.generate_urls_operational(start_date, end_date, fcst_cycle, lead_time, varinput, geoinput, runinput, urlbaseinput, meminput, write_to_file)\\n```  \\nAnd for retrospective datasets:  \\n```python\\nfile_list = nwmurl.generate_urls_retro(start_date, end_date, urlbaseinput, selected_object_types, selected_var_types, write_to_file)\\n```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Data Access / Water Modeling.  \\n- **Affiliation:** CIROH (2023).  \\n- **Purpose:** Simplify generation of NWM data URLs for modeling, visualization, and analysis workflows.\\n\\n## Input / Output  \\n**Input:** Date ranges, forecast cycles, variable selections, and geographic parameters.  \\n**Output:** Lists or text files of generated NWM URLs for easy data access.\\n\\n## License  \\nNot specified.",
        "summary": "nwmurl is a Python library that automates the generation of National Water Model (NWM) data URLs for operational and retrospective datasets. It allows configurable retrieval parameters and supports output to lists or text files, simplifying hydrologic data acquisition for modeling, visualization, and analysis within CIROH’s data ecosystem.",
        "keywords": ["National Water Model", "NWM", "Python", "data access", "URL generation", "retrospective data", "forecast data", "hydrology"],
        "entities": ["CIROH", "NOAA", "National Water Model"]
    },
    {
        "metadata": {
        "idurl": 35,
        "title": "OPG-CNN-Northern-Utah-CIROH-Workshop",
        "source": "https://github.com/CIROH-UA/OPG-CNN-Northern-Utah-CIROH-Workshop",
        "section_header": "# OPG-CNN-Northern-Utah-CIROH-Workshop"
        },
        "content": "# Project Title: **OPG-CNN-Northern-Utah-CIROH-Workshop**  \\n\\n## Project Objective  \\nThe **OPG-CNN-Northern-Utah-CIROH-Workshop** repository provides materials for a workshop on applying Convolutional Neural Networks (CNNs) to predict Orographic Precipitation Gradients (OPGs) in complex terrain. The project aims to improve precipitation forecasts by downscaling coarse-resolution climate model outputs using CNNs trained on ERA5 data.\\n\\n## Core Functionalities  \\n- Introduces participants to CNNs and their hydro-meteorological applications.  \\n- Explains CNN architecture, learning principles, and Explainable AI (XAI) concepts.  \\n- Guides hyperparameter tuning and architecture modification for downscaling experiments.  \\n- Includes scripts for predicting OPGs in northern Utah using ERA5 inputs.  \\n- Supports both CIROH 2i2c JupyterHub and local execution via conda environments.\\n\\n## Technical Stack  \\n- **Language:** Python  \\n- **Dependencies:** Provided via `environment.yml` and `requirements.txt`.  \\n- **Frameworks:** Not specified (likely TensorFlow/PyTorch per workshop context).\\n\\n## Setup and Usage  \\n### CIROH JupyterHub  \\n- Fork repo → access 2i2c Hub → select workshop environment → clone & run notebooks.  \\n### Local Installation  \\n```bash\\nconda env create -f environment.yml\\nconda activate cnn_env\\npython -m ipykernel install --user --name=cnn_env\\n```  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydro-meteorology / Machine Learning / Climate Modeling.  \\n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology).  \\n- **Purpose:** Provide training and hands-on experience in CNN-based precipitation downscaling for hydrologic and climate applications.\\n\\n## Input / Output  \\n**Input:** ERA5 reanalysis data and orographic precipitation datasets for northern Utah.  \\n**Output:** CNN-predicted OPG fields and downscaled precipitation products.\\n\\n## Data Sources  \\n- Bohne et al. (2020) OPG Climatology (Western U.S.).  \\n- ECMWF ERA5 reanalysis (1988–2017).",
        "summary": "OPG-CNN-Northern-Utah-CIROH-Workshop provides training materials for applying convolutional neural networks to orographic precipitation downscaling using ERA5 data. Participants learn model design, explainable AI methods, and parameter tuning for hydro-meteorological prediction. The workshop supports both CIROH JupyterHub and local setups, fostering education in machine learning applications to climate and precipitation modeling.",
        "keywords": ["CNN", "machine learning", "hydrology", "precipitation", "downscaling", "ERA5", "climate modeling", "JupyterHub", "OPG"],
        "entities": ["CIROH", "ECMWF", "ERA5", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 36,
        "title": "PINN_workshop_ciroh",
        "source": "https://github.com/CIROH-UA/PINN_workshop_ciroh",
        "section_header": "# PINN_workshop_ciroh"
        },
        "content": "# Project Title: **PINN_workshop_ciroh**  \\n\\n## Project Objective  \\nThe **PINN_workshop_ciroh** repository provides official materials for the Physics-Informed Neural Networks (PINN) Workshop held at the CIROH Development Conference 2024. It demonstrates the application of PINNs in solving conceptual problems and Partial Differential Equations (PDEs) related to hydrology.\\n\\n## Core Functionalities  \\n- Introduces Physics-Informed Neural Networks (PINNs) and their integration of physical laws into neural network models.  \\n- Provides step-by-step tutorials for implementing and training PINNs on hydrological models.  \\n- Includes example projects demonstrating PINNs for complex hydrological problems such as groundwater and surface water flow.  \\n- Supplies utility scripts for constructing and training PINNs on various PDEs in hydrology.  \\n- Features two hands-on Jupyter notebooks:  \\n  1. **heat_equation_pinn** – Solves a one-dimensional transient heat equation.  \\n  2. **2d_shallow_water_inverse** – Solves a two-dimensional transient shallow water equation as an inverse problem.  \\n\\n## Technical Stack  \\n- **Languages:** Python.  \\n- **Frameworks/Libraries:** Not specified.  \\n- **Dependencies:** Defined in the `environment.yml` file for Conda setup.  \\n\\n## Setup and Usage  \\n1. Install Conda if not already available.  \\n2. Clone the repository:  \\n   ```bash\\n   git clone https://github.com/AMBehroozi/PINN_workshop_ciroh.git\\n   cd PINN_workshop_ciroh\\n   ```  \\n3. Create and activate the Conda environment:  \\n   ```bash\\n   conda env create -f environment.yml\\n   conda activate pinn_workshop_env\\n   ```  \\n4. Alternatively, use the provided `setup.sh` script for automated setup.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Physics-Informed Modeling.  \\n- **Affiliation:** Cooperative Institute for Research to Operations in Hydrology (CIROH).  \\n- **Purpose:** Provide educational materials and practical examples for using PINNs in hydrological research and modeling.  \\n\\n## Input / Output  \\n**Input:** Jupyter notebooks and configuration files for training PINN models on hydrological PDEs.  \\n**Output:** Trained PINN models and results visualizing solutions to heat and shallow water equations.",
        "summary": "PINN_workshop_ciroh provides official CIROH Development Conference 2024 materials for learning and applying Physics-Informed Neural Networks (PINNs) to hydrologic problems. Through hands-on Jupyter notebooks and tutorials, it teaches how to integrate physical laws with deep learning for solving PDEs such as heat and shallow-water equations. The repository promotes educational exploration of physics-informed modeling for water and environmental systems.",
        "keywords": ["PINN", "Physics-Informed Neural Networks", "hydrology", "machine learning", "PDEs", "education", "Jupyter", "deep learning"],
        "entities": ["CIROH", "NextGen", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 37,
        "title": "SWEML",
        "source": "https://github.com/CIROH-UA/SWEML",
        "section_header": "# SWEML"
        },
        "content": "# Project Title: **SWEML**  \\n\\n## Project Objective  \\nThe **Snow Water Equivalent Machine Learning (SWEML)** project uses machine learning to advance snow state modeling. It produces high-resolution (1 km) SWE predictions across the Western U.S. and the Upper Colorado River Basin, using data from SNOTEL and CDEC sites combined with lidar-derived terrain features. The system aims to enhance snowpack monitoring and forecasting capabilities through an operational deep learning workflow.\\n\\n## Core Functionalities  \\n- Predicts 1 km x 1 km resolution SWE (Snow Water Equivalent) inferences.  \\n- Integrates nearly 700 snow telemetry (SNOTEL) and California Data Exchange Center (CDEC) stations.  \\n- Combines SWE observations with lidar-derived terrain features and seasonality metrics.  \\n- Trains region-specific multilayered perceptron (MLP) models for SWE prediction.  \\n- Provides hindcast simulations and visualizations of SWE for key snow regions.  \\n- Evaluates model performance using the Standardized Snow Water Equivalent Evaluation Tool (SSWEET).  \\n- Includes an interactive visualization of SWE estimates across the Western U.S.  \\n- Offers a tutorial and full workflow in a Jupyter Book format.  \\n\\n## Technical Stack  \\n- **Language:** Python (version 3.8 or later).  \\n- **Frameworks/Libraries:** TensorFlow, LightGBM, Pandas, NumPy, GeoPandas, Rasterio, Shapely, Xarray, Matplotlib, Cartopy, EarthPy, TQDM, Folium, RichDEM, PyProj, Requests.  \\n- **Visualization:** Matplotlib, Folium, and Jupyter Notebook.  \\n- **Tools:** GeoWeaver for workflow management, Binder for interactive exploration.  \\n\\n## Setup and Usage  \\n- The workflow retrieves SWE observations from SNOTEL and CDEC for the selected date.  \\n- Data are preprocessed into a model-ready dataframe including terrain, seasonal, and historical SWE features.  \\n- Model training uses 75% of NASA ASO and snow course data (2013–2018) with 25% held for testing.  \\n- Evaluation is performed on the 2019 water year hindcast simulation.  \\n- The Jupyter Book provides step-by-step tutorials for data preparation, model training, evaluation, and workflow management.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Snow Modeling / Machine Learning.  \\n- **Affiliation:** CIROH (Cooperative Institute for Research to Operations in Hydrology), The University of Alabama, NASA, NOAA, and USGS.  \\n- **Purpose:** Advance snow state modeling and forecasting using deep learning techniques for operational hydrology applications.  \\n\\n## Input / Output  \\n**Input:**  \\n- SWE observations from SNOTEL and CDEC stations.  \\n- Lidar-derived terrain features, latitude, longitude, elevation, and seasonality metrics.  \\n- Metadata and grid definitions in GeoJSON and CSV formats.  \\n\\n**Output:**  \\n- High-resolution (1 km) SWE predictions.  \\n- Hindcast SWE simulations and interactive visualizations.  \\n- Evaluation metrics and performance plots for SWE prediction accuracy.",
        "summary": "SWEML applies machine learning to snow water equivalent modeling, combining ground-based SNOTEL and CDEC data with lidar-derived terrain features to generate 1 km SWE predictions for the western United States. Using deep learning models like MLPs, it provides hindcasts, visualizations, and evaluation metrics to improve snowpack monitoring and forecasting under CIROH and NOAA initiatives.",
        "keywords": ["SWE", "machine learning", "snow modeling", "deep learning", "hydrology", "NOAA", "CIROH", "NASA", "USGS"],
        "entities": ["CIROH", "NOAA", "NASA", "USGS", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 38,
        "title": "Tethys-CSES",
        "source": "https://github.com/CIROH-UA/Tethys-CSES",
        "section_header": "# Tethys-CSES"
        },
        "content": "# Project Title: **Tethys-CSES**  \\n\\n## Project Objective  \\nThe **Community Streamflow Evaluation System (CSES)** is a Tethys web application developed to evaluate hydrological model performance using a standardized NHDPlus data model. It allows users to assess modeled streamflow at over 5,000 in-situ USGS monitoring sites, offering interactive visualizations for comprehensive hydrologic analysis.\\n\\n## Core Functionalities  \\n- Provides evaluation of modeled streamflow against USGS observations.  \\n- Supports multiple evaluation methods:  \\n  - **State Evaluation Methods:** Evaluate hydrological model performance by state.  \\n  - **Reach Evaluation Methods:** Analyze stream reaches using NWIS site data.  \\n  - **HUCid Evaluation Methods:** Assess hydrologic unit codes (HUCs) within selected basins.  \\n- Uses retrospective datasets from the National Water Model (NWM v2.1, with v3.0 planned).  \\n- Enables interactivity for exploring models, states, date ranges, and locations.  \\n- Hosts all colocated USGS and NWM data in AWS S3 for quick access and reproducible analysis.  \\n\\n## Technical Stack  \\n- **Framework:** Tethys Platform (Web-based).  \\n- **Languages:** Python.  \\n- **Data Sources:** USGS/NWIS, NWM v2.1 retrospective dataset.  \\n- **Hosting:** AWS S3 Cloud Storage.  \\n\\n## Setup and Usage  \\n- Default configuration evaluates streamflow for Alabama and Great Salt Lake regions using NWM v2.1 data.  \\n- Users can interactively select:  \\n  - States, reaches, or HUCs.  \\n  - Model type and date range (recommended ≤ 1 year).  \\n- If plots do not display, the selected model or observational data may be unavailable.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Water Modeling.  \\n- **Affiliation:** Alabama Water Institute (AWI), Cooperative Institute for Research to Operations in Hydrology (CIROH), ESIP, NASA, NOAA, and USGS.  \\n- **Purpose:** Facilitate large-scale hydrologic model evaluation and visualization through a community-accessible web platform.  \\n\\n## Input / Output  \\n**Input:**  \\n- User-selected parameters such as state, reach, HUC, model type, and date range.  \\n\\n**Output:**  \\n- Interactive plots and visualizations comparing modeled and observed streamflow data.  \\n\\n## Funding Acknowledgement  \\nFunding provided through the **NOAA Cooperative Agreement (NA22NWS4320003)** with The University of Alabama, under CIROH, and supported by **NASA, NOAA, and the USGS** via the **ESIP Lab**.",
        "summary": "Tethys-CSES is a CIROH-developed Tethys web app for evaluating hydrologic model performance using NWM and USGS data. It enables interactive streamflow analysis across thousands of monitoring sites with state, reach, and HUC-level evaluation modes. Hosted on AWS S3, it advances reproducible hydrologic model validation through open, community-accessible visualization tools.",
        "keywords": ["Tethys", "CSES", "hydrology", "model evaluation", "streamflow", "USGS", "NWM", "AWS", "visualization"],
        "entities": ["CIROH", "NOAA", "NASA", "USGS", "ESIP", "Alabama Water Institute", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 39,
        "title": "tethysportal-ciroh",
        "source": "https://github.com/CIROH-UA/tethysportal-ciroh",
        "section_header": "# tethysportal-ciroh"
        },
        "content": "# Project Title: **tethysportal-ciroh**  \\n\\n## Project Objective  \\nThe **CIROH Tethys Portal** is a web-based platform deployed on an Amazon EKS cluster. It integrates multiple hydrologic and water data applications under a unified interface and provides access to visualization, computation, and data tools. The infrastructure uses Kubernetes and Terraform for deployment and management.\\n\\n## Core Functionalities  \\n- Runs on Amazon EKS (`namespace: cirohportal`) with traffic routing via AWS Application Load Balancer.  \\n- Hosts multiple applications within a Django/Tethys web stack, including THREDDS, PostgreSQL, and Redis services.  \\n- Supports data access and visualization through built-in and proxied apps.  \\n- Provides infrastructure-as-code deployment using Terraform modules.  \\n- Allows Helm-based upgrades for updating Docker images.  \\n- Includes troubleshooting guidelines for AWS infrastructure and Kubernetes clusters.  \\n- Offers monitoring and visualization tools for cluster management.  \\n\\n## Technical Stack  \\n- **Languages/Frameworks:** Django, Tethys Platform.  \\n- **Infrastructure:** Amazon EKS, AWS Application Load Balancer, Terraform, Helm, Kubernetes.  \\n- **Databases/Services:** PostgreSQL, Redis, THREDDS Data Server.  \\n- **Monitoring Tools:** k9s, eks-node-viewer.  \\n- **Submodules:** Terraform modules for reproducible infrastructure setup.  \\n\\n## Setup and Usage  \\n1. **Prerequisites:**  \\n   - AWS CLI and configured credentials.  \\n   - Terraform installed.  \\n   - Access to AWS EKS cluster.  \\n\\n2. **Terraform Deployment:**  \\n   - Configure kubectl for the target cluster.  \\n   - Run the Terraform workflow:  \\n     ```bash\\n     terraform init\\n     terraform plan\\n     terraform apply\\n     ```  \\n   - Use `terraform destroy` to clean up development environments.  \\n\\n3. **Helm Upgrade Example:**  \\n   ```bash\\n   helm repo add tethysportal-ciroh https://docs.ciroh.org/tethysportal-ciroh\\n   helm upgrade cirohportal-prod tethysportal-ciroh/ciroh      --install --wait --timeout 3600      -f charts/ciroh/ci/prod_aws_values.yaml      --set storageClass.parameters.fileSystemId=<EFS_ID>      --set image.tag=<newTag>      --namespace cirohportal\\n   ```  \\n\\n4. **Troubleshooting:**  \\n   - Address ALB and VPC deletion issues manually if Terraform fails.  \\n   - Use Kubernetes and Terraform documentation for resolving cluster access errors.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Cloud Infrastructure / Web Portals.  \\n- **Affiliation:** CIROH.  \\n- **Purpose:** Provide a scalable and modular web-based infrastructure for hosting CIROH applications using Tethys Platform and AWS.  \\n\\n## Input / Output  \\n**Input:**  \\n- Terraform configuration variables (region, cluster name, Helm chart, etc.).  \\n- Helm values and Docker image tags for deployment.  \\n\\n**Output:**  \\n- Deployed Tethys Portal web application with integrated data visualization and analysis tools.",
        "summary": "tethysportal-ciroh deploys CIROH’s central web platform on AWS EKS, combining Terraform, Helm, and Kubernetes for scalable infrastructure management. It hosts Django/Tethys-based hydrologic applications, enabling data visualization and computation under a unified portal with CI/CD, monitoring, and troubleshooting support.",
        "keywords": ["Tethys", "AWS", "EKS", "Terraform", "Helm", "Kubernetes", "CIROH", "cloud infrastructure", "web portal", "hydrology"],
        "entities": ["CIROH", "AWS", "Amazon EKS", "Terraform", "Kubernetes", "THREDDS", "Tethys Platform"]
    },
    {
        "metadata": {
        "idurl": 40,
        "title": "training-HydroShare-101",
        "source": "https://github.com/CIROH-UA/training-HydroShare-101",
        "section_header": "# training-HydroShare-101"
        },
        "content": "# Project Title: **training-HydroShare-101**  \\n\\n## Project Objective  \\nThis repository provides a **template lesson** built with [The Carpentries Workbench](https://carpentries.github.io/sandpaper-docs/). It is designed to help educators, trainers, and researchers create and publish structured training materials or workshops—such as the **HydroShare 101** lesson—using the Carpentries framework.  \\nThe project’s goal is to support reproducible, version-controlled, and collaboratively maintained educational content for technical and scientific training.\\n\\n## Core Functionalities  \\n- **Template-based lesson creation:** Provides a complete framework for building new lesson repositories from a Carpentries template.  \\n- **GitHub Pages integration:** Automatically builds and hosts lesson websites through GitHub Pages.  \\n- **Configuration guidance:** Step-by-step setup for the `config.yaml` file, which controls lesson metadata and lifecycle stage.  \\n- **Lesson lifecycle management:** Includes configurable life cycle stages such as “pre-alpha” to indicate lesson maturity.  \\n- **Citation and metadata setup:** Offers instructions for updating citation, license, contribution, and conduct documentation (`CITATION.cff`, `LICENSE.md`, `CONTRIBUTING.md`, etc.).  \\n- **Contributor onboarding:** Provides guidelines for community collaboration, tagging, and repository annotation (topics, lesson language, etc.).  \\n- **Automated build and workflow setup:** Uses GitHub Actions to automate lesson deployment.  \\n\\n## Technical Stack  \\n- **Framework:** [The Carpentries Workbench](https://github.com/carpentries/workbench-template-md)  \\n- **Language:** Markdown (with YAML configuration)  \\n- **Hosting:** GitHub Pages  \\n- **Automation:** GitHub Actions  \\n- **License:** CC-BY (Creative Commons Attribution)  \\n\\n## Setup and Usage  \\n1. **Create a new repository:**  \\n   Use the template at [carpentries/workbench-template-md](https://github.com/carpentries/workbench-template-md/generate) and include all branches.  \\n2. **Activate GitHub Pages:**  \\n   Enable the `gh-pages` branch under repository settings to host the lesson site.  \\n3. **Edit `config.yaml`:**  \\n   Update fields such as `title`, `created`, `keywords`, `life_cycle`, and `contact`.  \\n4. **Update documentation files:**  \\n   Modify `CITATION.cff`, `CODE_OF_CONDUCT.md`, `CONTRIBUTING.md`, and `LICENSE.md` as needed.  \\n5. **Annotate the repository:**  \\n   Add lesson-related tags (e.g., `lesson`, `pre-alpha`, `english`) under repository topics.  \\n6. **Customize the README:**  \\n   Replace the template’s placeholder instructions with lesson-specific content.  \\n\\n## Project Context & Domain  \\n- **Domain:** Educational content / Training framework.  \\n- **Affiliation:** Based on **The Carpentries** teaching framework, used widely across data science and research computing communities.  \\n- **Intended Use:** Serves as the foundation for HydroShare or hydrology-related training materials developed within the Carpentries ecosystem.  \\n\\n## Input / Output  \\n**Input:**  \\n- Markdown lesson content, configuration files (`config.yaml`, metadata docs), and associated assets.  \\n\\n**Output:**  \\n- A fully rendered static website (via GitHub Pages) hosting the educational lesson.",
        "summary": "training-HydroShare-101 provides a Carpentries Workbench-based framework for creating reproducible, version-controlled training materials such as HydroShare 101. It automates website builds via GitHub Actions, supports metadata configuration, and enables collaborative lesson development under open educational licensing.",
        "keywords": ["education", "training", "Carpentries", "HydroShare", "GitHub Pages", "lesson development", "Markdown", "CIROH"],
        "entities": ["The Carpentries", "CIROH", "GitHub"]
    },
    {
        "metadata": {
        "idurl": 41,
        "title": "training-NGIAB-101",
        "source": "https://github.com/CIROH-UA/training-NGIAB-101",
        "section_header": "# training-NGIAB-101"
        },
        "content": "# Project Title: **training-NGIAB-101**  \\n\\n## Project Objective  \\nThe **NGIAB 101 Training Module** provides an introductory, hands-on learning experience with **NextGen In A Box (NGIAB)**, a modular, open-source hydrologic modeling framework. Developed by the **Cooperative Institute for Research to Operations in Hydrology (CIROH)**, the lesson is built using [The Carpentries Workbench](https://carpentries.github.io/sandpaper-docs/).  \\nIt guides users through installation, data preparation, model execution, evaluation, and visualization, with optional advanced topics for high-performance computing (HPC) environments.\\n\\n## Core Functionalities  \\n- Introduces users to the **NextGen In A Box (NGIAB)** modeling framework.  \\n- Covers the full workflow: installation, data preparation, model execution, evaluation, and visualization.  \\n- Includes **optional HPC-focused content** for advanced learners.  \\n- Built using **The Carpentries Workbench**, which provides a modular and reproducible lesson framework.  \\n- Cites foundational research on NGIAB for academic context.  \\n\\n## Technical Stack  \\n- **Framework:** [The Carpentries Workbench](https://carpentries.github.io/sandpaper-docs/)  \\n- **Language:** Markdown (used for Carpentries lessons)  \\n- **Primary Tools/Technologies:** Not specified.  \\n- **Hosting:** Not specified.  \\n- **License:** Not specified.  \\n\\n## Setup and Usage  \\nInstructions for lesson setup, installation, or deployment are **not specified** in the README.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Training and education.  \\n- **Affiliation:** Cooperative Institute for Research to Operations in Hydrology (CIROH).  \\n- **Supporting Reference:**  \\n  Patel, A., Halgren, J., Wills, Z., Frazier, N., Lee, B., Cunningham, J., Laser, J., Karimiziarani, M., Patel, T., Romero, G., Denno, M., Lamont, S., Maghami, I., Jajula, H. T., Alam, M. S., Koriche, S. A., Singh, M., Naser Neisary, S., Lee, Q., … Ames, D. P. (2025). *NextGen In A Box (NGIAB): Advancing Community Modeling with the U.S. National Water Model.* Tuscaloosa.  \\n\\n## Input / Output  \\n**Input:** Not specified.  \\n**Output:** Not specified.",
        "summary": "training-NGIAB-101 provides a beginner-friendly Carpentries-based module introducing users to NextGen In A Box (NGIAB). It covers model setup, data preparation, execution, evaluation, and visualization with optional HPC lessons. Developed under CIROH, it supports structured, reproducible training for hydrologic modeling within the NextGen ecosystem.",
        "keywords": ["NextGen", "NGIAB", "training", "education", "Carpentries", "hydrology", "HPC", "modeling"],
        "entities": ["CIROH", "The Carpentries", "NextGen In A Box", "NOAA", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 42,
        "title": "t-route",
        "source": "https://github.com/CIROH-UA/t-route",
        "section_header": "# t-route"
        },
        "content": "# Project Title: **t-route**  \\n\\n## Project Objective  \\n**T-Route (Tree-Based Channel Routing)** is a dynamic, modular routing model designed to handle one-dimensional (1-D) channel routing in vector-based river network data. It provides a fast and flexible solution for routing streamflows through river networks such as those in the **National Water Model (NWM)** and the **NextGen framework**.  \\nThe tool enables heterogeneous application of routing models—such as Muskingum-Cunge for headwater streams and Diffusive Wave for high-order rivers—allowing efficient use of computational resources and scalability across hydrologic systems.\\n\\n## Core Functionalities  \\n- **Multi-model routing:** Supports Muskingum-Cunge, Diffusive Wave, and Dynamic Wave routing methods.  \\n- **Compatibility:** Works with **NHDPlus High Resolution datasets** and **HY_Features** (OGC WaterML 2.0 Surface Hydrology Features model).  \\n- **Flexible architecture:** Combines hydrologic and hydraulic routing within the same network.  \\n- **Modular design:** Independent modules for reservoirs, data assimilation, and flow routing.  \\n- **Heterogeneous routing:** Enables mixed routing model configurations within a single domain.  \\n- **Python + Fortran integration:** Core routing algorithms implemented in Fortran, orchestrated by Python.  \\n- **Parallelization:** Supports large-scale routing through acyclic network traversal.  \\n- **Demonstration data:** Includes example runs such as the Lower Colorado River, TX test case.  \\n\\n## Technical Stack  \\n- **Languages:** Python and Fortran  \\n- **Core Components:**  \\n  - Python: Pre-processing, traversal framework, time-series data handling  \\n  - Fortran: Routing model engines (Muskingum-Cunge, Diffusive Wave)  \\n- **Dependencies:** NetCDF-Fortran libraries, Python 3.10, standard build tools (CMake, gfortran, pip).  \\n- **Platform Support:** macOS, Linux, and Windows (via WSL).  \\n- **Documentation:** Installation guides for macOS and Linux/WSL are included in the repository.  \\n\\n## Setup and Usage  \\n### Installation Overview  \\n1. **System preparation:** Install Python 3.10, Fortran compilers, and NetCDF libraries.  \\n2. **Clone and set up environment:**  \\n   ```bash\\n   git clone https://github.com/CIROH-UA/t-route.git\\n   cd t-route\\n   python3.10 -m venv troute_env\\n   source troute_env/bin/activate\\n   pip install -r requirements.txt\\n   ```  \\n3. **Build NetCDF Fortran libraries:** Follow UCAR installation steps and link them to the compiler script.  \\n4. **Compile T-Route:**  \\n   ```bash\\n   ./compiler.sh\\n   ```  \\n5. **Run example model:**  \\n   ```bash\\n   cd test/LowerColorado_TX\\n   python3 -m nwm_routing -f -V4 test_AnA_V4_NHD.yaml\\n   ```  \\n6. **Troubleshooting:** Permission and compilation issues can be resolved using `chmod` or checking library paths.  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Channel routing / Water resource modeling.  \\n- **Affiliation:** Developed under the **NOAA Office of Water Prediction (OWP)**.  \\n- **Purpose:** Provides high-performance, flexible routing capabilities for hydrologic models such as the **National Water Model (NWM) 3.0** and the **NextGen framework**.  \\n- **Mission Alignment:** Supports NOAA’s goal of providing state-of-the-science hydrologic forecasting and decision-support tools for emergency management and water resource planning.  \\n\\n## Input / Output  \\n**Input:**  \\n- Lateral inflows for each node in a river network.  \\n- Network topology data (e.g., NHDPlus HR or HY_Features).  \\n- Configuration files defining routing parameters and model selection.  \\n\\n**Output:**  \\n- Simulated streamflow time series at network nodes.  \\n- Optional data assimilation outputs and routing diagnostics.  \\n\\n## Known Issues  \\nRefer to the repository’s GitHub Issues page for ongoing improvements and bug tracking.  \\n\\n## Getting Help  \\nFor technical support, contact the T-Route maintainers:  \\n- dongha.kim@noaa.gov  \\n- sean.horvath@noaa.gov  \\n- amin.torabi@noaa.gov  \\n- jurgen.zach@noaa.gov  \\n\\n## Getting Involved  \\nContributors are encouraged to fork the repository and submit pull requests. Ongoing work includes improving parallel processing performance and expanding I/O capabilities. Contribution guidelines will be added to `CONTRIBUTING.md`.  \\n\\n## License  \\nOpen-source under the terms specified in:  \\n1. [TERMS](TERMS.md)  \\n2. [LICENSE](LICENSE)  \\n\\n## Credits and References  \\nAcknowledgments to Drs. Ehab Meslehe, Fred Ogden, and the NCAR WRF-Hydro development team for foundational contributions. Continued leadership and support from Dr. Trey Flowers.",
        "summary": "t-route (Tree-Based Channel Routing) is NOAA’s modular routing model for simulating 1D streamflow across vector-based river networks. Integrating Python and Fortran, it supports Muskingum-Cunge, Diffusive Wave, and Dynamic Wave methods and runs on NHDPlus HR and HY_Features data. Designed for scalability and flexibility, T-Route underpins routing in the National Water Model and NextGen frameworks.",
        "keywords": ["T-Route", "National Water Model", "NextGen", "routing", "hydrology", "Fortran", "Python", "NOAA", "channel flow", "NHDPlus"],
        "entities": ["NOAA", "OWP", "CIROH", "NHDPlus", "HY_Features", "National Water Model"]
    },
    {
        "metadata": {
        "idurl": 43,
        "title": "varnish",
        "source": "https://github.com/CIROH-UA/varnish",
        "section_header": "# varnish"
        },
        "content": "# Project Title: **varnish**  \\n\\n## Project Objective  \\nThe **varnish** package serves as a template for The Carpentries Workbench. It provides HTML, CSS, and JavaScript templates for Carpentries lessons, adapted from the `{pkgdown}` package, and is automatically installed and used by `{sandpaper}` to render lesson websites.\\n\\n## Core Functionalities  \\n- Supplies HTML templates for Carpentries lessons.  \\n- Uses the mustache templating language for rendering.  \\n- Compiles and minifies CSS and JavaScript via GitHub Actions.  \\n- Provides templates for lesson structure components, including:  \\n  - content-chapter  \\n  - content-syllabus  \\n  - content-extra  \\n  - content-overview  \\n  - head, navbar, header, footer, and layout.  \\n- Supports YAML-based global and page-specific parameters.  \\n- Compatible with `{pkgdown}` and `{sandpaper}` for automated site rendering.  \\n\\n## Technical Stack  \\n- **Languages/Frameworks:** R, mustache, HTML, CSS, JavaScript.  \\n- **Tools:** SASS, uglifyjs, Node.js (version 16), npm, GitHub Actions.  \\n- **Dependencies:** Node, npm, nvm, pkgdown, sandpaper.  \\n\\n## Setup and Usage  \\n### Installation  \\nInstall the package via The Carpentries R-universe repository:  \\n```r\\ninstall.packages(\"varnish\", repos = \"https://carpentries.r-universe.dev\")\\n```  \\nIt is automatically detected and used by `{sandpaper}` to apply styles and templates to lesson websites.\\n\\n### Building CSS and JavaScript Locally  \\n1. Install Node.js (via `nvm`):  \\n   ```sh\\n   nvm install 16\\n   npm install\\n   ```  \\n2. Minify assets:  \\n   ```sh\\n   bash squash-sass.sh     # compile CSS using sass\\n   bash squash-a-script.sh # compile JS using uglifyjs\\n   ```  \\n\\n### Template Parameters  \\n- Defined in `site/_pkgdown.yaml` or provided via `{pkgdown}`.  \\n- Includes parameters such as `title`, `time`, `source`, `branch`, `contact`, `license`, and lesson metadata.  \\n- Page-specific parameters include:  \\n  - `{{ instructor }}` – instructor view toggle.  \\n  - `{{ aio }}` – include all-in-one page.  \\n  - `{{ this_page }}` – current HTML file name.  \\n  - `{{{ schedule }}}`, `{{{ resources }}}` – sidebar content.  \\n\\n## Project Context & Domain  \\n- **Domain:** Educational Web Development / R Package Templates.  \\n- **Affiliation:** The Carpentries.  \\n- **Purpose:** Provide standardized lesson templates and styling for The Carpentries Workbench.  \\n\\n## Input / Output  \\n**Input:**  \\n- Source HTML templates, YAML configuration, and Node-based CSS/JS files.  \\n\\n**Output:**  \\n- Compiled and minified CSS and JS files.  \\n- Rendered lesson websites styled using the Varnish template.",
        "summary": "varnish is an R package that provides HTML, CSS, and JavaScript templates for The Carpentries Workbench, automating lesson site rendering with {pkgdown} and {sandpaper}. It compiles web assets via GitHub Actions and ensures standardized, reproducible styling across Carpentries educational materials.",
        "keywords": ["Carpentries", "R", "pkgdown", "sandpaper", "education", "template", "HTML", "CSS", "JavaScript", "web development"],
        "entities": ["The Carpentries", "pkgdown", "sandpaper", "R-universe"]
    },
    {
        "metadata": {
        "idurl": 44,
        "title": "docuhub-staging",
        "source": "https://github.com/CIROH-UA/docuhub-staging",
        "section_header": "# docuhub-staging"
        },
        "content": "# Project Title: **docuhub-staging**  \\n\\n## Project Objective  \\n**docuhub-staging** is a static documentation portal built on **Docusaurus v3.9.1**, serving as the centralized knowledge base for the **Cooperative Institute for Research to Operations in Hydrology (CIROH)**. It hosts technical documentation, organizational policies, data governance frameworks, and CIROH project resources in a structured and accessible format. This repository represents the **staging environment** for DocuHub, supporting continuous content updates and validation before production deployment.\\n\\n## Core Functionalities  \\n- Implements a **three-tier architecture** with a static frontend, content management layer, and external integrations.  \\n- Built on **Docusaurus** with modular React components for navigation, search, and theming.  \\n- Organizes content across key documentation categories including products, services, policies, blog, and news.  \\n- Integrates **Google Analytics (G-7KD31X6H62)** for metrics and **jsDelivr CDN** for efficient asset delivery.  \\n- Features metadata-driven SEO optimization, version control, and error handling.  \\n\\n## Technical Stack  \\n- **Languages/Frameworks:** Docusaurus v3.9.1 (React-based).  \\n- **Dependencies:** `@docusaurus/core`, `@docusaurus/theme-classic`, `docusaurus-plugin-drawio`, Webpack, Infima CSS.  \\n- **Hosting:** Deployed at `http://ciroh.org/docuhub-staging/`.\\n\\n## Setup and Usage  \\n1. Clone the repository and install dependencies with `npm install`.  \\n2. Build the site using `npm run build`.  \\n3. Preview locally with `npm run start` or deploy to staging.  \\n\\n## Project Context & Domain  \\n- **Domain:** Documentation / Knowledge Management / Hydrology Research.  \\n- **Affiliation:** CIROH, The University of Alabama.  \\n- **Purpose:** Provide a unified knowledge portal for CIROH’s researchers, partners, and stakeholders.\\n\\n## Input / Output  \\n**Input:** Markdown files, PDF documents, and configuration metadata.  \\n**Output:** Static site with searchable documentation and CIROH resources.",
        "summary": "docuhub-staging hosts the staging environment of CIROH’s DocuHub knowledge platform, built with Docusaurus v3.9.1. It centralizes hydrology research documentation, policies, and project resources, providing a structured staging system for updates and validation before production deployment.",
        "keywords": ["Docusaurus", "React", "CIROH", "documentation", "knowledge management", "static site", "staging", "AWS", "SEO"],
        "entities": ["CIROH", "The University of Alabama", "Docusaurus", "NOAA"]
    },
    {
        "metadata": {
        "idurl": 45,
        "title": "hydromachine-tutorials",
        "source": "https://github.com/CIROH-UA/hydromachine-tutorials",
        "section_header": "# hydromachine-tutorials"
        },
        "content": "# Project Title: **hydromachine-tutorials**  \\n\\n## Project Objective  \\n**hydromachine-tutorials** teaches the application of machine learning to hydrological modeling, comparing **LSTM neural networks** on AWS SageMaker with **XGBoost** for local experimentation. The tutorials cover the entire ML workflow—from data preprocessing and training to evaluation and deployment.\\n\\n## Core Functionalities  \\n- Two learning tracks: **LSTM (cloud-based)** and **XGBoost (local)**.  \\n- Demonstrates ML pipelines, model deployment, and bias correction for National Water Model streamflow outputs.  \\n- Shared AWS S3 dataset with 18 hydrometeorological features from multiple USGS sites.  \\n- Provides Jupyter notebooks and Conda environments for reproducibility.  \\n- Covers hyperparameter optimization using Optuna and interpretability with SHAP.\\n\\n## Technical Stack  \\n- **Languages:** Python  \\n- **Libraries:** PyTorch, XGBoost, scikit-learn, Optuna, SHAP, pandas, GeoPandas.  \\n- **Platforms:** AWS SageMaker, local Conda environments.  \\n- **Storage:** AWS S3 (Parquet).  \\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning / Streamflow Modeling.  \\n- **Affiliation:** CIROH, The University of Alabama.  \\n- **Purpose:** Enable scalable, educational ML workflows for improving NWM streamflow predictions.\\n\\n## Input / Output  \\n**Input:** Streamflow data and hydrometeorological features.  \\n**Output:** Trained models, performance plots, and deployment-ready artifacts.",
        "summary": "hydromachine-tutorials is a CIROH educational repository teaching ML applications for hydrologic modeling. It compares LSTM and XGBoost methods for bias correction of NWM streamflow predictions using AWS SageMaker and local workflows, providing reproducible tutorials for learners and researchers.",
        "keywords": ["machine learning", "LSTM", "XGBoost", "AWS SageMaker", "hydrology", "streamflow", "Optuna", "SHAP", "tutorials"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "AWS"]
    },
    {
        "metadata": {
        "idurl": 46,
        "title": "LSTM-Tutorials",
        "source": "https://github.com/CIROH-UA/LSTM-Tutorials",
        "section_header": "# LSTM-Tutorials"
        },
        "content": "# Project Title: **LSTM-Tutorials**  \\n\\n## Project Objective  \\n**LSTM-Tutorials** provides an interactive educational framework for applying LSTM deep learning to hydrologic time series forecasting. It guides users from univariate streamflow prediction to multivariate post-processing of National Water Model outputs.\\n\\n## Core Functionalities  \\n- Teaches LSTM model design, training, and evaluation in PyTorch.  \\n- Demonstrates data integration from AWS S3 and preprocessing pipelines.  \\n- Includes metrics such as NSE, KGE, RMSE, and MAPE for hydrological evaluation.  \\n- Supports GPU acceleration with CUDA and modular Python scripts for reproducibility.  \\n- Provides multiple notebooks scaling from basic to production-ready workflows.\\n\\n## Technical Stack  \\n- **Language:** Python (PyTorch, pandas, NumPy, scikit-learn).  \\n- **Domain Libraries:** hydroeval, hydrotools-nwis-client, hydrotools-nwm-client.  \\n- **Cloud Integration:** AWS S3 via boto3.  \\n- **Visualization:** Matplotlib, Seaborn.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Machine Learning.  \\n- **Affiliation:** CIROH, The University of Alabama.  \\n- **Purpose:** Provide structured training materials for hydrologists learning deep learning workflows.\\n\\n## Input / Output  \\n**Input:** Streamflow data and hydrometeorological features.  \\n**Output:** Trained LSTM models, metrics, and visualizations.",
        "summary": "LSTM-Tutorials offers CIROH’s structured learning framework for hydrologists to apply LSTM deep learning to streamflow prediction. Using PyTorch and AWS data integration, it covers model construction, evaluation, and GPU-accelerated training for production-level hydrologic forecasting.",
        "keywords": ["LSTM", "PyTorch", "hydrology", "streamflow", "machine learning", "AWS S3", "CUDA", "deep learning"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "AWS"]
    },
    {
        "metadata": {
        "idurl": 47,
        "title": "nrds-onprem",
        "source": "https://github.com/CIROH-UA/nrds-onprem",
        "section_header": "# nrds-onprem"
        },
        "content": "# Project Title: **nrds-onprem**  \\n\\n## Project Objective  \\n**nrds-onprem** is the Alabama Water Institute’s open-source project template, providing a standardized structure for software repositories across AWI and CIROH. It defines consistent documentation, contribution workflows, and security practices to ensure high-quality, maintainable codebases.\\n\\n## Core Functionalities  \\n- Provides a repository skeleton with standardized documentation and version control files.  \\n- Includes GitHub workflow templates for issues and pull requests.  \\n- Establishes coding and documentation best practices via CONTRIBUTING and SECURITY templates.  \\n- Licensed under MIT for open collaboration.  \\n- Enforces repository hygiene and merge conflict prevention.\\n\\n## Technical Stack  \\n- **Language:** Not language-specific.  \\n- **Tools:** GitHub Actions, Markdown, Git.  \\n- **Runtime:** GitHub template repository.\\n\\n## Project Context & Domain  \\n- **Domain:** Software Engineering / Repository Management.  \\n- **Affiliation:** Alabama Water Institute (AWI), CIROH.  \\n- **Purpose:** Ensure consistency, compliance, and maintainability across CIROH and AWI projects.\\n\\n## Input / Output  \\n**Input:** None directly (template files).  \\n**Output:** Structured repository ready for development.",
        "summary": "nrds-onprem defines the Alabama Water Institute’s standard GitHub template for open-source projects. It provides preconfigured documentation, workflow, and security templates to enforce software best practices across CIROH and AWI repositories.",
        "keywords": ["template", "repository", "GitHub", "CIROH", "AWI", "software engineering", "documentation", "open source"],
        "entities": ["Alabama Water Institute", "CIROH", "NOAA", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 48,
        "title": "training-data-workflows-101",
        "source": "https://github.com/CIROH-UA/training-data-workflows-101",
        "section_header": "# training-data-workflows-101"
        },
        "content": "# Project Title: **training-data-workflows-101**  \\n\\n## Project Objective  \\n**Training Data Workflows 101** introduces hydrologists and data scientists to reproducible, modular software engineering practices for research workflows. It helps transition from exploratory Jupyter notebooks to production-ready Python modules, teaching scalable data pipeline design.\\n\\n## Core Functionalities  \\n- Covers workflow modularization, reproducibility, and maintainability using Git and pandas.  \\n- Teaches data acquisition from USGS NWIS and NOAA NWM APIs.  \\n- Demonstrates a three-stage workflow: Acquire → Manipulate → Visualize.  \\n- Implements a standardized `pandas.Series` interface across all workflow stages.  \\n- Provides nine Jupyter notebooks and Python modules for hands-on practice.\\n\\n## Technical Stack  \\n- **Languages:** Python 3.11  \\n- **Libraries:** pandas, NumPy, xarray, dataretrieval, s3fs, matplotlib, seaborn, holoviews, plotly.  \\n- **Environments:** CIROH 2i2c JupyterHub and local setups.  \\n- **License:** Apache 2.0.\\n\\n## Project Context & Domain  \\n- **Domain:** Hydrology / Data Science / Software Engineering.  \\n- **Affiliation:** CIROH, The University of Alabama.  \\n- **Purpose:** Teach reproducible hydrologic data workflow development aligned with software engineering principles.\\n\\n## Input / Output  \\n**Input:** Hydrologic data from USGS and NOAA APIs.  \\n**Output:** Modular Python workflows, visualizations, and reproducible data pipelines.",
        "summary": "training-data-workflows-101 is a CIROH educational workshop teaching reproducible data workflows for hydrology. It emphasizes modular design, version control, and standardized pandas-based interfaces to help researchers transition from exploratory notebooks to production-ready pipelines.",
        "keywords": ["data workflows", "hydrology", "Python", "USGS", "NOAA", "modularity", "reproducibility", "Jupyter"],
        "entities": ["CIROH", "NOAA", "USGS", "The University of Alabama"]
    },
    {
        "metadata": {
        "idurl": 49,
        "title": "ciroh-ua.github.io",
        "source": "https://github.com/CIROH-UA/ciroh-ua.github.io",
        "section_header": "# ciroh-ua.github.io"
        },
        "content": "# Project Title: **ciroh-ua.github.io**  \\n\\n## Project Objective  \\n**CIROH DocuHub** is CIROH’s main documentation and community portal built with Docusaurus v3.8.1. It consolidates technical documentation, service guides, policies, and community updates across CIROH’s hydrologic research network.\\n\\n## Core Functionalities  \\n- Hosts documentation for CIROH’s products, services, and infrastructure.  \\n- Integrates Algolia search, Google Analytics, and Draw.io for visualization.  \\n- Provides blog, news, and release note feeds with tag-based discovery.  \\n- Highlights sponsors, partners, and member institutions.  \\n- Maintains full accessibility and responsive design for public engagement.\\n\\n## Technical Stack  \\n- **Framework:** Docusaurus v3.8.1.  \\n- **Technologies:** React, Webpack, Infima CSS.  \\n- **Integrations:** Algolia, Google Tag Manager, jsDelivr CDN.  \\n- **Hosting:** CIROH Web infrastructure.  \\n- **License:** Apache 2.0.\\n\\n## Project Context & Domain  \\n- **Domain:** Documentation / Hydrology / Cloud Infrastructure.  \\n- **Affiliation:** CIROH, NOAA, The University of Alabama.  \\n- **Purpose:** Provide a unified, public-facing documentation portal for CIROH research and cloud services.\\n\\n## Input / Output  \\n**Input:** Markdown/MDX documents, configuration files, images.  \\n**Output:** Compiled static site for CIROH’s documentation and outreach.",
        "summary": "ciroh-ua.github.io is CIROH’s public documentation hub built with Docusaurus. It hosts technical guides, policy documents, and community news for the hydrologic research community, integrating analytics and search for accessibility and transparency.",
        "keywords": ["Docusaurus", "React", "documentation", "CIROH", "NOAA", "knowledge hub", "AWS", "static site"],
        "entities": ["CIROH", "NOAA", "The University of Alabama", "Algolia"]
    }
]