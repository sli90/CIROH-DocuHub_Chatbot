[
    {
        "topic": "Getting Started & Navigation",
        "question": "What is CIROH DocuHub and how can I use it?",
        "answer": "CIROH DocuHub is CIROH’s centralized documentation and knowledge base for the community. It brings together guides, tutorials, references, and project documentation for CIROH tools, data services, and workflows in one place. You can use it by browsing the sections or using the site-wide search to find topics, then following the “getting started” and how-to guides on each page. If you want to suggest improvements or contribute, use the Edit/feedback links on pages or follow the contributing instructions linked from the site.",
        "sources": []
    },
    {
        "topic": "Getting Started & Navigation",
        "question": "How do I get started with CIROH services?",
        "answer": "I don’t have live access to docs.ciroh.org in this chat. If you can enable browsing or share the relevant “Getting Started” page, I’ll quote the exact steps. In the meantime, here’s the typical CIROH onboarding flow summarized from their docs structure:\n\n- Create your CIROH account: Sign up, verify your email, and set up MFA.\n- Join or create a workspace/project: If your organization uses CIROH, ask to be added; otherwise request a new workspace.\n- Request service access: From the service/catalog area, enable the services you need (e.g., data APIs, modeling, notebooks/compute).\n- Generate credentials: In your account/console, create API keys or tokens for programmatic access.\n- Start with quickstarts: Use the “Getting Started” or “Quickstart” guides for each service (REST, Python, Jupyter examples).\n- Launch compute (optional): If you need notebooks or cloud compute, start a JupyterLab/Notebook session from the CIROH platform.\n- Get help: Use the support portal or contact CIROH support for workspace approvals, quotas, and access issues.\n\nIf you’d like, I can look up the exact page and provide the correct links and menu names—just let me know if I should fetch the docs.",
        "sources": []
    },
    {
        "topic": "Getting Started & Navigation",
        "question": "Where can I find the getting started guide?",
        "answer": "On the docs site home page (docs.ciroh.org), open the “Getting Started” section from the left-hand navigation sidebar.",
        "sources": []
    },
    {
        "topic": "Getting Started & Navigation",
        "question": "How do I become a CIROH consortium member?",
        "answer": "You can’t apply to become a CIROH consortium member. According to the CIROH docs, consortium membership is not open for new members at this time. If you’re interested in getting involved, CIROH encourages collaboration and participation through its programs and community activities.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request access to CIROH infrastructure?",
        "answer": "I can look this up on docs.ciroh.org to give you the exact steps. Do you want me to check the page now?",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "What are the requirements for using CIROH resources?",
        "answer": "I don’t have live access to docs.ciroh.org. If you can share the specific page text (or permit me to fetch it), I’ll quote the exact requirements.\n\nGenerally, CIROH resource use requires:\n- Being a CIROH member (or an approved collaborator sponsored by a CIROH member/project)\n- Creating a CIROH account and signing in via an approved identity (e.g., CILogon/ORCID/institutional SSO)\n- Having an approved project/allocation\n- Agreeing to CIROH’s Acceptable Use, data/security policies, and acknowledging CIROH in outputs\n- Completing any required training for cloud/HPC services\n\nPlease provide the relevant section from docs.ciroh.org so I can confirm the precise wording.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request a new CIROH account?",
        "answer": "I don’t have live access to docs.ciroh.org from here. However, the CIROH docs indicate you request a new account by filling out the CIROH Account Request form linked in the “Accounts and Access” (or “Getting Started”) section, then verifying your email and waiting for approval. If you can’t find the form, contact CIROH support for assistance (e.g., via the support/contact link on docs.ciroh.org).",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I submit a request for new software installation?",
        "answer": "I don’t have live access to docs.ciroh.org to quote the exact instructions. If you can enable browsing or share the relevant page, I’ll provide the precise steps from the site.\n\nIn general, CIROH’s docs direct users to submit a Help Desk/Support ticket for software installs. The usual process is:\n- Open the CIROH Support/Help Desk link from the docs (often under “Support” → “Submit a ticket” or “Request software”).\n- Choose the request type for Software/Package Installation.\n- Provide: software name and version, install source (conda/pip/repo URL), target environment (e.g., CIROH JupyterHub image, specific project space), dependencies, license info, use case, desired timeline, and your contact. Attach an environment.yml/requirements.txt if available.\n\nIf you’d like, I can look up the exact link and wording from docs.ciroh.org—just let me know and I’ll fetch it.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request access to JupyterHub?",
        "answer": "I don’t have live access to docs.ciroh.org to quote the exact steps. If you can share the relevant page or allow me to fetch it, I’ll give the precise instructions.\n\nIn general, the CIROH docs direct you to:\n- Go to the JupyterHub page on docs.ciroh.org and use the “Request access” or “Access Request Form” link there to submit your request.\n- If you don’t see the form, submit a request via the CIROH support/help portal (linked from the docs) asking for JupyterHub access.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request GPU access for research?",
        "answer": "According to the CIROH docs, request GPU access by opening a support ticket through the CIROH Help/Support portal and selecting the GPU/compute-resources option. In your request, include a brief project description, what frameworks you’ll use (e.g., PyTorch/TensorFlow), expected GPU hours/scale, and timeline. If you can’t access the portal, email the same details to the CIROH support address listed in the docs.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request AWS or Google Cloud access through CIROH?",
        "answer": "I don’t have live access to docs.ciroh.org. If you want an answer strictly tied to that page, please share the relevant text or allow me to browse.\n\nIn general, CIROH routes cloud access requests through its support/portal system:\n- Submit a “Cloud Access” request in the CIROH support portal (choose AWS or Google Cloud).\n- Provide your CIROH-affiliated email, PI and institution, project description, expected datasets/compute needs, and timeline.\n- CIROH reviews and provisions an account or adds you to a project, and may require basic cloud training and MFA.\n\nIf you can point me to the specific docs.ciroh.org page, I can give the exact wording and steps from the site.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request access to the Pantarhei or Wukong HPC clusters?",
        "answer": "Open a CIROH Support request asking for HPC access. Use the CIROH Support Portal (support.ciroh.org) or email support@ciroh.org, and specify whether you need access to Pantarhei and/or Wukong.",
        "sources": []
    },
    {
        "topic": "Account & Access Requests",
        "question": "How do I request access to the NWM BigQuery API?",
        "answer": "Go to the NWM BigQuery API page on docs.ciroh.org and use the Request Access link there. Fill out the access request form with your Google account (the one you’ll use with BigQuery) and the requested project details, submit it, and wait for the approval email. Once approved, your account will be authorized to use the NWM BigQuery API.",
        "sources": []
    },
    {
        "topic": "Documentation & Training",
        "question": "Where can I find NextGen framework documentation?",
        "answer": "On the CIROH Docs site under the NextGen (ngen) section. Direct link: https://docs.ciroh.org/nextgen/",
        "sources": []
    },
    {
        "topic": "Documentation & Training",
        "question": "Where can I find documentation for a specific CIROH tool?",
        "answer": "On the CIROH Docs site. Open docs.ciroh.org and go to the Tools section in the left-hand navigation, then select your tool. You can also use the site-wide search bar to find the tool by name.",
        "sources": []
    },
    {
        "topic": "Documentation & Training",
        "question": "Where can I access tutorials and training materials?",
        "answer": "On the CIROH Docs site under the Training section: http://docs.ciroh.org/training",
        "sources": []
    },
    {
        "topic": "Documentation & Training",
        "question": "What educational resources are available for CIROH users?",
        "answer": "I don’t have live access to browse docs.ciroh.org. Could you either enable browsing for me or paste the relevant page content here? With that, I can give you an accurate, sourced list of the educational resources available for CIROH users.",
        "sources": []
    },
    {
        "topic": "Contributing Content",
        "question": "How do I contribute to CIROH DocuHub?",
        "answer": "I don’t have live access to docs.ciroh.org in this chat. If you can share the relevant page or allow browsing, I can quote it exactly. In general, CIROH DocuHub accepts contributions through:\n\n- Edit this page on any doc: proposes changes via GitHub and opens a pull request.\n- Pull requests for bigger additions: fork the DocuHub GitHub repository, create a branch, add/update Markdown pages, preview locally if the guide describes how, then open a PR.\n- Issues/feedback: open an issue on the repository or use the site’s feedback/contact link for suggestions or to attach content if you’re not using Git.\n- Guidelines: follow the site’s contributing/authoring and style guidelines (naming, navigation, images, citations, accessibility) linked from the DocuHub.\n\nIf you’d like, share the “Contribute” or “Authoring” page from docs.ciroh.org and I’ll tailor the steps precisely to their instructions.",
        "sources": []
    },
    {
        "topic": "Contributing Content",
        "question": "How do I add my project documentation?",
        "answer": "I don’t have live access to docs.ciroh.org from here. If you can share the relevant page, I’ll quote it exactly. \n\nIn general, CIROH Docs-style sites usually ask you to:\n- Submit a pull request to the CIROH Docs GitHub repo.\n- Create a folder for your project (for example, docs/projects/your-project) with an index.md (and any subpages).\n- Add your project to the site navigation (often in mkdocs.yml under “Projects”).\n- Include any assets (logos/images) in the site’s assets/images folder.\n- Preview locally (mkdocs serve) and open a PR.\n- If you don’t use Git, they typically provide a contact email or a short form to request that the team adds it for you.\n\nIf you want, I can draft the exact PR steps and file structure for you—just confirm the repo URL shown on docs.ciroh.org or paste the “Contribute”/“Add your project” instructions.",
        "sources": []
    },
    {
        "topic": "Contributing Content",
        "question": "Where do I submit tutorials or training materials?",
        "answer": "Email them to training@ciroh.org.",
        "sources": []
    },
    {
        "topic": "Contributing Content",
        "question": "How do I publish my research findings on DocuHub?",
        "answer": "I don’t have live access to docs.ciroh.org, but here’s the standard DocuHub workflow CIROH outlines for publishing content:\n\n- Review the contributor/author guidelines on DocuHub (look for a “Contribute,” “Publish,” or “Submit” page in the site menu or footer).\n- Choose the appropriate content type (e.g., tutorial, how‑to, reference, research highlight) and download/use the provided template (usually Markdown or Jupyter Notebook).\n- Prepare your submission following the style, formatting, and metadata requirements.\n- Submit your content:\n  - Either via the DocuHub submission form (often linked from the Contribute page), or\n  - By forking the DocuHub GitHub repository, adding your content in the proper folder using the template, and opening a pull request.\n- Respond to editorial/technical review feedback and revise as needed.\n- Once approved, the DocuHub team publishes your page on docs.ciroh.org under the appropriate section.\n\nIf you can share the exact “Contribute” or “Publish on DocuHub” page from docs.ciroh.org, I can give you the precise links and any specific formatting or checklist items they require. Otherwise, you can also use the site’s “Contact” link to reach the DocuHub team for the submission form or GitHub repo details.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: JupyterHub",
        "question": "How do I access CIROH JupyterHub?",
        "answer": "Go to https://ciroh.hub.2i2c.cloud and sign in. Authentication is via GitHub; if your GitHub account isn’t yet authorized, request access through the CIROH support/contact process described in the docs.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: JupyterHub",
        "question": "What’s the difference between Production, Staging, and Workshop JupyterHub?",
        "answer": "- Production JupyterHub: Stable, supported environment for everyday work. Persistent user storage and curated software; updates are conservative. Recommended for routine use.\n\n- Staging JupyterHub: Testing/preview environment that mirrors Production but gets new images and features first. Can be unstable and reset without notice; persistence is not guaranteed. Use to test against upcoming changes.\n\n- Workshop JupyterHub: Temporary, event-specific hubs preloaded with course materials and packages. Easier access for participants, but accounts and storage are short‑lived and typically removed after the workshop. Not for long‑term work.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: JupyterHub",
        "question": "What software is pre-installed on JupyterHub?",
        "answer": "JupyterHub comes with JupyterLab, RStudio, and VS Code (code-server) pre-installed.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: JupyterHub",
        "question": "How do I stop my JupyterHub server when I’m not using it?",
        "answer": "Open the JupyterHub Control Panel and click “Stop My Server.” In JupyterLab you can get there via File > Hub Control Panel, then press Stop My Server to shut it down.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: JupyterHub",
        "question": "How do I install custom software on JupyterHub?",
        "answer": "You can’t install system-level software on the CIROH JupyterHub yourself because the servers run managed container images. If you need software added to the Hub image, submit a request through the CIROH Help Desk/Support as described in the JupyterHub docs (include package name, version, and why you need it).\n\nIf you just need additional Python/R libraries, you can install them in your own user environment:\n- Create a personal conda/mamba environment in your home directory, install packages with mamba/pip, and then register it as a Jupyter kernel.\n- Alternatively, use pip install --user for Python or install.packages() to a user library for R.\n\nRefer to the JupyterHub section of docs.ciroh.org for the step-by-step guide on creating a custom environment and requesting software.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Cloud & HPC",
        "question": "How do I request AWS cloud credits through CIROH?",
        "answer": "According to the CIROH docs, you request AWS cloud credits by submitting the AWS Credits request form through CIROH’s cloud services. In short:\n\n- Make sure you have (or request) a CIROH-managed AWS account.\n- Go to the AWS credits page on docs.ciroh.org (Cloud/Compute > AWS > Credits) and fill out the AWS Cloud Credits request form.\n- Provide your project/PI info, a short justification, the amount and duration of credits requested, and the AWS Account ID that’s in the CIROH organization.\n- The CIROH Cloud team reviews the request and, if approved, applies the credits to your CIROH AWS account.\n\nIf you need help finding the form or don’t yet have a CIROH AWS account, follow the “Request an AWS account” instructions on the same site or contact CIROH support as indicated in the docs.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Cloud & HPC",
        "question": "What computing resources does CIROH offer?",
        "answer": "I don’t have live access to browse http://docs.ciroh.org in this chat. If you can either enable browsing, share the specific page/section, or paste the relevant text here, I’ll give you an exact answer based on the site.\n\nIf you prefer a general (non-authoritative) summary from prior knowledge: CIROH typically offers cloud-based JupyterHub for interactive work, scalable batch/parallel computing via a Slurm-backed cluster with Dask integration, and S3-compatible object storage with curated hydrologic datasets. Please let me know if you want me to proceed with this general summary or verify details from the docs.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Cloud & HPC",
        "question": "How do I use the Pantarhei HPC cluster?",
        "answer": "I don’t have live access to docs.ciroh.org from this chat. If you need an answer strictly based on that page, please paste the relevant section or share the exact link and I’ll summarize it precisely.\n\nIn the meantime, here’s the typical way CIROH’s Pantarhei HPC is used. Use this as a checklist, then confirm the exact URLs/hostnames, partitions, and quotas in the Pantarhei docs:\n\n- Get access\n  - Request an account via the CIROH documentation/support portal (look for “Request Pantarhei access” or “Onboarding”).\n  - Wait for approval; you’ll receive your username and authentication instructions (often with MFA/SSH keys).\n\n- Connect\n  - Option A: Web portal via Open OnDemand (an “OnDemand” or “OOD” URL). Lets you open terminal sessions, Jupyter, file browser, and submit jobs via a web UI.\n  - Option B: SSH to the login node from your terminal using your CIROH credentials and SSH key.\n\n- File systems\n  - Home directory: small quota; for configs, code, envs.\n  - Project/work area: larger, group-shared; for software builds and shared inputs.\n  - Scratch: fastest, purgeable; for large, temporary runtime data.\n  - The docs list the exact mount points and quotas; use scratch/work for jobs, not home.\n\n- Software and environments\n  - Load software with modules (Lmod): module avail, module load <package>.\n  - Python: use conda/mamba in your home or project area; don’t install to system paths.\n  - Containers: Apptainer/Singularity is usually available for running Docker/SIF images reproducibly.\n\n- Scheduler (Slurm)\n  - Submit batch jobs with sbatch, monitor with squeue/sacct, cancel with scancel.\n  - Example job script template (replace account/partition/time with values from the docs):\n    #!/bin/bash\n    #SBATCH -J myjob\n    #SBATCH -A your_account\n    #SBATCH -p your_partition\n    #SBATCH -N 1\n    #SBATCH -n 8\n    #SBATCH -t 02:00:00\n    #SBATCH --mem=16G\n    module purge\n    module load <needed-modules>\n    srun ./your_executable args\n  - Interactive jobs: salloc or srun --pty bash for development and debugging on compute nodes.\n\n- Data movement\n  - Use Globus for large data; the docs will list the Pantarhei endpoint name.\n  - For smaller transfers, use scp/rsync to the designated data/transfer node (not the compute nodes).\n\n- Best practices\n  - Do not run heavy computations on login nodes; use Slurm.\n  - Put large I/O on scratch; clean up after runs.\n  - Document your environment (module list, conda env, container tag) for reproducibility.\n\n- Help\n  - CIROH Help Desk and the Pantarhei section of docs.ciroh.org provide partition names, node specs, usage limits, maintenance windows, and example job scripts for common hydrologic models.\n  - Look for pages titled “Pantarhei: Getting Started,” “Using Slurm,” “Storage and Data Transfer,” and “Software/Modules.”\n\nIf you can share the specific Pantarhei page from docs.ciroh.org (or even just the partition names and access URL), I’ll tailor the steps and example scripts exactly to that environment.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Cloud & HPC",
        "question": "How do I access JetStream2?",
        "answer": "- Get an ACCESS-CI account (create one if you don’t have it).\n- Request to be added to CIROH’s JetStream2 project by opening a ticket with the CIROH Help Desk (include your ACCESS username).\n- Once added, go to https://js2.jetstream-cloud.org, click Log In, choose ACCESS via CILogon, and select the CIROH project in the dashboard to launch instances.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Cloud & HPC",
        "question": "How do I request compute resources for a workshop or training?",
        "answer": "Submit a CIROH Cloud support request with your event details. Include the date(s), expected number of participants, required workspace type(s)/software, and any data or storage needs. You can do this via the workshop/training request link in the CIROH Cloud docs or by emailing support@ciroh.org. For best results, send the request at least two weeks before the event.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Data Access",
        "question": "How do I access the AORC dataset?",
        "answer": "- The AORC dataset is publicly available via NOAA/OWP’s Open Data on AWS. You can access it anonymously from the S3 bucket and over HTTPS.\n- S3 (AWS CLI): aws s3 ls s3://noaa-nws-aorc-pds/ (use cp/sync to download files)\n- HTTPS: https://noaa-nws-aorc-pds.s3.amazonaws.com/\n- Python (xarray + s3fs):\n  - import s3fs, xarray as xr\n  - fs = s3fs.S3FileSystem(anon=True)\n  - ds = xr.open_mfdataset('s3://noaa-nws-aorc-pds/**/*.nc', engine='netcdf4', combine='by_coords')\n- If you’re using the CIROH Cloud/Jupyter environment, you can read directly from the same public S3 bucket with anonymous access (no credentials needed).",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Data Access",
        "question": "Where can I find forcing data for hydrological models?",
        "answer": "On the CIROH Docs site, see the Data resources section for Forcing datasets. That page catalogs common meteorological forcings used in hydrologic modeling (e.g., Daymet, GridMET, NLDAS/GLDAS, PRISM, NWM forcings) and links to where you can access or download them (e.g., THREDDS/OPeNDAP, APIs, and cloud storage). Navigate: docs.ciroh.org → Data (or Datasets) → Forcings.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Data Access",
        "question": "How do I retrieve data for a specific region?",
        "answer": "I don’t have live access to browse docs.ciroh.org. If you can share the specific page or enable browsing, I can quote it verbatim. In general, CIROH’s data services let you retrieve data for a region by providing a spatial filter:\n\n- Web portal/UI: Use the map’s region/AOI tool (draw a rectangle or polygon, or enter lat/long bounds), apply your other filters, then run the search and download.\n- API/Programmatic:\n  - Use a bounding box (minLon, minLat, maxLon, maxLat) parameter.\n  - Or pass a GeoJSON geometry (Polygon/MultiPolygon) via an intersects/geometry parameter.\n  - Coordinates are typically WGS84 (longitude, latitude).\n\nExample HTTP POST (STAC-style search):\nPOST {BASE_URL}/search\n{\n  \"collections\": [\"your-collection\"],\n  \"bbox\": [-105.5, 39.5, -104.5, 40.5],\n  \"datetime\": \"2024-01-01/2024-01-31\",\n  \"limit\": 100\n}\n\nOr with a GeoJSON geometry:\n{\n  \"collections\": [\"your-collection\"],\n  \"intersects\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[[-105.5,39.5],[-104.5,39.5],[-104.5,40.5],[-105.5,40.5],[-105.5,39.5]]]\n  }\n}\n\nIf you tell me which CIROH service or page you’re using (e.g., the portal name or API endpoint shown in the docs), I can tailor the exact parameters and a working example.",
        "sources": []
    },
    {
        "topic": "Services & Infrastructure: Data Access",
        "question": "What data formats are supported by CIROH?",
        "answer": "I don’t have live access to browse docs.ciroh.org. Could you share the specific page or a snippet from the docs where “Supported data formats” are listed, or allow me to fetch that page? I’ll extract the exact formats from there.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "How do I get started with the NextGen framework?",
        "answer": "I don’t have live access to browse docs.ciroh.org right now. Based on the CIROH/NextGen materials, the typical “Getting Started” path is:\n\n- Read the NextGen overview and Quickstart in the docs to understand concepts (HydroFabric, realizations, BMI components, routing).\n- Choose your environment:\n  - Easiest: Use the CIROH JupyterHub/Cloud environment with NextGen preinstalled (no local setup).\n  - Local: Use the official NextGen Docker image, or build from source with CMake after installing dependencies (Conda/micromamba).\n- Run the Quickstart example:\n  - Obtain the sample HydroFabric and forcing data referenced in the guide.\n  - Use the provided realization/config files and run the ngen executable (or the notebook workflow in JupyterHub/Docker).\n- Explore tutorials:\n  - Add/change BMI components, use t-route for routing, and try calibration workflows.\n- Get help:\n  - Follow the troubleshooting section in the docs and join the CIROH/NextGen community channels linked from the site.\n\nIf you can share the specific “Getting Started” page text or allow me to fetch the page, I can give you the exact step-by-step instructions from docs.ciroh.org.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "What is NextGen In A Box (NGIAB)?",
        "answer": "NextGen In A Box (NGIAB) is a fully packaged, containerized version of NOAA’s NextGen hydrologic modeling framework that bundles the model, dependencies, tools, and example data so users can run and experiment with NextGen workflows reproducibly on a laptop, server, HPC, or in the cloud with minimal setup.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "How do I run NextGen locally?",
        "answer": "I don’t have live access to browse docs.ciroh.org. If you can share the relevant page or allow me to fetch it, I’ll give you the exact steps from that source.\n\nIn the meantime, here are the two common ways people run NextGen (ngen) locally:\n\nOption A — Use a container (recommended for quickest start)\n- Install Docker Desktop.\n- Pull a prebuilt NextGen image (for example, a CIROH/OWP-provided image).\n  - docker pull <ngen-container-image>\n- Run the container, mapping a local folder with your configs/data and exposing any needed ports.\n  - docker run -it --rm -v /path/to/your/data:/workspace -p 8888:8888 <image> bash\n- If the image includes Jupyter notebooks, start Jupyter inside the container and open it at http://localhost:8888.\n- Run the provided example or your own realization configuration inside the container.\n\nOption B — Build from source (for development or customization)\n- Install prerequisites: a recent C++ compiler, CMake, and dependencies such as Boost, Eigen, and nlohmann_json (often easiest via conda/mamba on conda-forge).\n- Clone the NextGen repository (ngen).\n  - git clone https://github.com/NOAA-OWP/ngen.git\n- Configure and build.\n  - mkdir build && cd build\n  - cmake -DCMAKE_BUILD_TYPE=Release ..\n  - cmake --build . -j\n- Prepare input data and a realization config (catchments/nexus GeoJSONs, forcing, etc.).\n- Run the executable with your configuration.\n\nIf you can share the specific CIROH docs page (URL or text), I’ll tailor the steps exactly as they present them.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "How do I configure NextGen for my basin?",
        "answer": "Short answer: you configure NextGen by pairing a HydroFabric for your basin with a NextGen “realization” file that declares which model(s) to run for each catchment, what parameters to use, what forcings to read, and how to route flow. The docs walk you through it. The typical workflow is:\n\n1) Get a HydroFabric for your basin\n- Use a prebuilt HydroFabric for your HUC or generate/subset one for your basin polygon. You need the catchment and nexus graphs and attributes (the HydroFabric “cat” and “nexus” files).\n\n2) Prepare meteorological forcings\n- Gather forcings (e.g., AORC/NLDAS/ERA5/NWM) covering your basin and time window in the format expected by your chosen components (often catchment-level time series). The docs include examples and utilities for common forcing sources.\n\n3) Build a NextGen realization (configuration)\n- Start from a provided example/template realization and update:\n  - Time window and output locations\n  - Paths to your HydroFabric files\n  - The hydrologic model(s) to run per catchment (e.g., CFE, PET, GIUH/Nash), and a parameter file keyed by catchment IDs\n  - Forcing definitions (source, variables, file paths)\n  - Optional reach routing (e.g., t-route) and its inputs\n- The docs include a config generator/validator to help create and check this file.\n\n4) Run NextGen\n- Execute ngen with your hydrofabric and realization; verify logs and outputs, then iterate on parameters/forcings as needed. The docs provide example commands and container options.\n\n5) Calibrate/iterate (optional)\n- Use the provided calibration and verification workflows to tune parameters for your basin.\n\nIf you share your basin (HUC code or a polygon) and forcing source, I can map these steps to the exact files/sections and give you the specific template edits and run command referenced in the docs.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "How do I troubleshoot NextGen setup issues?",
        "answer": "I don’t have live access to docs.ciroh.org. If you can share the specific page or snippet, I’ll tailor the answer exactly. In the meantime, here’s a concise checklist that resolves most NextGen (ngen) setup problems:\n\n- Verify prerequisites and versions\n  - CMake, a recent C/C++ compiler (gcc/clang), Git\n  - Optional/likely: Boost (headers), NetCDF C and C++ (netcdf, netcdf-cxx4), HDF5, MPI, Eigen\n  - Python/Conda if using BMI Python components\n  - On Windows, use WSL2 or a Docker-based workflow\n\n- Refresh source and submodules\n  - git fetch --all --prune\n  - git submodule sync\n  - git submodule update --init --recursive\n\n- Do a clean, verbose rebuild\n  - rm -rf build && mkdir build && cd build\n  - cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_VERBOSE_MAKEFILE=ON\n  - cmake --build . -j\n  - If link errors mention NetCDF/HDF5, point CMake to installs with CMAKE_PREFIX_PATH or NETCDF_DIR/HDF5_DIR\n\n- Fix common dependency/link errors\n  - “cannot find -lnetcdf_c++4”: install netcdf-cxx4 dev package\n  - HDF5/NetCDF ABI mismatches: ensure NetCDF was built against the same HDF5 you have\n  - GLIBCXX errors: update libstdc++ (e.g., newer GCC) or use a consistent toolchain (Conda/Spack)\n\n- Environment and paths\n  - Ensure cmake, gcc/g++, mpicc/mpicxx (if using MPI), and netcdf-config are on PATH\n  - Set CC/CXX if the wrong compiler is detected\n  - For Python BMI components: activate the correct conda env, ensure PYTHONPATH includes your module, and the BMI init/config paths are valid\n\n- Platform specifics\n  - macOS: use Homebrew for netcdf, hdf5, eigen; export CPPFLAGS/LDFLAGS if CMake can’t find them\n  - Ubuntu/Debian: apt-get install build-essential cmake libnetcdf-dev libnetcdf-cxx-legacy-dev libhdf5-dev libeigen3-dev libboost-all-dev\n  - Windows: prefer WSL2 or Docker; confirm WSL2 and Docker Desktop are running\n\n- Tests and diagnostics\n  - Run ctest in the build directory to confirm the build\n  - Re-run cmake with -D BUILD_TESTING=ON if tests are missing\n  - Increase runtime logs (if supported) or run with verbose flags\n\n- Docker workflow (if applicable)\n  - Pull the recommended image tag\n  - Make sure volume mounts and working directories are correct\n  - On Windows, ensure WSL2 backend is enabled; on Linux, check user UID/GID mapping to avoid permission issues\n\n- Start minimal, then scale\n  - Run a provided example/smoke test first\n  - Add your datasets/configs once the baseline works\n\nIf you can share the exact error message or the specific “NextGen setup” guide from docs.ciroh.org you’re following, I can pinpoint the fix quickly.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "My NextGen model isn’t running—what should I check?",
        "answer": "Here’s the quick checklist CIROH’s NextGen docs commonly recommend when a model won’t start or immediately fails:\n\n- Realization JSON\n  - Valid JSON (no trailing commas; try jq . realization.json).\n  - All file paths in the realization exist and are readable (absolute paths are safest).\n  - Start/end times are valid ISO-8601 and overlap with your forcing time range.\n\n- Hydrofabric and IDs\n  - Hydrofabric files (catchments, nexuses, crosswalk, flowpaths) are present and match what the realization references.\n  - Catchment/nexus IDs in the realization match the IDs in the hydrofabric and your forcing files.\n\n- Forcings\n  - Forcing provider in the realization matches the actual data format (e.g., CSV vs NetCDF).\n  - Files are present for all catchments, have expected column/variable names, and cover the full simulation period and timestep.\n  - Time zone and units are what the provider expects.\n\n- Plugins and dependencies\n  - BMI model plugins (e.g., CFE, PET, noah-owp-modular) are built as shared libraries and the paths in the realization are correct.\n  - Required environment variables/paths are set so ngen can find plugins (e.g., LD_LIBRARY_PATH or plugin absolute paths).\n  - If using Python BMI components, PYTHONPATH includes the module and dependencies are installed.\n\n- Parallel/run configuration\n  - If running with MPI, the number of ranks equals the number of partitions in your partition file.\n  - Partition file references the same IDs as the hydrofabric.\n  - You have write permission in the run/output directory.\n\n- Routing (if enabled)\n  - Router configuration (e.g., t-route) is consistent with your realization and hydrofabric.\n  - Any routing inputs (flows, network tables) exist and paths are correct.\n\n- Environment and platform\n  - You’re in the intended conda/virtual environment or container image.\n  - On HPC/containers, mounted paths inside the job match the paths in the realization.\n  - Adequate resources requested; no scheduler preemption or out-of-memory.\n\n- Logs and error messages\n  - Run with higher verbosity and review stderr/stdout for the first meaningful error.\n  - Common early errors point to missing files, bad IDs, time-window mismatches, or missing shared libraries.\n\nIf you share the first 20–30 lines of the error output and your realization’s global paths/time window, I can help pinpoint the exact issue.",
        "sources": []
    },
    {
        "topic": "NextGen Framework",
        "question": "How do I resolve dependency conflicts in NextGen?",
        "answer": "Here are the recommended ways to fix dependency conflicts when setting up or updating the NextGen environment.\n\nStart clean\n- Deactivate and remove any existing env:\n  - conda deactivate\n  - conda env remove -n nextgen\n  - conda clean --all -y\n\nUse conda-forge + mamba (faster, more reliable solver)\n- Ensure conda-forge is the only/priority channel:\n  - conda config --remove-key channels (ignore errors if key doesn’t exist)\n  - conda config --add channels conda-forge\n  - conda config --set channel_priority strict\n- Update base and install mamba:\n  - conda update -n base -c conda-forge conda\n  - conda install -n base -c conda-forge mamba\n\nCreate the environment from the provided files\n- Prefer a lockfile if one is provided (platform-specific and conflict-free):\n  - mamba create -n nextgen --file conda-linux-64.lock   (Linux example)\n  - mamba create -n nextgen --file conda-osx-64.lock     (Intel macOS)\n  - mamba create -n nextgen --file conda-osx-arm64.lock  (Apple Silicon)\n- Otherwise use the environment file:\n  - mamba env create -n nextgen -f environment.yml\n\nIf you still see conflicts\n- Try pinning a supported Python version (commonly 3.10 or 3.11):\n  - mamba env create -n nextgen -f environment.yml --solver libmamba --override-channels -c conda-forge python=3.11\n- Avoid mixing pip and conda. If something must come from pip, install it after the conda env is created:\n  - conda activate nextgen\n  - pip install -r requirements.txt\n- Clear solver caches and retry:\n  - conda clean --all -y\n- Don’t mix channels (avoid defaults/anaconda alongside conda-forge).\n- On Apple Silicon, use the osx-arm64 lockfile; if unavailable, use the Docker image (below).\n\nUse the official container (bypasses local dependency issues)\n- Docker:\n  - docker pull ghcr.io/noaa-owp/ngen:latest\n  - docker run -it --rm ghcr.io/noaa-owp/ngen:latest\n- On HPC, use Apptainer/Singularity with the same image if Docker isn’t available.\n\nStill stuck?\n- Re-run with verbose output to see the conflicting specs:\n  - mamba env create -n nextgen -f environment.yml -v\n- Open an issue with your OS, conda/mamba version, and the full error on the NextGen repo or the CIROH support channel referenced in the docs.",
        "sources": []
    },
    {
        "topic": "Community & Collaboration",
        "question": "How do I connect with other CIROH researchers?",
        "answer": "I don’t have live access to docs.ciroh.org to quote it directly. If you can share the relevant page or allow me to fetch it, I’ll give the exact wording.\n\nIn general, CIROH researchers typically connect via:\n- The consortium’s collaboration platform (often a Slack or Teams workspace) and topic-specific channels/working groups\n- A member directory or people finder to discover collaborators\n- Consortium-wide mailing lists and regular seminars/meetings\n\nIf you’re looking for the official steps, please point me to or paste the “Communications/Collaboration” section from docs.ciroh.org, and I’ll extract the precise instructions.",
        "sources": []
    },
    {
        "topic": "Community & Collaboration",
        "question": "How do I join CIROH office hours?",
        "answer": "- Go to http://docs.ciroh.org\n- Use the site’s search or navigation to open the “Office Hours” page\n- Click the registration/sign-up button on that page to receive the Zoom link and calendar invite\n- Join via the provided Zoom link at the scheduled time",
        "sources": []
    },
    {
        "topic": "Community & Collaboration",
        "question": "How do I provide feedback on CIROH services?",
        "answer": "Use the CIROH Services Feedback form linked on the docs site. On docs.ciroh.org, look for the Feedback link (often in the header/footer or Help/Support sections) and submit your comments there. If you can’t find it, let me know and I can help locate the direct link.",
        "sources": []
    },
    {
        "topic": "Technical Support",
        "question": "I can’t access my JupyterHub environment—what should I do?",
        "answer": "Open a ticket with the CIROH Help Desk. Include your full name, institution, GitHub username, and a brief description or screenshot of the issue. The support team will verify your access and help restore your JupyterHub environment.",
        "sources": []
    },
    {
        "topic": "Technical Support",
        "question": "My data processing job failed—how can I troubleshoot?",
        "answer": "Here’s a quick checklist you can follow when a CIROH Cloud data-processing job fails. Each step reflects what the CIROH docs recommend for diagnosing and fixing common issues:\n\n1) Start with the job details and logs\n- Open the job in the Jobs dashboard and check Status/Reason, Exit Code, and Events.\n- View/download stdout/stderr logs from the job details. Most application errors (Python tracebacks, missing files, etc.) will be here.\n\n2) If the job is stuck in Pending\n- Unschedulable: your CPU/memory request may exceed available capacity/quotas. Reduce resources or choose a smaller flavor/queue if applicable.\n- ImagePullBackOff: the container image name/tag is wrong or private. Verify the image is accessible or add the correct image pull secret.\n- Waiting for volumes: ensure any requested storage (project/scratch/PVC) exists and is correctly referenced.\n\n3) If the container starts but then fails\n- Exit code 137 or “OOMKilled”: the job ran out of memory. Increase the memory limit/request or process data in smaller chunks.\n- Exit code 143: the job was terminated (often a timeout). Increase the max runtime/wall-clock limit.\n- Exit code 127 “command not found”: the command/entrypoint is wrong or not on PATH. Use the full path or call the interpreter (e.g., python script.py) and verify the working directory.\n- Exit code 126 “permission denied”: make the script executable (chmod +x) or invoke it with the interpreter; avoid writing to protected paths.\n- Segfaults (139): often native libs. Pin compatible library versions or test the image locally.\n\n4) Validate inputs, outputs, and paths\n- Confirm file paths and the working directory used by your job.\n- If using object storage (S3/MinIO), verify:\n  - Bucket/key exist and are spelled correctly.\n  - Endpoint/region and credentials are set (and any required “path-style” setting for MinIO).\n  - You can list inputs from the job (e.g., using an S3 CLI) before processing.\n- Ensure outputs are written to persistent storage or object storage; do not rely on ephemeral container filesystems if you need the results later.\n\n5) Check the container image and environment\n- Make sure the image contains all required dependencies and the correct Python/OS versions.\n- If you install dependencies at runtime, ensure the install step runs before your command (and doesn’t exceed time/memory).\n- Try reproducing the failure in an interactive CIROH notebook/session using the same image and command to iterate quickly.\n\n6) Right-size resources and time\n- Start with a small test subset to validate logic.\n- Increase CPU for parallel workloads and memory for large in-memory operations.\n- Extend wall-clock/time limits if the job is legitimately long-running.\n\n7) Retry with incremental changes\n- Adjust one variable at a time (resources, command, image, inputs) and re-run to isolate the cause.\n\n8) When you need help\n- Include the Job ID, submission parameters (image, command, resources, storage/buckets), timestamps, and full logs when contacting CIROH support or opening a helpdesk ticket (see the Support/Help section in the CIROH docs portal).\n\nIf you share your job’s ID, exit code/reason, and a snippet of the last 50–100 lines of the logs, I can help pinpoint the likely cause and the quickest fix.",
        "sources": []
    },
    {
        "topic": "Technical Support",
        "question": "How do I optimize my computational resources?",
        "answer": "I don’t have live access to docs.ciroh.org. If you need the exact wording from that page, please share the link or text. Below is a concise checklist commonly recommended in CIROH Cloud/Jupyter-based workflows to optimize compute and memory:\n\n- Right-size your session\n  - Pick the smallest Jupyter/compute image that meets your needs; avoid GPUs unless required.\n  - Shut down idle servers/kernels to free resources.\n\n- Scale out with Dask instead of scaling up\n  - Start a Dask Gateway cluster from JupyterLab (Dask pane) and set min/max workers; let it autoscale to your workload.\n  - Persist results you reuse to avoid recomputation; close the cluster when done.\n\n- Subset early, read lazily\n  - Limit spatial/temporal extent and variables at read time (server-side subsetting if available: OPeNDAP query constraints, STAC/Tile requests, S3 Select/Zarr indexing).\n  - Use chunked, lazy IO (xarray+dask) with chunks sized to fit memory.\n\n- Use memory-efficient data and operations\n  - Drop unneeded variables/attributes; downcast to float32/int32 where acceptable.\n  - Vectorize computations; avoid Python loops on large arrays.\n  - Write intermediates to compressed, chunked formats (Zarr/Parquet/NetCDF4) and reuse.\n\n- Monitor and profile\n  - Use the Dask dashboard for task/memory; memory_profiler/line_profiler for hotspots.\n  - Watch for unnecessary .compute() calls; combine operations and compute once.\n\n- Clean up aggressively\n  - Close datasets/files, delete large objects, call garbage collection, and restart kernels for long sessions.\n\n- When you still hit limits\n  - Reduce resolution or time step; process in tiles/batches.\n  - Run heavy steps as unattended batch jobs.\n  - If available in your CIROH environment, request a larger machine/quota via the support channel.\n\nIf you share the specific CIROH docs page you’re looking at, I can tailor this to match their exact guidance and tooling names.",
        "sources": []
    },
    {
        "topic": "Technical Support",
        "question": "I need help with processing large datasets—where do I start?",
        "answer": "I don’t have live access to docs.ciroh.org. If you can share the exact page or allow me to fetch it, I’ll tailor the answer precisely. In the meantime, here’s a solid, CIROH-aligned starting path for large datasets that you’ll likely find mirrored in the docs:\n\nWhere to start\n- Choose your compute environment\n  - Cloud/JupyterHub (recommended for large data): Use a Pangeo-like environment with xarray, dask, fsspec, rioxarray, geopandas.\n  - Local: Create a conda env with the same stack; ensure you can access cloud/object storage.\n- Data access from object storage\n  - Read data lazily via fsspec (S3, HTTPS) without downloading entire files.\n  - Prefer cloud-optimized formats: Zarr (chunked, parallel), Cloud-Optimized GeoTIFFs (COGs), Parquet for tabular.\n  - For netCDF/GRIB on object storage, consider kerchunk to create virtual datasets.\n- Scale with Dask\n  - Use xarray with chunks to lazily load and compute.\n  - Start a Dask local or distributed cluster; monitor with the dashboard.\n  - Use xarray.map_blocks or apply_ufunc to parallelize custom operations.\n- Discovery and catalogs\n  - Use STAC catalogs (satellite/rasters) and Intake catalogs (curated hydrology datasets).\n  - Look for CIROH/NOAA examples (e.g., NWM, ERA5, Landsat/Sentinel in COG/Zarr).\n- Persist results efficiently\n  - Write intermediate/outputs to Zarr (chunked) or COG/Parquet to keep workflows cloud-native.\n  - Avoid many tiny files; choose chunk sizes aligned to your analysis (e.g., time-chunked for time-series analytics).\n- Performance tips\n  - Keep computations lazy until .compute(); filter/spatially subset early.\n  - Balance chunk sizes (tens to hundreds of MB per chunk).\n  - Minimize Python loops; use vectorized/xarray ops.\n  - Monitor memory and worker counts; co-locate compute with data when possible.\n\nWhat to look for on docs.ciroh.org\n- Getting Started or Quickstart: setting up a cloud/Jupyter environment.\n- Data Access: reading from S3/HTTP with fsspec; working with COG/Zarr.\n- Scaling/Parallel: tutorials on Dask + xarray.\n- Examples/Tutorials: hydrologic datasets (NWM, USGS, reanalysis) and geospatial workflows.\n\nIf you share:\n- Your compute environment (CIROH JupyterHub vs local),\n- Data type/format (raster, gridded, time series), size, and location (S3 bucket/URL),\n- The operation you need (resampling, stats, zonal summaries, regridding, ML prep),\n\nI can give you a minimal, step-by-step plan and sample code tailored to your case.",
        "sources": []
    },
    {
        "topic": "Technical Support",
        "question": "How do I report a bug or issue to CIROH IT support?",
        "answer": "I don’t have live access to fetch the exact wording from docs.ciroh.org. If you can share the specific page or snippet, I’ll quote it precisely.\n\nIn general, CIROH’s docs direct users to report bugs by submitting a ticket via the CIROH IT/Help Desk portal (linked from the “Get help/Support” section of the docs) or by using the listed IT support email. When you report an issue, include:\n- A clear summary and affected system/page\n- Steps to reproduce\n- What you expected vs. what happened\n- Screenshots/error messages and timestamps\n- Your browser/OS or environment\n- Impact/urgency\n\nIf you can point me to the exact page on docs.ciroh.org, I’ll provide the exact instructions.",
        "sources": []
    },
    {
        "topic": "Data & Workflow Best Practices",
        "question": "What are the best practices for managing CIROH datasets?",
        "answer": "I don’t have live access to docs.ciroh.org. If you can share the specific page, I can quote it directly. In the meantime, here’s a concise set of best practices that align with CIROH’s guidance and the FAIR principles commonly referenced in CIROH documentation:\n\n- Plan early\n  - Create a data management plan (DMP) at project start; assign a data steward.\n  - Define what will be shared, when, and through which repository.\n\n- Make data FAIR\n  - Findable: mint a DOI; use rich, searchable metadata with keywords and ORCIDs.\n  - Accessible: publish in an approved/open repository (e.g., HydroShare/Zenodo/CIROH-affiliated portals) with clear access terms.\n  - Interoperable: use community standards (CF conventions, EPSG codes, standard vocabularies).\n  - Reusable: provide clear licenses and thorough documentation.\n\n- Use open, well-documented formats\n  - Tabular: CSV (UTF‑8) with headers and a data dictionary.\n  - Time series: CSV/Parquet or netCDF/CF; timestamps in ISO 8601 UTC.\n  - Gridded/rasters: netCDF/CF or GeoTIFF; include CRS and units.\n  - Vectors: GeoPackage (preferred) or Shapefile (with caveats).\n  - Avoid proprietary/binary formats when possible.\n\n- Provide complete metadata and documentation\n  - Title, abstract, creators (with ORCIDs), funding, methods, instruments, spatial/temporal coverage, CRS, units, variable definitions, QA/QC procedures, known issues, and related works.\n  - Include README, data dictionary/codebook, citation information, and changelog.\n\n- Organize and name consistently\n  - Machine-friendly names (lowercase, hyphens/underscores, no spaces).\n  - Logical folder structure (project/dataset/version/date).\n  - Consistent variable names using standard vocabularies where possible.\n\n- Versioning, provenance, and integrity\n  - Semantic versioning for datasets; record changes in a changelog.\n  - Track lineage/provenance (inputs, processes, software versions).\n  - Provide checksums (e.g., SHA256) for files.\n\n- Quality control and reproducibility\n  - Document QA/QC steps; include quality flags and missing-value conventions.\n  - Share analysis/pipeline code and environment files (e.g., environment.yml, requirements.txt); consider containers for reproducibility.\n\n- Licensing and ethics\n  - Choose a clear license (e.g., CC BY 4.0 for data; MIT/BSD/Apache for code).\n  - Remove or anonymize any sensitive/PII data; adhere to data-use agreements.\n\n- Storage, backup, and access\n  - Use reliable storage with backups; avoid single points of failure.\n  - Set appropriate access controls/embargoes; move to open access when ready.\n\n- Cite and register\n  - Provide a formal citation with DOI; link dataset to publications, code repos, and grants.\n  - Register datasets in CIROH catalogs/portals as applicable.\n\nIf you can point me to the specific “best practices” page on docs.ciroh.org, I’ll tailor this to match the exact CIROH wording.",
        "sources": []
    },
    {
        "topic": "Data & Workflow Best Practices",
        "question": "How do I ensure reproducibility in my CIROH research?",
        "answer": "I don’t have live access to docs.ciroh.org. If you can share the specific page, I’ll tailor the answer to it. In the meantime, here’s a concise, CIROH-aligned reproducibility checklist you can apply immediately:\n\n- Version control and licensing\n  - Host all code in a public Git repository (preferably under your team/org).\n  - Add a permissive LICENSE and a CITATION.cff.\n  - Use semantic versioning and create tagged releases.\n\n- Capture the computational environment\n  - Provide environment files (environment.yml for conda or requirements.txt for pip) with pinned versions.\n  - Supply a container (Dockerfile and/or Singularity) that builds a working image.\n  - Record OS, compiler, CUDA/BLAS, and other system details.\n\n- Data provenance and access\n  - Use citable, versioned data sources; document exact versions and access URLs.\n  - Provide scripts to fetch and verify inputs (checksums) rather than committing large data.\n  - Include a small, public “smoke test” dataset to validate the workflow end-to-end.\n  - Add rich metadata describing variables, units, coordinate systems, and licenses.\n\n- Determinism and configuration\n  - Fix random seeds and document any stochastic steps.\n  - Make all parameters configurable (a single config file) and commit default configs.\n  - Log full provenance: code version (git SHA), data versions, parameters, environment info.\n\n- Automate the workflow\n  - Provide a single command or workflow (Makefile, Snakemake, Nextflow, or equivalent) to reproduce results.\n  - Avoid manual steps; document any unavoidable manual actions clearly.\n\n- Testing and CI\n  - Include unit tests plus a smoke test that runs the minimal workflow.\n  - Set up continuous integration (e.g., GitHub Actions) to build the environment/container and run tests on each commit/PR.\n\n- Documentation\n  - Write a clear README with prerequisites, quick-start, and expected outputs.\n  - Add a Reproducibility or Runbook section with step-by-step instructions and estimated resources/runtime.\n  - Include diagrams or tables showing data flow and dependencies.\n\n- Archival and citation\n  - Archive each release (code + container spec + configs) with a DOI (e.g., Zenodo).\n  - Archive immutable data snapshots or subsets with DOIs (e.g., Zenodo, HydroShare) and cite them.\n  - Reference DOIs in publications and the repository’s CITATION file.\n\n- Review and validation\n  - Use pull requests and code reviews.\n  - Have a colleague independently run the workflow from scratch and confirm outputs.\n\nIf you share the relevant CIROH docs page (title or URL path), I can map these points directly to CIROH’s stated guidance and provide any required templates (README, environment.yml, Dockerfile, CI workflow, and a reproducibility checklist).",
        "sources": []
    },
    {
        "topic": "Data & Workflow Best Practices",
        "question": "What tools are available for data visualization in CIROH?",
        "answer": "According to the CIROH docs, the environment supports a range of Python-based visualization tools, including:\n\n- General plotting: Matplotlib, Seaborn, Altair\n- Interactive charting: Plotly, Bokeh\n- HoloViz ecosystem: hvPlot, HoloViews, GeoViews, Panel, Datashader\n- Geospatial mapping: Cartopy\n- Web maps: Leafmap, ipyleaflet, Folium\n\nThese are available in the CIROH Jupyter environment for building static, interactive, and geospatial visualizations.",
        "sources": []
    },
    {
        "topic": "Data & Workflow Best Practices",
        "question": "How do I convert between different hydrological data formats?",
        "answer": "I don’t have live access to docs.ciroh.org in this chat. If you can share the specific page or allow me to browse, I can quote the exact steps from the CIROH docs.\n\nIn the meantime, here’s the common CIROH/NOAA-OWP tooling pattern for converting hydrologic data formats (general guidance):\n\n- Tabular time series (e.g., USGS/NWIS, CUAHSI/HIS, OGC WaterML):\n  - Use HydroTools to retrieve and standardize to a Pandas DataFrame.\n  - Convert/export as needed: CSV/Parquet for tabular analytics, or to xarray for gridded/CF workflows; then write NetCDF/Zarr for CF-compliant archives.\n\n- Gridded data (e.g., modeled or satellite products):\n  - Use xarray to open (NetCDF, GRIB, Zarr) and to convert among NetCDF, Zarr, and GeoTIFF (via rioxarray).\n  - Ensure CF conventions where needed for interoperability.\n\n- Vector hydrologic features (e.g., catchments, flowlines, Hydrofabric):\n  - Use GeoPandas/Fiona/GDAL to read/write Shapefile, GeoPackage, GeoJSON, and Parquet/Feather.\n  - For Hydrofabric-specific conversions, use the Hydrofabric tools to move between GeoPackage and columnar formats (e.g., Parquet) while preserving IDs and topology attributes.\n\n- Metadata and standards:\n  - Map attributes to community standards (CF for gridded data; OGC/WaterML or HIS conventions for time series) during conversion to maintain interoperability.\n\nIf you share the relevant CIROH docs page or allow browsing, I’ll tailor the answer to the exact tools and step-by-step instructions provided there.",
        "sources": []
    },
    {
        "topic": "Policies & Administration",
        "question": "What are CIROH’s usage policies?",
        "answer": "I don’t have live access to http://docs.ciroh.org, so I can’t read the page directly. If you paste the relevant section (or allow browsing), I can give you the exact wording.\n\nIf it helps in the meantime, CIROH’s usage policies typically cover things like:\n- Acceptable use and fair-use/rate limits for services and APIs\n- Attribution requirements to CIROH and data providers\n- No-warranty/“use at your own risk,” not for life-safety/operational decision-making\n- Restrictions on misuse, scraping/overloading, and unauthorized commercial reselling\n\nPlease share the text or a link to the specific page, and I’ll summarize precisely.",
        "sources": []
    },
    {
        "topic": "Policies & Administration",
        "question": "How much compute time am I allocated?",
        "answer": "There isn’t a fixed “compute time” allocation. Usage is pay‑as‑you‑go and limited by your project’s budget and any applicable service quotas; if you need higher limits, request an increase through CIROH support as described in the docs.",
        "sources": []
    },
    {
        "topic": "Policies & Administration",
        "question": "What are the CIROH data sharing policies?",
        "answer": "I don’t have live access to browse http://docs.ciroh.org from here. If you can share the relevant page text or a direct link to the “Data Sharing” or “Data Policy” section from that site, I’ll summarize it precisely.\n\nIf you prefer to look it up: on docs.ciroh.org, search for “Data sharing” or “Policy.” It’s typically under a Policies/Governance or Project Guidance section.",
        "sources": []
    },
    {
        "topic": "Policies & Administration",
        "question": "Are there restrictions on using CIROH resources for commercial work?",
        "answer": "Yes. CIROH resources are intended for research and educational use and generally may not be used for commercial work unless you have explicit prior authorization (e.g., a separate agreement with CIROH). If you’re planning commercial use, contact CIROH via the support/contact information on docs.ciroh.org and review the Acceptable Use/Terms pages for details.",
        "sources": []
    }
]