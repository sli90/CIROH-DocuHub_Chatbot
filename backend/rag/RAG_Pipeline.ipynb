{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef4182e",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe904a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "load_dotenv()\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "BASE_URL = \"https://docs.ciroh.org\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc560df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databes connection\n",
    "conn = psycopg2.connect(\n",
    "    host=PG_HOST,\n",
    "    database=PG_DB,\n",
    "    user=PG_USER,\n",
    "    password=PG_PASS\n",
    ")\n",
    "\n",
    "def execute_query(conn, query, params=None, fetch=False):\n",
    "    cur = conn.cursor()\n",
    "    \"\"\"Execute a SQL query with optional parameters.\"\"\"\n",
    "    try:\n",
    "        if params:\n",
    "            cur.execute(query, params)\n",
    "        else:\n",
    "            cur.execute(query)\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "            return result\n",
    "        else:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54333dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadcrumb(conn, url_id):\n",
    "    \"\"\"\n",
    "    Generates the breadcrumb trail for a given idurl using a recursive query.\n",
    "\n",
    "    Args:\n",
    "        conn: The database connection object.\n",
    "        url_id: The idurl of the page for which to generate the trail.\n",
    "\n",
    "    Returns:\n",
    "        A string with the breadcrumb (e.g., \"Home > Products > NGIAB\"),\n",
    "        or None if an error occurs or the idurl is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    WITH RECURSIVE breadcrumb_path AS (\n",
    "        -- Anchor Member: Select the starting page\n",
    "        SELECT\n",
    "            idurl,\n",
    "            name,\n",
    "            idurlparent,\n",
    "            1 AS depth -- Initial depth level\n",
    "        FROM\n",
    "            tblurls\n",
    "        WHERE\n",
    "            idurl = %s\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Recursive Member: Join the table to find the parent\n",
    "        SELECT\n",
    "            u.idurl,\n",
    "            u.name,\n",
    "            u.idurlparent,\n",
    "            bp.depth + 1 -- Increment depth at each level\n",
    "        FROM\n",
    "            tblurls u\n",
    "        JOIN\n",
    "            breadcrumb_path bp ON u.idurl = bp.idurlparent\n",
    "    )\n",
    "    -- Select the final result, aggregating the names into a single string\n",
    "    SELECT\n",
    "        string_agg(name, ' > ' ORDER BY depth DESC) AS breadcrumb\n",
    "    FROM\n",
    "        breadcrumb_path;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = execute_query(conn, query, params=(url_id,), fetch=True)\n",
    "        \n",
    "        if result and result[0] and result[0][0]:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_breadcrumb for idurl {url_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb74cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"How can I get the meeting link to join the monthly CIROH office hours for AWS and cyberinfrastructure support?\",\n",
    "    \"What are the main differences between the Anvil and Derecho supercomputers?\",\n",
    "    \"What is the correct procedure for tagging an EC2 instance on AWS according to CIROH's convention?\",\n",
    "    \"I need to run a job on the Pantarhei cluster. What is the maximum duration and core count I can request for a single job without needing special permission?\",\n",
    "    \"I'm new to CIROH and want to start with hydrologic modeling. What is NextGen In A Box (NGIAB) and what are the main deployment options available?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b29167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, dimensions, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Call OpenAI to get an embedding for the given text.\"\"\"\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=model,\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(question_embedding):\n",
    "    \"\"\"Queries the database to find the most relevant summaries.\"\"\"\n",
    "    # The query finds the top 3 most similar summaries using cosine similarity (<=>)\n",
    "    # The trailing comma in `params=(question_embedding,)` is crucial to ensure it's a tuple.\n",
    "    responses = execute_query(conn, \"\"\"\n",
    "        SELECT idurl, summary_data ->> 'summary_text' as summary\n",
    "        FROM tblurls\n",
    "        WHERE summary_data IS NOT NULL\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT 3\n",
    "    \"\"\", params=(question_embedding,), fetch=True)\n",
    "    \n",
    "    # Return the database rows\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, context_summaries):\n",
    "    \"\"\"Builds the prompt for the LLM to answer the question based on context.\"\"\"\n",
    "    \n",
    "    # Joins the retrieved summaries into a single block of text\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(context_summaries)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert AI assistant for the CIROH DocuHub. Your task is to answer the user's question based *only* on the provided context.\n",
    "\n",
    "If the context does not contain the answer, state that you cannot answer the question with the information given. Do not use any external knowledge.\n",
    "\n",
    "**CONTEXT:**\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "\n",
    "**QUESTION:**\n",
    "{question}\n",
    "\n",
    "**ANSWER:**\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final answer from the LLM\n",
    "def get_rag_answer(prompt):\n",
    "    \"\"\"Calls the LLM to generate the final answer.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\", # Or your preferred model\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            reasoning_effort=\"minimal\",\n",
    "            verbosity=\"low\"\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating RAG answer: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(questions):\n",
    "    print(f\"--- \\n\\n‚ùì PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "        \n",
    "    # 1. Get the embedding for the current question\n",
    "    question_embedding = get_embedding(question, dimensions=1792)\n",
    "        \n",
    "    if question_embedding is None:\n",
    "        print(\"Could not generate embedding for the question. Skipping.\")\n",
    "        continue\n",
    "            \n",
    "    # 2. Query the database to get relevant context\n",
    "    retrieved_context = query_embedding(question_embedding)\n",
    "        \n",
    "    if not retrieved_context:\n",
    "        print(\"No relevant context found in the database. Skipping.\")\n",
    "        continue\n",
    "            \n",
    "    # Extract just the summary texts for the prompt\n",
    "    context_ids = [row[0] for row in retrieved_context]\n",
    "    context_summaries = [row[1] for row in retrieved_context]\n",
    "        \n",
    "    print(\"üìö Retrieved Context:\")\n",
    "    for summary in context_summaries:\n",
    "        print(f\"- {summary[:120]}...\") # Print a snippet of each summary\n",
    "    print()\n",
    "\n",
    "    source_breadcrumbs = []\n",
    "    for url_id in context_ids:\n",
    "        breadcrumb = get_breadcrumb(conn, url_id)\n",
    "        if breadcrumb:\n",
    "            source_breadcrumbs.append(breadcrumb)\n",
    "\n",
    "    # 3. Build the RAG prompt\n",
    "    rag_prompt = build_rag_prompt(question, context_summaries)\n",
    "        \n",
    "    # 4. Get the final answer from the LLM\n",
    "    final_answer = get_rag_answer(rag_prompt)\n",
    "        \n",
    "    if final_answer:\n",
    "        print(f\"‚úÖ Final Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "        if source_breadcrumbs:\n",
    "            print(\"üîó Sources:\")\n",
    "            unique_trails = list(dict.fromkeys(source_breadcrumbs))\n",
    "            for trail in unique_trails:\n",
    "                print(f\"- {trail}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIROH_AIBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
