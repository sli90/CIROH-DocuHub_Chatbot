{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef4182e",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe904a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "load_dotenv()\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "BASE_URL = \"https://docs.ciroh.org\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc560df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databes connection\n",
    "conn = psycopg2.connect(\n",
    "    host=PG_HOST,\n",
    "    database=PG_DB,\n",
    "    user=PG_USER,\n",
    "    password=PG_PASS\n",
    ")\n",
    "\n",
    "def execute_query(conn, query, params=None, fetch=False):\n",
    "    cur = conn.cursor()\n",
    "    \"\"\"Execute a SQL query with optional parameters.\"\"\"\n",
    "    try:\n",
    "        if params:\n",
    "            cur.execute(query, params)\n",
    "        else:\n",
    "            cur.execute(query)\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "            return result\n",
    "        else:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54333dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadcrumb(conn, url_id):\n",
    "    \"\"\"\n",
    "    Generates the breadcrumb trail for a given idurl using a recursive query.\n",
    "\n",
    "    Args:\n",
    "        conn: The database connection object.\n",
    "        url_id: The idurl of the page for which to generate the trail.\n",
    "\n",
    "    Returns:\n",
    "        A string with the breadcrumb (e.g., \"Home > Products > NGIAB\"),\n",
    "        or None if an error occurs or the idurl is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    WITH RECURSIVE breadcrumb_path AS (\n",
    "        -- Anchor Member: Select the starting page\n",
    "        SELECT\n",
    "            idurl,\n",
    "            name,\n",
    "            idurlparent,\n",
    "            1 AS depth -- Initial depth level\n",
    "        FROM\n",
    "            tblurls\n",
    "        WHERE\n",
    "            idurl = %s\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Recursive Member: Join the table to find the parent\n",
    "        SELECT\n",
    "            u.idurl,\n",
    "            u.name,\n",
    "            u.idurlparent,\n",
    "            bp.depth + 1 -- Increment depth at each level\n",
    "        FROM\n",
    "            tblurls u\n",
    "        JOIN\n",
    "            breadcrumb_path bp ON u.idurl = bp.idurlparent\n",
    "    )\n",
    "    -- Select the final result, aggregating the names into a single string\n",
    "    SELECT\n",
    "        string_agg(name, ' > ' ORDER BY depth DESC) AS breadcrumb\n",
    "    FROM\n",
    "        breadcrumb_path;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = execute_query(conn, query, params=(url_id,), fetch=True)\n",
    "        \n",
    "        if result and result[0] and result[0][0]:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_breadcrumb for idurl {url_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb74cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "'''\n",
    "questions = [\n",
    "    \"How can I get the meeting link to join the monthly CIROH office hours for AWS and cyberinfrastructure support?\",\n",
    "    \"What are the main differences between the Anvil and Derecho supercomputers?\",\n",
    "    \"What is the correct procedure for tagging an EC2 instance on AWS according to CIROH's convention?\",\n",
    "    \"I need to run a job on the Pantarhei cluster. What is the maximum duration and core count I can request for a single job without needing special permission?\",\n",
    "    \"I'm new to CIROH and want to start with hydrologic modeling. What is NextGen In A Box (NGIAB) and what are the main deployment options available?\"\n",
    "]\n",
    "'''\n",
    "questions = {\n",
    "    \"Getting Started & Navigation\": [\n",
    "        \"What is CIROH DocuHub and how can I use it?\",\n",
    "        \"How do I get started with CIROH services?\",\n",
    "        \"Where can I find the getting started guide?\",\n",
    "        \"How do I become a CIROH consortium member?\"\n",
    "    ],\n",
    "    \"Account & Access Requests\": [\n",
    "        \"How do I request access to CIROH infrastructure?\",\n",
    "        \"What are the requirements for using CIROH resources?\",\n",
    "        \"How do I request a new CIROH account?\",\n",
    "        \"How do I submit a request for new software installation?\",\n",
    "        \"How do I request access to JupyterHub?\",\n",
    "        \"How do I request GPU access for research?\",\n",
    "        \"How do I request AWS or Google Cloud access through CIROH?\",\n",
    "        \"How do I request access to the Pantarhei or Wukong HPC clusters?\",\n",
    "        \"How do I request access to the NWM BigQuery API?\"\n",
    "    ],\n",
    "    \"Documentation & Training\": [\n",
    "        \"Where can I find NextGen framework documentation?\",\n",
    "        \"Where can I find documentation for a specific CIROH tool?\",\n",
    "        \"Where can I access tutorials and training materials?\",\n",
    "        \"What educational resources are available for CIROH users?\"\n",
    "    ],\n",
    "    \"Contributing Content\": [\n",
    "        \"How do I contribute to CIROH DocuHub?\",\n",
    "        \"How do I add my project documentation?\",\n",
    "        \"Where do I submit tutorials or training materials?\",\n",
    "        \"How do I publish my research findings on DocuHub?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: JupyterHub\": [\n",
    "        \"How do I access CIROH JupyterHub?\",\n",
    "        \"What‚Äôs the difference between Production, Staging, and Workshop JupyterHub?\",\n",
    "        \"What software is pre-installed on JupyterHub?\",\n",
    "        \"How do I stop my JupyterHub server when I‚Äôm not using it?\",\n",
    "        \"How do I install custom software on JupyterHub?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: Cloud & HPC\": [\n",
    "        \"How do I request AWS cloud credits through CIROH?\",\n",
    "        \"What computing resources does CIROH offer?\",\n",
    "        \"How do I use the Pantarhei HPC cluster?\",\n",
    "        \"How do I access JetStream2?\",\n",
    "        \"How do I request compute resources for a workshop or training?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: Data Access\": [\n",
    "        \"How do I access the AORC dataset?\",\n",
    "        \"Where can I find forcing data for hydrological models?\",\n",
    "        \"How do I retrieve data for a specific region?\",\n",
    "        \"What data formats are supported by CIROH?\"\n",
    "    ],\n",
    "    \"NextGen Framework\": [\n",
    "        \"How do I get started with the NextGen framework?\",\n",
    "        \"What is NextGen In A Box (NGIAB)?\",\n",
    "        \"How do I run NextGen locally?\",\n",
    "        \"How do I configure NextGen for my basin?\",\n",
    "        \"How do I troubleshoot NextGen setup issues?\",\n",
    "        \"My NextGen model isn‚Äôt running‚Äîwhat should I check?\",\n",
    "        \"How do I resolve dependency conflicts in NextGen?\"\n",
    "    ],\n",
    "    \"Community & Collaboration\": [\n",
    "        \"How do I connect with other CIROH researchers?\",\n",
    "        \"How do I join CIROH office hours?\",\n",
    "        \"How do I provide feedback on CIROH services?\"\n",
    "    ],\n",
    "    \"Technical Support\": [\n",
    "        \"I can‚Äôt access my JupyterHub environment‚Äîwhat should I do?\",\n",
    "        \"My data processing job failed‚Äîhow can I troubleshoot?\",\n",
    "        \"How do I optimize my computational resources?\",\n",
    "        \"I need help with processing large datasets‚Äîwhere do I start?\",\n",
    "        \"How do I report a bug or issue to CIROH IT support?\"\n",
    "    ],\n",
    "    \"Data & Workflow Best Practices\": [\n",
    "        \"What are the best practices for managing CIROH datasets?\",\n",
    "        \"How do I ensure reproducibility in my CIROH research?\",\n",
    "        \"What tools are available for data visualization in CIROH?\",\n",
    "        \"How do I convert between different hydrological data formats?\"\n",
    "    ],\n",
    "    \"Policies & Administration\": [\n",
    "        \"What are CIROH‚Äôs usage policies?\",\n",
    "        \"How much compute time am I allocated?\",\n",
    "        \"What are the CIROH data sharing policies?\",\n",
    "        \"Are there restrictions on using CIROH resources for commercial work?\"\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b29167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, dimensions, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Call OpenAI to get an embedding for the given text.\"\"\"\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=model,\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(question_embedding, max_relevant_urls=3):\n",
    "    \"\"\"Queries the database to find the most relevant summaries.\"\"\"\n",
    "    # The query finds the top N most similar summaries using cosine similarity (<=>)\n",
    "    responses = execute_query(conn, \"\"\"\n",
    "        SELECT idurl, summary_data ->> 'summary_text' as summary\n",
    "        FROM tblurls\n",
    "        WHERE summary_data IS NOT NULL\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s\n",
    "    \"\"\", params=(question_embedding, max_relevant_urls), fetch=True)\n",
    "    \n",
    "    # Return the database rows\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, context_summaries):\n",
    "    \"\"\"Builds the prompt for the LLM to answer the question based on context.\"\"\"\n",
    "    \n",
    "    # Joins the retrieved summaries into a single block of text\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(context_summaries)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert AI assistant for the CIROH DocuHub. Your task is to answer the user's question based *only* on the provided context.\n",
    "\n",
    "If the context does not contain the answer, state that you cannot answer the question with the information given. Do not use any external knowledge.\n",
    "\n",
    "**CONTEXT:**\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "\n",
    "**QUESTION:**\n",
    "{question}\n",
    "\n",
    "**ANSWER:**\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final answer from the LLM\n",
    "def get_rag_answer(prompt):\n",
    "    \"\"\"Calls the LLM to generate the final answer.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\", \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            reasoning_effort=\"minimal\",\n",
    "            verbosity=\"low\"\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating RAG answer: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4795d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chunks(question_embedding, relevant_url_ids, max_relevant_chunks=5):\n",
    "    \"\"\"\n",
    "    Queries the TBLContent table to find the most relevant chunks\n",
    "    only within the pages identified in the first-level search.\n",
    "    \"\"\"\n",
    "    # The query searches for the top 5 most similar chunks, but only\n",
    "    # within the list of provided idurls.\n",
    "    responses = execute_query(conn, \"\"\"\n",
    "        SELECT idurl, \"order\", Content\n",
    "        FROM TBLContent\n",
    "        WHERE idurl = ANY(%s)\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "    \"\"\", params=(relevant_url_ids, question_embedding, max_relevant_chunks), fetch=True)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6685351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to control the RAG pipeline behavior\n",
    "max_relevant_urls = 5\n",
    "max_relevant_chunks = 10\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_data = []\n",
    "\n",
    "# Loop for RAG Pipeline (searching in TBLURLs only) with new dictionary structure\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "    \n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n‚ùì PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Get the embedding for the current question\n",
    "        question_embedding = get_embedding(question, dimensions=1792)\n",
    "            \n",
    "        if question_embedding is None:\n",
    "            print(\"Could not generate embedding for the question. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # 2. Query the database to get relevant context\n",
    "        retrieved_context = query_embedding(question_embedding, max_relevant_urls=max_relevant_urls)\n",
    "            \n",
    "        if not retrieved_context:\n",
    "            print(\"No relevant context found in the database. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # Extract just the summary texts for the prompt\n",
    "        context_ids = [row[0] for row in retrieved_context]\n",
    "        context_summaries = [row[1] for row in retrieved_context]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"üìö Retrieved Context:\")\n",
    "            for summary in context_summaries:\n",
    "                print(f\"- {summary[:120]}...\") # Print a snippet of each summary\n",
    "            print()\n",
    "\n",
    "        source_breadcrumbs = []\n",
    "        for url_id in context_ids:\n",
    "            breadcrumb = get_breadcrumb(conn, url_id)\n",
    "            if breadcrumb:\n",
    "                source_breadcrumbs.append(breadcrumb)\n",
    "\n",
    "        # 3. Build the RAG prompt\n",
    "        rag_prompt = build_rag_prompt(question, context_summaries)\n",
    "            \n",
    "        # 4. Get the final answer from the LLM\n",
    "        final_answer = get_rag_answer(rag_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"‚úÖ Final Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            sources_list = []\n",
    "            if source_breadcrumbs:\n",
    "                print(\"üîó Sources:\")\n",
    "                unique_trails = list(dict.fromkeys(source_breadcrumbs))\n",
    "                sources_list = unique_trails\n",
    "                for trail in unique_trails:\n",
    "                    print(f\"- {trail}\")\n",
    "                print()\n",
    "            \n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": sources_list\n",
    "            }\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "output_file = \"rag_results_by_url.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ All questions processed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_data = []\n",
    "\n",
    "# RAG Pipeline Loop for knowledge searching in TBLURLs and TBLContent with new dictionary structure\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "\n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n‚ùì PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Get the embedding for the current question\n",
    "        question_embedding = get_embedding(question, dimensions=1792)\n",
    "            \n",
    "        if question_embedding is None:\n",
    "            print(\"Could not generate embedding for the question. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # --- LEVEL 1 SEARCH: Find relevant pages and their summaries ---\n",
    "        retrieved_pages = query_embedding(question_embedding, max_relevant_urls=max_relevant_urls)\n",
    "\n",
    "        if not retrieved_pages:\n",
    "            print(\"No relevant pages found in the database. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # Create a map of idurl -> summary from the initial results for efficient access\n",
    "        page_summary_map = {row[0]: row[1] for row in retrieved_pages}\n",
    "        context_ids = list(page_summary_map.keys())\n",
    "        if verbose:\n",
    "            print(f\"üìö Level 1: Found relevant pages with IDs: {context_ids}\")\n",
    "\n",
    "        # --- LEVEL 2 SEARCH: Find relevant chunks within those pages ---\n",
    "        retrieved_chunks = query_chunks(question_embedding, context_ids, max_relevant_chunks=max_relevant_chunks)\n",
    "\n",
    "        final_context_list = []\n",
    "        source_ids = []\n",
    "\n",
    "        if not retrieved_chunks:\n",
    "            # Fallback strategy: If no specific chunks are found, use the page summaries from Level 1\n",
    "            if verbose:\n",
    "                print(\"No specific chunks found, falling back to page summaries for context.\")\n",
    "            final_context_list = list(page_summary_map.values())\n",
    "            source_ids = context_ids\n",
    "        else:\n",
    "            # --- NEW LOGIC: Group chunks by page and build rich, multi-source context ---\n",
    "            if verbose:\n",
    "                print(f\"üéØ Level 2: Found {len(retrieved_chunks)} relevant chunks across pages.\")\n",
    "            \n",
    "            chunks_by_page = {}\n",
    "            for idurl, order, content in retrieved_chunks:\n",
    "                if idurl not in chunks_by_page:\n",
    "                    chunks_by_page[idurl] = []\n",
    "                chunks_by_page[idurl].append(order)\n",
    "\n",
    "            # Iterate through the original pages to maintain relevance order\n",
    "            for page_id in context_ids:\n",
    "                if page_id in chunks_by_page:\n",
    "                    source_ids.append(page_id)\n",
    "                    \n",
    "                    # Get the page summary from the map (no new DB call)\n",
    "                    page_summary = page_summary_map.get(page_id, \"No summary available.\")\n",
    "                    \n",
    "                    # Get the order numbers of relevant chunks for this page\n",
    "                    relevant_orders = chunks_by_page[page_id]\n",
    "                    \n",
    "                    # Expand to include neighbors and remove duplicates\n",
    "                    orders_with_neighbors = set()\n",
    "                    for order_num in relevant_orders:\n",
    "                        orders_with_neighbors.add(order_num - 1)\n",
    "                        orders_with_neighbors.add(order_num)\n",
    "                        orders_with_neighbors.add(order_num + 1)\n",
    "                    \n",
    "                    # Fetch all unique chunks (originals + neighbors) in correct document order\n",
    "                    expanded_chunk_rows = execute_query(conn, \"\"\"\n",
    "                        SELECT Content FROM TBLContent\n",
    "                        WHERE idurl = %s AND \"order\" = ANY(%s)\n",
    "                        ORDER BY \"order\" ASC;\n",
    "                    \"\"\", params=(page_id, list(orders_with_neighbors)), fetch=True)\n",
    "                    \n",
    "                    if expanded_chunk_rows:\n",
    "                        detailed_context = \"\\n\\n\".join([row[0] for row in expanded_chunk_rows])\n",
    "                        \n",
    "                        # Combine summary and detailed context for this page\n",
    "                        page_context = f\"Source Page Summary:\\n{page_summary}\\n\\nDetailed Information from this page:\\n{detailed_context}\"\n",
    "                        final_context_list.append(page_context)\n",
    "\n",
    "        if not final_context_list:\n",
    "            print(\"Could not build any context. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nüìù Final Combined Context for LLM:\")\n",
    "            for idx, ctx in enumerate(final_context_list):\n",
    "                print(f\"--- Context Block {idx+1} ---\\n{ctx[:10000]}...\\n\")\n",
    "\n",
    "        # 3. Build the RAG prompt with the new, multi-source context\n",
    "        rag_prompt = build_rag_prompt(question, final_context_list)\n",
    "            \n",
    "        # 4. Get the final answer from the LLM\n",
    "        final_answer = get_rag_answer(rag_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"‚úÖ Final Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            # Get breadcrumbs for all source pages used in the context\n",
    "            source_breadcrumbs = []\n",
    "            for url_id in source_ids:\n",
    "                breadcrumb = get_breadcrumb(conn, url_id)\n",
    "                if breadcrumb:\n",
    "                    source_breadcrumbs.append(breadcrumb)\n",
    "\n",
    "            sources_list = []\n",
    "            if source_breadcrumbs:\n",
    "                print(\"üîó Sources:\")\n",
    "                unique_trails = list(dict.fromkeys(source_breadcrumbs))\n",
    "                sources_list = unique_trails\n",
    "                for trail in unique_trails:\n",
    "                    print(f\"- {trail}\")\n",
    "                print()\n",
    "\n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": sources_list\n",
    "            }\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "output_file = \"rag_results_by_chunk.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ All questions processed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3defe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_prompt(question):\n",
    "    \"\"\"\n",
    "    Builds a simple prompt asking the LLM to answer based on its knowledge of a website.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Based on the information available on the website http://docs.ciroh.org, please answer the following question.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_standard_gpt_answer(prompt):\n",
    "    \"\"\"\n",
    "    Calls the LLM with standard parameters to get a general knowledge-based answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            # No 'reasoning_effort' or 'verbosity' parameters are used here\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating standard GPT answer: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results_data = []\n",
    "\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "    \n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n‚ùì PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Build the standard prompt\n",
    "        standard_prompt = build_standard_prompt(question)\n",
    "            \n",
    "        # 2. Get the answer from the standard LLM call\n",
    "        final_answer = get_standard_gpt_answer(standard_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"‚úÖ Standard GPT-5 Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": []  # Empty list as no sources are retrieved\n",
    "            }\n",
    "            # Add the result to our main list\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "# After the loop is complete, save all results to a new JSON file\n",
    "output_filename = \"rag_results_standard_gpt.json\"\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ All questions processed. Benchmark results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIROH_AIBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
