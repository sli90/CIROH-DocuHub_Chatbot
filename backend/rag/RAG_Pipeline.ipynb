{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef4182e",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea60c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe904a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "load_dotenv()\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "BASE_URL = \"https://docs.ciroh.org\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc560df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databes connection\n",
    "conn = psycopg2.connect(\n",
    "    host=PG_HOST,\n",
    "    database=PG_DB,\n",
    "    user=PG_USER,\n",
    "    password=PG_PASS\n",
    ")\n",
    "\n",
    "def execute_query(conn, query, params=None, fetch=False):\n",
    "    cur = conn.cursor()\n",
    "    \"\"\"Execute a SQL query with optional parameters.\"\"\"\n",
    "    try:\n",
    "        if params:\n",
    "            cur.execute(query, params)\n",
    "        else:\n",
    "            cur.execute(query)\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "            return result\n",
    "        else:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54333dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fbc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadcrumb(conn, url_id):\n",
    "    \"\"\"\n",
    "    Generates the breadcrumb trail for a given idurl using a recursive query.\n",
    "\n",
    "    Args:\n",
    "        conn: The database connection object.\n",
    "        url_id: The idurl of the page for which to generate the trail.\n",
    "\n",
    "    Returns:\n",
    "        A string with the breadcrumb (e.g., \"Home > Products > NGIAB\"),\n",
    "        or None if an error occurs or the idurl is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    WITH RECURSIVE breadcrumb_path AS (\n",
    "        -- Anchor Member: Select the starting page\n",
    "        SELECT\n",
    "            idurl,\n",
    "            name,\n",
    "            idurlparent,\n",
    "            1 AS depth -- Initial depth level\n",
    "        FROM\n",
    "            tblurls\n",
    "        WHERE\n",
    "            idurl = %s\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Recursive Member: Join the table to find the parent\n",
    "        SELECT\n",
    "            u.idurl,\n",
    "            u.name,\n",
    "            u.idurlparent,\n",
    "            bp.depth + 1 -- Increment depth at each level\n",
    "        FROM\n",
    "            tblurls u\n",
    "        JOIN\n",
    "            breadcrumb_path bp ON u.idurl = bp.idurlparent\n",
    "    )\n",
    "    -- Select the final result, aggregating the names into a single string\n",
    "    SELECT\n",
    "        string_agg(name, ' > ' ORDER BY depth DESC) AS breadcrumb\n",
    "    FROM\n",
    "        breadcrumb_path;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = execute_query(conn, query, params=(url_id,), fetch=True)\n",
    "        \n",
    "        if result and result[0] and result[0][0]:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in get_breadcrumb for idurl {url_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fb74cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "'''\n",
    "questions = [\n",
    "    \"How can I get the meeting link to join the monthly CIROH office hours for AWS and cyberinfrastructure support?\",\n",
    "    \"What are the main differences between the Anvil and Derecho supercomputers?\",\n",
    "    \"What is the correct procedure for tagging an EC2 instance on AWS according to CIROH's convention?\",\n",
    "    \"I need to run a job on the Pantarhei cluster. What is the maximum duration and core count I can request for a single job without needing special permission?\",\n",
    "    \"I'm new to CIROH and want to start with hydrologic modeling. What is NextGen In A Box (NGIAB) and what are the main deployment options available?\"\n",
    "]\n",
    "'''\n",
    "questions = {\n",
    "    \"Getting Started & Navigation\": [\n",
    "        \"What is CIROH DocuHub and how can I use it?\",\n",
    "        \"How do I get started with CIROH services?\",\n",
    "        \"Where can I find the getting started guide?\",\n",
    "        \"How do I become a CIROH consortium member?\"\n",
    "    ],\n",
    "    \"Account & Access Requests\": [\n",
    "        \"How do I request access to CIROH infrastructure?\",\n",
    "        \"What are the requirements for using CIROH resources?\",\n",
    "        \"How do I request a new CIROH account?\",\n",
    "        \"How do I submit a request for new software installation?\",\n",
    "        \"How do I request access to JupyterHub?\",\n",
    "        \"How do I request GPU access for research?\",\n",
    "        \"How do I request AWS or Google Cloud access through CIROH?\",\n",
    "        \"How do I request access to the Pantarhei or Wukong HPC clusters?\",\n",
    "        \"How do I request access to the NWM BigQuery API?\"\n",
    "    ],\n",
    "    \"Documentation & Training\": [\n",
    "        \"Where can I find NextGen framework documentation?\",\n",
    "        \"Where can I find documentation for a specific CIROH tool?\",\n",
    "        \"Where can I access tutorials and training materials?\",\n",
    "        \"What educational resources are available for CIROH users?\"\n",
    "    ],\n",
    "    \"Contributing Content\": [\n",
    "        \"How do I contribute to CIROH DocuHub?\",\n",
    "        \"How do I add my project documentation?\",\n",
    "        \"Where do I submit tutorials or training materials?\",\n",
    "        \"How do I publish my research findings on DocuHub?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: JupyterHub\": [\n",
    "        \"How do I access CIROH JupyterHub?\",\n",
    "        \"What’s the difference between Production, Staging, and Workshop JupyterHub?\",\n",
    "        \"What software is pre-installed on JupyterHub?\",\n",
    "        \"How do I stop my JupyterHub server when I’m not using it?\",\n",
    "        \"How do I install custom software on JupyterHub?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: Cloud & HPC\": [\n",
    "        \"How do I request AWS cloud credits through CIROH?\",\n",
    "        \"What computing resources does CIROH offer?\",\n",
    "        \"How do I use the Pantarhei HPC cluster?\",\n",
    "        \"How do I access JetStream2?\",\n",
    "        \"How do I request compute resources for a workshop or training?\"\n",
    "    ],\n",
    "    \"Services & Infrastructure: Data Access\": [\n",
    "        \"How do I access the AORC dataset?\",\n",
    "        \"Where can I find forcing data for hydrological models?\",\n",
    "        \"How do I retrieve data for a specific region?\",\n",
    "        \"What data formats are supported by CIROH?\"\n",
    "    ],\n",
    "    \"NextGen Framework\": [\n",
    "        \"How do I get started with the NextGen framework?\",\n",
    "        \"What is NextGen In A Box (NGIAB)?\",\n",
    "        \"How do I run NextGen locally?\",\n",
    "        \"How do I configure NextGen for my basin?\",\n",
    "        \"How do I troubleshoot NextGen setup issues?\",\n",
    "        \"My NextGen model isn’t running—what should I check?\",\n",
    "        \"How do I resolve dependency conflicts in NextGen?\"\n",
    "    ],\n",
    "    \"Community & Collaboration\": [\n",
    "        \"How do I connect with other CIROH researchers?\",\n",
    "        \"How do I join CIROH office hours?\",\n",
    "        \"How do I provide feedback on CIROH services?\"\n",
    "    ],\n",
    "    \"Technical Support\": [\n",
    "        \"I can’t access my JupyterHub environment—what should I do?\",\n",
    "        \"My data processing job failed—how can I troubleshoot?\",\n",
    "        \"How do I optimize my computational resources?\",\n",
    "        \"I need help with processing large datasets—where do I start?\",\n",
    "        \"How do I report a bug or issue to CIROH IT support?\"\n",
    "    ],\n",
    "    \"Data & Workflow Best Practices\": [\n",
    "        \"What are the best practices for managing CIROH datasets?\",\n",
    "        \"How do I ensure reproducibility in my CIROH research?\",\n",
    "        \"What tools are available for data visualization in CIROH?\",\n",
    "        \"How do I convert between different hydrological data formats?\"\n",
    "    ],\n",
    "    \"Policies & Administration\": [\n",
    "        \"What are CIROH’s usage policies?\",\n",
    "        \"How much compute time am I allocated?\",\n",
    "        \"What are the CIROH data sharing policies?\",\n",
    "        \"Are there restrictions on using CIROH resources for commercial work?\"\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4b29167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, dimensions, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Call OpenAI to get an embedding for the given text.\"\"\"\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=model,\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac1a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(question_embedding, max_relevant_urls=3):\n",
    "    \"\"\"Queries the database to find the most relevant summaries.\"\"\"\n",
    "    # The query finds the top N most similar summaries using cosine similarity (<=>)\n",
    "    responses = execute_query(conn, \"\"\"\n",
    "        SELECT idurl, summary_data ->> 'summary_text' as summary\n",
    "        FROM tblurls\n",
    "        WHERE summary_data IS NOT NULL\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s\n",
    "    \"\"\", params=(question_embedding, max_relevant_urls), fetch=True)\n",
    "    \n",
    "    # Return the database rows\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a1b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, context_summaries):\n",
    "    \"\"\"Builds the prompt for the LLM to answer the question based on context.\"\"\"\n",
    "    \n",
    "    # Joins the retrieved summaries into a single block of text\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(context_summaries)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert AI assistant for the CIROH DocuHub. Your task is to answer the user's question based *only* on the provided context.\n",
    "\n",
    "If the context does not contain the answer, state that you cannot answer the question with the information given. Do not use any external knowledge.\n",
    "\n",
    "**CONTEXT:**\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "\n",
    "**QUESTION:**\n",
    "{question}\n",
    "\n",
    "**ANSWER:**\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final answer from the LLM\n",
    "def get_rag_answer(prompt):\n",
    "    \"\"\"Calls the LLM to generate the final answer.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\", \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            reasoning_effort=\"minimal\",\n",
    "            verbosity=\"low\"\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating RAG answer: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4795d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chunks(question_embedding, relevant_url_ids, max_relevant_chunks=5):\n",
    "    \"\"\"\n",
    "    Queries the TBLContent table to find the most relevant chunks\n",
    "    only within the pages identified in the first-level search.\n",
    "    \"\"\"\n",
    "    # The query searches for the top 5 most similar chunks, but only\n",
    "    # within the list of provided idurls.\n",
    "    responses = execute_query(conn, \"\"\"\n",
    "        SELECT idurl, \"order\", Content\n",
    "        FROM TBLContent\n",
    "        WHERE idurl = ANY(%s)\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "    \"\"\", params=(relevant_url_ids, question_embedding, max_relevant_chunks), fetch=True)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6685351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to control the RAG pipeline behavior\n",
    "max_relevant_urls = 5\n",
    "max_relevant_chunks = 10\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8534e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================\n",
      "TOPIC: Getting Started & Navigation\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What is CIROH DocuHub and how can I use it?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH DocuHub is CIROH’s central entry point for documentation, services, and community resources supporting hydrologic research and operations. It orients you to CIROH cyberinfrastructure (e.g., CIROH JupyterHub on Google Cloud, AWS, Google Cloud, Pantarhei HPC), product docs (NextGen, Snow model, Tethys, NextGen In A Box), policies, training, blogs, and collaboration tools (CIROH Research Portal).\n",
      "\n",
      "How to use it:\n",
      "- Discover and follow documentation for models, tools, data processing, and best practices.\n",
      "- Access computing resources via linked hubs and platforms.\n",
      "- Join office hours and explore tutorials, blogs, and training.\n",
      "- Contribute: click “Edit page” on docs.ciroh.org to propose changes via GitHub; submit blog posts and request product pages using the provided GitHub issue templates.\n",
      "- Get support via Slack, email, or the site’s contact page.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Products > CIROH Research Portal\n",
      "- Release Notes\n",
      "- DocuHub > Contributing to CIROH DocuHub > DocuHub technologies\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I get started with CIROH services?\n",
      "\n",
      "✅ Final Answer:\n",
      "Start at the CIROH DocuHub, which links to all services and guides. To use computing resources:\n",
      "- Cloud: Request a CIROH AWS subaccount via the Infrastructure Access page and GitHub issue forms (projects get $500/month across AWS and Google Cloud; request more if needed). You can also use the CIROH-2i2c JupyterHub on Google Cloud.\n",
      "- On‑prem HPC (Pantarhei): Complete two steps—your project PI submits the On‑Premises Infrastructure Request (GitHub template), then each user submits the Microsoft Forms On‑Premise Access Request (UA users only submit the latter; external users obtain VPN first).\n",
      "\n",
      "For orientation and help, watch the CIROH CyberInfrastructure YouTube overview and join the monthly Cyberinfrastructure and Community NextGen Office Hours (email ciroh-it-admin@ua.edu for the Teams link). Support: ciroh-it-admin@ua.edu and Slack.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- DocuHub\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where can I find the getting started guide?\n",
      "\n",
      "✅ Final Answer:\n",
      "See “Intro to NGIAB,” which centralizes entry points and how to get started, and the “Installation and Setup” tutorial for hands-on setup.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > Installation and Setup\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "- Products > NGIAB 101 Learning Module > Learner Profiles\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I become a CIROH consortium member?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer that with the information given. The provided context covers accessing CIROH computing services and office hours but does not explain how to become a consortium member.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- DocuHub\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Account & Access Requests\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I request access to CIROH infrastructure?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Have your PI/workshop lead open a GitHub On‑Premises/Infrastructure Request issue for your project using the CIROH templates (to specify resources for JupyterHub, cloud, or on‑prem HPC like Pantarhei/Wukong).\n",
      "- After PI approval, each individual requests their own access:\n",
      "  - For JupyterHub CPU/GPU or custom images, follow the instructions linked from Infrastructure Access.\n",
      "  - For on‑prem HPC (Pantarhei or Wukong), submit the Microsoft Forms On‑Premise Access Request. UA users submit only this form; non‑UA users must first obtain VPN access via the same workflow, then submit the form.\n",
      "- Default cloud budget is $500/month per project across AWS/Google; request overages via a GitHub issue template.\n",
      "- If you can’t access the forms or need help, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Infrastructure Access\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What are the requirements for using CIROH resources?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t determine the exact “requirements” from the provided excerpts. The context describes services, policies, and access processes (e.g., CIROH AWS subaccounts, cost policy, NOAA-aligned data/code sharing expectations), but it doesn’t list a specific set of requirements for using CIROH resources.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > External Resources\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I request a new CIROH account?\n",
      "\n",
      "✅ Final Answer:\n",
      "You must follow the “Obtaining an Account” process for the resource you need:\n",
      "\n",
      "- On‑premises clusters (Pantarhei or Wukong): The PI opens a GitHub On‑Premises Infrastructure Request for the project. Then each individual submits the CIROH On‑Premise Access Request (Microsoft Form). UA users submit only the On‑Premise Access Request; external users must obtain VPN access first via the same form workflow.\n",
      "\n",
      "- CIROH Cloud (AWS or Google Cloud): Request access through the Infrastructure Access workflow using the referenced GitHub issue forms. CIROH will create a project subaccount and PI identity; the project team manages user access.\n",
      "\n",
      "If you can’t access the forms, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > CIROH Subdomain Request Form\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I submit a request for new software installation?\n",
      "\n",
      "✅ Final Answer:\n",
      "Export your conda environment to an environment.yml, submit the JupyterHub (2i2c) Software Install Form selecting the CIROH 2i2c installation, then email the environment.yml to CIROH-IT support.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > CIROH Subdomain Request Form\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I request access to JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "For CIROH JupyterHub: Request access through CIROH’s Infrastructure Access process (then use the production/staging/workshop hubs as appropriate). For help, contact CIROH Cloud support via email or Slack.\n",
      "\n",
      "For CUAHSI JupyterHub: Join the CUAHSI JupyterHub Group via your HydroShare account, then launch from the HydroShare Apps library, the “Open with” menu, or directly at jupyterhub.cuahsi.org. For support, email help@cuahsi.org.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 6: How do I request GPU access for research?\n",
      "\n",
      "✅ Final Answer:\n",
      "- On-premises HPC (Pantarhei or Wukong): Have your PI submit a project-level On‑Premises Infrastructure Request via the GitHub issue template specifying GPU needs. Then each user submits the On‑Premise Access Request form. External (non‑UA) users must obtain VPN access first via the same workflow.\n",
      "- CIROH‑2i2c JupyterHub: Request individual CPU/GPU access (and custom images if needed) per the Infrastructure Access guidance.\n",
      "- For help or if forms aren’t accessible: ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Infrastructure Access\n",
      "- Services > External Resources\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 7: How do I request AWS or Google Cloud access through CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "Request access via the CIROH Infrastructure Access workflow. CIROH creates AWS and Google Cloud subaccounts and PI identities; use the referenced GitHub issue templates/forms to submit your project request. For help, contact ciroh-it-admin@ua.edu or the CIROH IT Slack channel.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 8: How do I request access to the Pantarhei or Wukong HPC clusters?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Pantarhei: Submit a project-level On-Premises Infrastructure Request (via the GitHub issue template, ideally by the PI), then each user submits the Microsoft Forms On-Premise Access Request. UA users only need the On-Premise Access Request; external users must obtain VPN access first, then submit the access request. If you can’t access the forms, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "- Wukong: The PI opens a GitHub On‑premises Infrastructure Request issue with project details (PI identity, roster with GitHub usernames/emails, resource needs). Each team member must also submit the CIROH On‑Premise Access Form. Non‑UA users without UA credentials must first request VPN access via the same form workflow. Accounts are provisioned per user.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Pantarhei\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 9: How do I request access to the NWM BigQuery API?\n",
      "\n",
      "✅ Final Answer:\n",
      "Submit the NWM BigQuery API Access Form. After approval, you’ll receive an API key to use with https://nwm-api.ciroh.org.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Data Management and Access Tools > NWM BigQuery API\n",
      "- Blog > Accessing National Water Model (NWM) Data via Google Cloud BigQuery API\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Products > Data Management and Access Tools > Data Access > nwmurl\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Documentation & Training\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: Where can I find NextGen framework documentation?\n",
      "\n",
      "✅ Final Answer:\n",
      "The NextGen framework documentation is provided in the NextGen Technical Guidance page, which links to the full guidance PDF and serves as the authoritative integration and design reference.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > NextGen Framework\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: Where can I find documentation for a specific CIROH tool?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the CIROH DocuHub. It’s the central entry point that links to product documentation pages (e.g., NextGen, Snow model, Tethys, NextGen In A Box) and domain pages like Snow Sensing. You can also find tool pages via the CIROH Research Portal (portal.ciroh.org) which catalogs apps and links back to documentation.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub\n",
      "- Policies\n",
      "- Products > Snow Sensing and Modeling Tools > Intro to Snow Observations Modeling Analysis\n",
      "- Products > CIROH Research Portal\n",
      "- Products > Snow Sensing and Modeling Tools > Snow Sensing\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where can I access tutorials and training materials?\n",
      "\n",
      "✅ Final Answer:\n",
      "- NGIAB 101 training module (episodes from setup to advanced topics), with links to GitHub source, CIROH Docs, quick-starts, Key Points, and Glossary\n",
      "- Intro to NGIAB hub (orientation, setup, local installation guidance, glossary, run config structure)\n",
      "- Learner Profiles page (routes by audience: general, HPC, DevCon 2025; links to Advanced Topics, Key Points, Glossary)\n",
      "- Getting Started with NETWA tutorial (CIROH testbed access, shared storage, Python environments)\n",
      "- Instructor Notes (facilitation guidance and links for the DevCon 2025 workshop)\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NGIAB 101 Learning Module > Learner Profiles\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "- Products > NGIAB 101 Learning Module\n",
      "- Products > NGIAB 101 Learning Module > Instructor - Summary and Schedule > Instructor Notes\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: What educational resources are available for CIROH users?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Educational courses via the CIROH Research Portal (portal.ciroh.org), including HydroLearn integrations\n",
      "- Product documentation (e.g., NextGen, Snow model, Tethys, NextGen In A Box) on the CIROH DocuHub\n",
      "- Training materials and tutorials linked from the DocuHub\n",
      "- Monthly NextGen-focused office hours\n",
      "- Blogs and community contributions (tutorials, datasets, presentations) via the DocuHub and Portal\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > CIROH Research Portal\n",
      "- DocuHub\n",
      "- Services\n",
      "- Services > External Resources\n",
      "- Policies\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Contributing Content\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I contribute to CIROH DocuHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can contribute in several ways:\n",
      "\n",
      "- Simple doc edits: Click “Edit page” on docs.ciroh.org, make changes, and submit a pull request to the GitHub repo for admin review.\n",
      "- Major edits: Fork CIROH-UA/ciroh-ua_website, make commits, open a pull request; GitHub Actions will build and publish. For local dev: install Node.js, run npm install, then npm run start (dev) or npm run build (prod).\n",
      "- Blog posts: Must highlight projects using CIROH cyberinfrastructure. Submit via the blog post GitHub issue template for CIROH tech team review.\n",
      "- Product pages: Ensure you have a public GitHub project with a clear README, then request via the dedicated product issue template.\n",
      "- Developer resources: Use the repository workflows, site technologies, and blog authoring guides linked from DocuHub.\n",
      "- Support: Reach out via Slack, email ciroh-it-admin@ua.edu, or the site’s contact page.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > DocuHub technologies\n",
      "- Release Notes\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I add my project documentation?\n",
      "\n",
      "✅ Final Answer:\n",
      "- For simple fixes: click “Edit page” on docs.ciroh.org, make changes, and submit the GitHub pull request.\n",
      "- For larger additions: fork CIROH-UA/ciroh-ua_website, add your Markdown docs, commit, open a pull request; GitHub Actions will build and publish. You can test locally with Node.js (npm install; npm run start/build).\n",
      "- If you want a Product page for your project: ensure it’s on a public GitHub repo with a clear README, then submit a request via the Product issue template.\n",
      "- Need help: contact ciroh-it-admin@ua.edu or use Slack/contact page.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub > DocuHub technologies\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "- Release Notes\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where do I submit tutorials or training materials?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the “Edit page” link on docs.ciroh.org for simple tutorial fixes, or fork the CIROH-UA/ciroh-ua_website repo for larger additions and open a pull request. For help, contact ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Products > NGIAB 101 Learning Module > Advanced Topics\n",
      "- Products > NGIAB 101 Learning Module > Learner Profiles\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I publish my research findings on DocuHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can publish via a blog post or by contributing docs/product pages:\n",
      "\n",
      "- Blog post: Must highlight work leveraging CIROH cyberinfrastructure. Submit via the blog post GitHub issue template for review by CIROH’s tech team. Alternatively, add a Markdown post in /blog (date-based filename, YAML front matter, include <!-- truncate -->) and open a pull request.\n",
      "- Documentation/tutorials: For simple fixes, use the “Edit page” link on docs.ciroh.org and submit a PR. For major additions, fork CIROH-UA/ciroh-ua_website, commit changes, open a PR; GitHub Actions will build/publish.\n",
      "- Product page: Ensure your project has a public GitHub repo with a clear README, then request a page via the product issue template.\n",
      "\n",
      "For help, contact ciroh-it-admin@ua.edu or use Slack/contact page.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > DocuHub technologies\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: JupyterHub\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I access CIROH JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "Access is granted through CIROH’s Infrastructure Access process. Once you have access, you can choose from the available production, staging, or workshop hubs and start a server (Small–Huge) in the provided Jupyter environment. For details and the access link, see the CIROH JupyterHub page; support: CIROH Cloud email and Slack.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "- Services > Public Cloud > HydroShare and CIROH JupyterHub Integration\n",
      "- Services\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What’s the difference between Production, Staging, and Workshop JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Production: Main, persistent CIROH JupyterHub for day-to-day research use.\n",
      "- Staging: Test/preview hub for trying changes before they reach production.\n",
      "- Workshop: Time-limited hub provisioned for events/training sessions.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > JupyterHub User Directory\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "- Services > Public Cloud > HydroShare and CIROH JupyterHub Integration\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What software is pre-installed on JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "The provided information does not list specific pre-installed software. It notes that CIROH JupyterHub links to the deployed software Dockerfile and a request form for additional packages, and that the 2i2c docs link to the awi-ciroh-image GitHub repository for hub image code. Without those linked details, the exact pre-installed software cannot be determined from the given context.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > JupyterHub User Directory\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I stop my JupyterHub server when I’m not using it?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the JupyterHub controls to stop your server when idle. In the CIROH 2i2c environment, servers should be stopped when not in use (there’s also an auto-stop after ~1 hour of inactivity). From your JupyterHub interface, choose to shut down/stop your server to free resources. If you’ve enabled a keepalive method for long jobs, disable it before stopping.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Prevent Server Timeout\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > JupyterHub User Directory\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I install custom software on JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the CIROH custom image request process:\n",
      "- Export your conda environment to an environment.yml.\n",
      "- Submit the JupyterHub (2i2c) Software Install Form and select the CIROH 2i2c installation.\n",
      "- Email the environment.yml to CIROH-IT support.\n",
      "\n",
      "For additional packages on existing hubs, use the linked request form; for persistent user-level Conda environments, follow the CIROH 2i2c how-to guides. Support: ciroh-it-admin@ua.edu or CIROH Cloud Slack.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Push and Pull to GitHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: Cloud & HPC\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I request AWS cloud credits through CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the CIROH Infrastructure Access workflow and the referenced GitHub issue templates. Each project has a default $500/month budget shared across AWS and Google Cloud; if you need more, submit a request for additional funds via the GitHub issue form linked from the CIROH AWS Account page. For help, contact the CIROH IT admin email or Slack channel.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > Public Cloud > CIROH AWS Account > CIROH AWS Office Hours\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- Services > Public Cloud > CIROH AWS Account > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What computing resources does CIROH offer?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH offers:\n",
      "- Multi‑cloud resources: AWS and Google Cloud (Compute Engine VMs, Cloud Storage, BigQuery, Google Earth Engine), with managed JupyterHub (2i2c) and GPU allocations by request.\n",
      "- On‑premises HPC: Pantarhei and Wukong clusters operated at the University of Alabama.\n",
      "- Data services: Google BigQuery API for National Water Model data.\n",
      "- DevOps/JupyterHub environments: preconfigured, scalable compute/storage with hydrologic software pre‑installed.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- Services > External Resources\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I use the Pantarhei HPC cluster?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Ensure you have a Pantarhei account and are compliant with UA acceptable use, information classification, and protection policies.\n",
      "- Connect to the University of Alabama network via the campus VPN.\n",
      "- Open a terminal (macOS/Linux/Windows) and SSH to pantarhei.ua.edu using your Pantarhei username and password. You can also set up SSH public keys as directed.\n",
      "- Do not run intensive work on the login node. Submit compute work as Slurm jobs to the appropriate queue (CPU, GPU, or FPGA).\n",
      "- Use shared-node jobs when suitable to improve throughput and utilization.\n",
      "- Job limits: up to 7 days on as many as six full nodes; standard max job size is 240 cores. Larger requests require a support consultation.\n",
      "- Refer to the Running Jobs and Accessing the Compute Nodes guidance for command-line submission and queue options.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "- Services > On-Premises > Pantarhei\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Services > On-Premises > Pantarhei > System Architecture\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I access JetStream2?\n",
      "\n",
      "✅ Final Answer:\n",
      "Jetstream2 is available to CIROH members and partners via NSF ACCESS. Request an allocation through CIROH’s Infrastructure Access process, which links to the NSF ACCESS portals and the official Jetstream2 documentation for setup and technical details.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > External Resources > NSF Access > JetStream2\n",
      "- Services > Infrastructure Access\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "- Services > External Resources > NSF Access\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I request compute resources for a workshop or training?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Have the PI or workshop lead open a GitHub On‑Premises/Infrastructure Request for the project, specifying the workshop needs (platforms, resources, roster).\n",
      "- Use CIROH’s Infrastructure Access process to select platforms (e.g., CIROH‑2i2c JupyterHub for CPU/GPU, AWS/Google Cloud, JetStream2, on‑prem HPC like Pantarhei/Wukong).\n",
      "- For JupyterHub: attendees can request CPU/GPU access; submit custom images via the JupyterHub (2i2c) Software Install Form with an environment.yml if needed.\n",
      "- CIROH offers conference/workshop IT support—use the provided forms/contacts on the Infrastructure Access page to coordinate setup.\n",
      "- Budget: projects get $500/month in cloud credits by default; request overages if your workshop requires more.\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > DevCon 2025: A DevOps and Cyberinfrastructure Success Story\n",
      "- Services > Infrastructure Access\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: Data Access\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I access the AORC dataset?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can access AORC via:\n",
      "- HydroShare/CIROH 2i2c JupyterHub: Use the “AORC Data in Your Hands” Jupyter notebooks (requires a HydroShare account and CIROH-2i2c JupyterHub authorization via GitHub). The notebooks retrieve AORC precipitation/meteorological data from AWS in lat–lon and NWM projections, for point and polygon workflows.\n",
      "- NWM Data Access: AORC v1.0 (and forthcoming v1.1) is referenced as forcing; use the Data Access guide to reach NOAA Open Data Dissemination (NODD) cloud endpoints (AWS, Google Cloud, Azure) and CIROH Kerchunk catalogs.\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "- Services > External Resources > NSF Access\n",
      "- Services > External Resources\n",
      "- Services > Public Cloud > CIROH AWS Account > Documentation and Tutorial > AWS Data Science Tools\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: Where can I find forcing data for hydrological models?\n",
      "\n",
      "✅ Final Answer:\n",
      "- CIROH DataStream portal: Browse daily NextGen forcings and outputs at datastream.ciroh.org (AWS S3-backed).\n",
      "- For NWM-based forcings usable with NextGen: Use the ngen-datastream forcing processor with nwmurl-supported endpoints across NOAA NOMADS (HTTPS), Google Cloud Storage (GCS/GS), and AWS S3 for NWM operational v3 and retrospective v2.1/v3.0 datasets.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream > Datastream Components > Forcing Processor > Forcing Sources\n",
      "- Products > Research Datastream\n",
      "- Products > Research Datastream > Datastream Components > Datastream Configuration\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > Research Datastream > Datastream Components > Forcing Processor\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I retrieve data for a specific region?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the “AORC Data in Your Hands” Jupyter notebooks on HydroShare/CIROH 2i2c. They support spatial subsetting for both point and polygon (shapefile) workflows in lat–lon and NWM projections, letting you retrieve AORC/NWM data for your specified region. Access requires a HydroShare account and CIROH-2i2c JupyterHub authorization via GitHub.\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "- Blog > Accessing National Water Model (NWM) Data via Google Cloud BigQuery API\n",
      "- Products > Research Datastream\n",
      "- Products > Data Management and Access Tools > NETWA\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: What data formats are supported by CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "I cannot answer with the information given. The provided context does not specify supported data formats.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Products > CIROH Research Portal\n",
      "- Policies > Data and Code Sharing\n",
      "- Products\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: NextGen Framework\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I get started with the NextGen framework?\n",
      "\n",
      "✅ Final Answer:\n",
      "Start with the NGIAB 101 training module and the “Intro to NGIAB” hub. Install NGIAB via Docker (Singularity on HPC), then use the provided installation guidance and run configuration directory docs to set up a standard ngen-run. The framework page offers background on BMI-based modular modeling and links to a CIROH webinar for deeper understanding. If you need technical integration details, consult the NextGen Technical Guidance PDF.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "- Policies > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What is NextGen In A Box (NGIAB)?\n",
      "\n",
      "✅ Final Answer:\n",
      "NextGen In A Box (NGIAB) is an open-source, containerized distribution of the Next Generation Water Resources Modeling Framework (NextGen) that packages BMI-compliant models, hydrofabric data, and the ngen engine to enable rapid, portable, and reproducible hydrologic modeling. It standardizes OS, dependencies, and runtime via Docker/Singularity, integrates tools for data preprocessing, model execution, calibration, evaluation (e.g., KGE, NSE, bias), and visualization, and supports workflows from local to cloud/HPC, including NWM configurations and community models.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NGIAB 101 Learning Module > Introduction\n",
      "- Products > NextGen In A Box > Intro to NGIAB > NWM, NextGen, and NGIAB\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NextGen In A Box\n",
      "- Blog > NextGen In A Box Updates\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I run NextGen locally?\n",
      "\n",
      "✅ Final Answer:\n",
      "To run NextGen locally with NGIAB:\n",
      "\n",
      "1) Install prerequisites\n",
      "- Install Docker Desktop (Windows via WSL2, macOS, or Linux). Verify with: docker ps -a\n",
      "- On Windows, avoid using WSL “Docker-linked” distros for your project files.\n",
      "\n",
      "2) Set up a project and sample data\n",
      "- Create a working directory.\n",
      "- Download and extract the Sipsey Fork sample input data into it.\n",
      "\n",
      "3) Get NGIAB-CloudInfra\n",
      "- Clone the NGIAB-CloudInfra repository into your project directory.\n",
      "\n",
      "4) Validate and run via the interactive guide\n",
      "- From the NGIAB-CloudInfra folder, run: ./guide.sh\n",
      "- Follow the prompts to validate your local setup and execute a NextGen run.\n",
      "- If you need custom inputs, use the Data Preprocess tool (NGIAB-datapreprocessor/ngen-datastream) to build an ngen-run directory, then run via guide.sh.\n",
      "\n",
      "Troubleshooting: See the NGIAB 101 module for Git line-ending issues (LF vs CRLF) and broader usage.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > DevCon 2025 Jetstream VM Instructions\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I configure NextGen for my basin?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use NGIAB’s guided workflow:\n",
      "- Install NGIAB via Docker (or Singularity on HPC) and run guide.sh.\n",
      "- Prepare inputs with the Data Preprocess tool: subset the hydrofabric for your basin, generate meteorological forcings, and create a realization file in a standard ngen-run directory.\n",
      "- Execute the NextGen run using the generated realization and inputs (either directly after preprocessing or from the ngen-run directory).\n",
      "- Optionally calibrate with ngiab-cal (YAML-driven) and evaluate performance with TEEHR; visualize results with the Tethys-based Data Visualizer.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NGIAB 101 Learning Module > Introduction\n",
      "- Products > NextGen In A Box > Intro to NGIAB > NWM, NextGen, and NGIAB\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I troubleshoot NextGen setup issues?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t provide detailed troubleshooting steps beyond what’s in the provided info. The context suggests:\n",
      "- Use the NGIAB 101 training module for onboarding and common issues (e.g., Git line endings LF vs CRLF on Windows/WSL).\n",
      "- Verify Docker is installed and running (e.g., docker ps -a) and avoid WSL Docker-linked distros as defaults.\n",
      "- Run NGIAB-CloudInfra’s guide.sh to validate your setup.\n",
      "- For help: open GitHub issues for bugs/feature requests, attend CIROH monthly office hours, or reach out via email/CIROH Slack.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Contact Us\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > DevCon 2025 Jetstream VM Instructions\n",
      "- Products > NGIAB 101 Learning Module > Installation and Setup\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 6: My NextGen model isn’t running—what should I check?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Verify your run directory matches the required NextGen Run Directories structure: ngen-run root with config, forcings, outputs, and metadata; optional lakeout and restart.\n",
      "- Confirm the config folder contains your realization file, BMI configs, a single hydrofabric geopackage, and any needed per-catchment configs in cat-config (e.g., PET, CFE, NOAH-OWP-M).\n",
      "- Check that forcings exist for all modeled catchments and are correctly referenced by the realization/configs.\n",
      "- Ensure module sequence and BMI component references in the realization file are valid and point to available modules.\n",
      "- If using NGIAB, rerun the Data Preprocess tool to auto-build the run directory and inputs, and use guide.sh to configure/execute.\n",
      "- Confirm container/runtime environment (Docker or Singularity on HPC) is set up and that paths in configs match the container’s filesystem.\n",
      "- Review autogenerated metadata and outputs folders for error logs or missing files indicating misconfiguration.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "- Policies > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 7: How do I resolve dependency conflicts in NextGen?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer that with the information given. The provided context doesn’t include guidance on resolving dependency conflicts in NextGen.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Policies > NextGen Framework\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Blog > NextGen Monthly News Update - January 2024\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Community & Collaboration\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I connect with other CIROH researchers?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Join the monthly Cyberinfrastructure and Community NextGen Office Hours (email ciroh-it-admin@ua.edu for the Teams link and calendar invite).\n",
      "- Use the CIROH Research Portal (portal.ciroh.org) to engage with apps, datasets, courses, and contribute your own materials.\n",
      "- Explore CIROH DocuHub links to collaboration tools, training, blogs, and contribution pathways to share documentation, tutorials, datasets, and presentations.\n",
      "- Check CIROH Science Meeting 2024 and AGU 2024 posts for shared slides, photos, and GitHub repositories of presentations to find collaborators and contact points.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- Products > CIROH Research Portal\n",
      "- Blog > CIROH Science Meeting 2024\n",
      "- Blog > CIROH at AGU 2024\n",
      "- DocuHub\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I join CIROH office hours?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Cyberinfrastructure and Community NextGen Office Hours: Email ciroh-it-admin@ua.edu to request the Microsoft Teams meeting link and calendar invite.\n",
      "- AWS Office Hours: Use the contact email provided on the AWS Office Hours page to subscribe and receive session details.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- Services > Public Cloud > CIROH AWS Account > CIROH AWS Office Hours\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- DocuHub\n",
      "- Services\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I provide feedback on CIROH services?\n",
      "\n",
      "✅ Final Answer:\n",
      "Email ciroh-it-admin@ua.edu to connect with the team (as used for Office Hours) and use the DocuHub contact page for support/feedback.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Services\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- DocuHub\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Technical Support\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: I can’t access my JupyterHub environment—what should I do?\n",
      "\n",
      "✅ Final Answer:\n",
      "- For CUAHSI JupyterHub: Make sure you’ve joined the CUAHSI JupyterHub Group via HydroShare. Try launching from the HydroShare Apps library, the HydroShare “Open with” menu, or directly at jupyterhub.cuahsi.org and select a profile. If issues persist, contact help@cuahsi.org.\n",
      "\n",
      "- For CIROH JupyterHub (2i2c): Verify you have CIROH Infrastructure Access and are using the correct hub (production/staging/workshop). Ensure your server isn’t just stopped; start it, and stop idle servers to avoid quota issues. If you still can’t access, contact CIROH Cloud support via email or Slack, or ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > JupyterHub User Directory\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: My data processing job failed—how can I troubleshoot?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t diagnose specifics from the info given, but here are steps based on CIROH/Pantarhei guidance:\n",
      "\n",
      "- Don’t run heavy work on login nodes; submit via Slurm. Check your job script and queue/partition selection (CPU/GPU/FPGA), walltime, and core counts within limits (shared-node or up to 240 cores standard; longer runs up to 7 days, ≤6 nodes).\n",
      "- Inspect Slurm outputs: .out/.err files and Slurm job status for errors (OOM, walltime exceeded, node preemption).\n",
      "- If running ngen-datastream:\n",
      "  - Reduce memory pressure: fewer catchments, shorter duration, or fewer variables; or request more RAM/cores.\n",
      "  - Control processes: default is nprocs_host − 2; throttle with -n to fit available memory. Rule of thumb RAM per process: ~1 GB (days–weeks), ~4 GB (months–year).\n",
      "  - Check that forcing.tar.gz and ngen-bmi-configs.tar.gz are valid if you replaced defaults.\n",
      "  - Expect runtime scaling: ~1 minute per 10 hourly steps; adjust walltime accordingly.\n",
      "- If using NGIAB:\n",
      "  - Re-run via guide.sh and confirm input paths and parallel/serial mode selections; or use Data Preprocess CLI with -a and review its logs.\n",
      "- For persistent failures or larger resource needs (>240 cores or special nodes), contact support for consultation.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Products > NGIAB 101 Learning Module > Model Execution\n",
      "- Products > Research Datastream\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I optimize my computational resources?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Submit intensive work as Slurm jobs, not on login nodes; choose appropriate queues (CPU/GPU/FPGA), shared-node options for small jobs, and request longer walltimes only as needed (up to 7 days; larger-than-standard >240 cores require support).\n",
      "- For ngen-datastream:\n",
      "  - Match processes to memory: default is nprocs_host − 2; throttle with -n to avoid OOM. Plan ~1 GB RAM per process for days–weeks runs, ~4 GB per process for months–year.\n",
      "  - Scale domain/time: memory grows with catchment count, timesteps, and variables; reduce any of these if nearing limits.\n",
      "  - Use performance-aware BMI modules (CFE, PET, SLOTH, NOM) based on needs; they affect runtime.\n",
      "  - Expect ~1 minute per 10 hourly steps as a rule of thumb; benchmark and adjust processes, duration, or domain.\n",
      "  - Example reference: ~11k catchments, 24 hourly steps ran ~5 minutes on 8 processes with 32 GB RAM (t4g.2xlarge).\n",
      "- Use shared-node jobs to improve throughput and cluster utilization when full nodes aren’t needed.\n",
      "- For NGIAB on HPC, run within Singularity on compute nodes via SLURM, not as root; leverage native scheduler integration and host filesystem access; keep outputs in a dedicated directory.\n",
      "- If CIROH resources are constrained, request supplemental HPC via NSF ACCESS to expand capacity.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Services > External Resources\n",
      "- Products > NGIAB 101 Learning Module > Advanced Topics\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: I need help with processing large datasets—where do I start?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can start with CIROH’s available infrastructure and workflows:\n",
      "\n",
      "- On the CIROH testbed (NETWA): use remote desktop or SSH to ciroh-testbed.uvm.edu, store large downloads under /netfiles/ciroh/downloadedData/, and manage Python environments with Mamba/Conda (shared envs in /data/condaShared/envs). Forecast and data tools (e.g., nwm_fc.py, gfs_fc_thredds.py) are available to fetch large NWM/GFS datasets efficiently without duplication.\n",
      "\n",
      "- For cloud-scale hydrologic workflows: use Research DataStream’s open, AWS-based pipeline to automate inputs, run NextGen via DataStreamCLI, and access outputs on S3 (browse at datastream.ciroh.org). You can deploy reference AWS infrastructure and run multi-process NextGen jobs with community configurations.\n",
      "\n",
      "- For heavy compute/ML modeling: leverage CIROH’s Wukong HPC cluster integrated with Amazon S3 for scalable, multi-node runs and durable, shareable storage.\n",
      "\n",
      "- If running NextGen locally or in AWS, follow ngen-datastream usage guidance: match memory/processes to domain and duration (rule-of-thumb RAM per process ≈1 GB for days–weeks, ≈4 GB for months–year), throttle processes with -n, and consider model module choices (CFE, PET, SLOTH, NOM) for performance. Example: ~11k catchments, 24 hourly steps ran in ~5 minutes on t4g.2xlarge with 8 procs and 32 GB RAM.\n",
      "\n",
      "Choose the path that fits your need: testbed + shared data for quick starts, DataStream for cloud automation and S3 access, or Wukong+S3 for large-scale HPC.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Products > Research Datastream\n",
      "- Products > Research Datastream > Datastream Components > Datastream Configuration\n",
      "- Blog > Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I report a bug or issue to CIROH IT support?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the GitHub issues tracker for the relevant CIROH project (e.g., NGIAB) to report bugs or feature requests. For direct support, you can also reach CIROH IT via email or CIROH Slack.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Contact Us\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > CIROH Subdomain Request Form\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Data & Workflow Best Practices\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What are the best practices for managing CIROH datasets?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Use HydroShare for small, citable, API-enabled, or spatial datasets; assign a DOI and apply a Creative Commons license.\n",
      "- Use CIROH Cloud object storage (AWS S3, Google Cloud Buckets, Azure, or on‑prem) for large or NWM‑linked datasets; optionally cross‑link records from HydroShare.\n",
      "- Ensure machine-readable, openly documented access at no or minimal cost; include described quality control methods.\n",
      "- Make data publicly available by article publication or within two years of collection or grant end.\n",
      "- Include required NOAA disclaimer, proper data citation with DOIs, and funding acknowledgments (including FundRef).\n",
      "- Align with NOAA Information Quality Guidelines and CIROH’s Data Management Plan; submit final accepted manuscripts to the NOAA Institutional Repository (one-year embargo).\n",
      "- Leverage the Water Prediction Node when appropriate and list datasets in the CIROH Research Portal for discoverability.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Products > CIROH Research Portal\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I ensure reproducibility in my CIROH research?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Use CIROH CyberInfrastructure: run work in the managed JupyterHub with preconfigured environments, scalable compute/storage, and pre-installed hydrologic software to reduce setup variability and enable reproducible NextGen experiments.\n",
      "- Share data and code per CIROH’s Data and Code Sharing Policy: publish data in HydroShare (small) with DOIs and CC licenses; use CIROH Cloud object storage for large/NWM-linked datasets and cross-link from HydroShare; host code on GitHub (CIROH or institutional) with BSD-3-Clause or MIT licenses and mint DOIs via Zenodo; optionally publish reproducible notebooks in HydroShare and launch via CIROH or CUAHSI JupyterHub.\n",
      "- For models and workflows: share source on GitHub, containers via NextGen In A Box, packages/instances in HydroShare, executables through the CIROH Portal; document workflows with GitHub gists, DocuHub pages, or notebooks.\n",
      "- Follow policy requirements: provide machine-readable, openly documented data with described QC, include NOAA disclaimer, cite data with DOIs, acknowledge funding (including FundRef), and make outputs public by publication or within two years of collection or grant end. Submit final accepted manuscripts to the NOAA Institutional Repository (up to one-year embargo).\n",
      "- Use CIROH recommendations and support: align with the living guidance for consistent data management; leverage HydroShare Help, CUAHSI support, CIROH Cloud team, and Slack channels.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing\n",
      "- Services\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What tools are available for data visualization in CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Visualization and Analysis Tools index with links to Tethys-CSES (Community Streamflow Evaluation System)\n",
      "- CIROH Research Portal native apps: TethysDash, Water Data Explorer, SWEML, GRACE Groundwater Tool, Snow Inspector, CSES (for NWM evaluation)\n",
      "- Proxied/external tools via the portal: FIM Visualization Deck, OWP NWM Map Viewer, CIROH JupyterHub, HydroShare, NFFA APP\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Visualization and Analysis Tools\n",
      "- Products > CIROH Research Portal\n",
      "- Products > Snow Sensing and Modeling Tools > Intro to Snow Observations Modeling Analysis\n",
      "- Services\n",
      "- Community Impact\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I convert between different hydrological data formats?\n",
      "\n",
      "✅ Final Answer:\n",
      "I cannot answer with the information given. The context lists common formats used (GeoPackage, JSON, NetCDF-4, YAML, tar) and related tools (Hydrofabric R package, NGIAB/NextGen utilities), but it does not provide conversion procedures or tools for converting between formats.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Glossary\n",
      "- Products > Hydrofabric\n",
      "- Products > NextGen In A Box > Components > NGIAB Data Preprocess\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Policies & Administration\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What are CIROH’s usage policies?\n",
      "\n",
      "✅ Final Answer:\n",
      "I cannot answer with the information given. The provided context does not define “usage policies” beyond general data/code sharing and cloud account guidance.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How much compute time am I allocated?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t determine your exact compute time allocation from the provided information. The context describes queues, job size/walltime limits (up to seven days per job and up to six full nodes on Pantarhei), default cloud budget ($500/month across AWS and Google Cloud), and NSF ACCESS allocations, but it doesn’t state a total compute time allocation for your account.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Wukong > System Architecture\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Services > External Resources > NSF Access\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What are the CIROH data sharing policies?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH’s data sharing policies commit to open, machine‑readable, and well‑documented sharing of environmental data and research products in alignment with NOAA’s Data Sharing Directive (v3.0). Key requirements:\n",
      "- Make data publicly available at no or minimal cost, with described quality control methods.\n",
      "- Release by journal article publication or within two years of collection or grant end, whichever comes first.\n",
      "- Include a specific NOAA disclaimer, proper data citation with DOIs, and funding acknowledgments (including FundRef).\n",
      "- Submit final accepted manuscripts to the NOAA Institutional Repository with up to a one‑year embargo.\n",
      "- Follow NOAA Information Quality Guidelines and NAO 212-15 definitions.\n",
      "- Use CIROH’s Data Management Plan as the implementation appendix.\n",
      "\n",
      "Guidance for where to share:\n",
      "- Data: HydroShare for small, citable, API-enabled/spatial datasets with DOIs and CC licenses; CIROH Cloud object storage for large or NWM-linked datasets (optionally cross-link from HydroShare); leverage Water Prediction Node as needed.\n",
      "- Code: Host on GitHub (institutional or CIROH org) with BSD-3-Clause or MIT licenses; use CIROH/OWP templates; archive DOIs via Zenodo; Jupyter notebooks can be published in HydroShare and launched via CIROH/CUAHSI JupyterHub.\n",
      "- Models/workflows/publications/education: Share via GitHub, HydroShare, CIROH Portal, and HydroLearn per recommendations; avoid preprints before peer review per NOAA.\n",
      "\n",
      "These policies are a living document to ensure consistent data management, transparency, and broad reuse across CIROH collaborations.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Products > CIROH Research Portal\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: Are there restrictions on using CIROH resources for commercial work?\n",
      "\n",
      "✅ Final Answer:\n",
      "I cannot answer with the information given. The provided context does not mention restrictions on using CIROH resources for commercial work.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > External Resources\n",
      "\n",
      "\n",
      "✅ All questions processed. Results saved to rag_results_by_url.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_data = []\n",
    "\n",
    "# Loop for RAG Pipeline (searching in TBLURLs only) with new dictionary structure\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "    \n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n❓ PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Get the embedding for the current question\n",
    "        question_embedding = get_embedding(question, dimensions=1792)\n",
    "            \n",
    "        if question_embedding is None:\n",
    "            print(\"Could not generate embedding for the question. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # 2. Query the database to get relevant context\n",
    "        retrieved_context = query_embedding(question_embedding, max_relevant_urls=max_relevant_urls)\n",
    "            \n",
    "        if not retrieved_context:\n",
    "            print(\"No relevant context found in the database. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # Extract just the summary texts for the prompt\n",
    "        context_ids = [row[0] for row in retrieved_context]\n",
    "        context_summaries = [row[1] for row in retrieved_context]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"📚 Retrieved Context:\")\n",
    "            for summary in context_summaries:\n",
    "                print(f\"- {summary[:120]}...\") # Print a snippet of each summary\n",
    "            print()\n",
    "\n",
    "        source_breadcrumbs = []\n",
    "        for url_id in context_ids:\n",
    "            breadcrumb = get_breadcrumb(conn, url_id)\n",
    "            if breadcrumb:\n",
    "                source_breadcrumbs.append(breadcrumb)\n",
    "\n",
    "        # 3. Build the RAG prompt\n",
    "        rag_prompt = build_rag_prompt(question, context_summaries)\n",
    "            \n",
    "        # 4. Get the final answer from the LLM\n",
    "        final_answer = get_rag_answer(rag_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"✅ Final Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            sources_list = []\n",
    "            if source_breadcrumbs:\n",
    "                print(\"🔗 Sources:\")\n",
    "                unique_trails = list(dict.fromkeys(source_breadcrumbs))\n",
    "                sources_list = unique_trails\n",
    "                for trail in unique_trails:\n",
    "                    print(f\"- {trail}\")\n",
    "                print()\n",
    "            \n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": sources_list\n",
    "            }\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "output_file = \"rag_results_by_url.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ All questions processed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7226e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================\n",
      "TOPIC: Getting Started & Navigation\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What is CIROH DocuHub and how can I use it?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH DocuHub is CIROH’s centralized gateway to documentation, services, and community resources that support hydrologic research and operations. It links you to computing (CIROH-2i2c JupyterHub on Google Cloud, CIROH AWS/Google Cloud, Pantarhei HPC), product docs (e.g., NextGen, Snow model, Tethys, NGIAB), policies, training, blogs, and collaboration tools (CIROH Portal).\n",
      "\n",
      "How to use it:\n",
      "- Browse Products to access documentation and tutorials: https://docs.ciroh.org/docs/products/intro\n",
      "- Launch computing resources:\n",
      "  - CIROH-2i2c JupyterHub: https://docs.ciroh.org/docs/services/cloudservices/2i2c\n",
      "  - Pantarhei HPC: https://docs.ciroh.org/docs/services/on-prem/Pantarhei\n",
      "  - Google Cloud resources: https://docs.ciroh.org/docs/services/cloudservices/google-cloud\n",
      "- Use the CIROH Portal for apps, datasets, courses, and publications: https://portal.ciroh.org/\n",
      "- Contribute docs, tutorials, datasets, or blogs: https://docs.ciroh.org/docs/contribute\n",
      "- Follow updates in Release Notes: https://docs.ciroh.org/release-notes/\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Products > CIROH Research Portal\n",
      "- Release Notes\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I get started with CIROH services?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the Infrastructure Access page linked from the CIROH Services Introduction to request access. For cloud, CIROH creates AWS/Google Cloud subaccounts for projects; PIs receive identities, and project teams manage users/environments. If you need help or an overview, join the CyberInfrastructure and Community NextGen Office Hours via ciroh-it-admin@ua.edu. Links:\n",
      "- Get started: https://docs.ciroh.org/docs/services/access/\n",
      "- Services overview: https://docs.ciroh.org/docs/services/intro\n",
      "- AWS details: https://docs.ciroh.org/docs/services/cloudservices/aws\n",
      "- Office Hours “How to Join”: https://docs.ciroh.org/docs/products/ngiab/office-hours/#how-to-join\n",
      "\n",
      "🔗 Sources:\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- DocuHub\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where can I find the getting started guide?\n",
      "\n",
      "✅ Final Answer:\n",
      "See “Intro to NGIAB” and click NGIAB 101 (https://docs.ciroh.org/training-NGIAB-101/) for the best getting-started guide.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > Installation and Setup\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "- Products > NGIAB 101 Learning Module > Learner Profiles\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I become a CIROH consortium member?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer with the information given. The provided pages describe services and access for existing CIROH members and partners but do not explain how to become a CIROH consortium member. For assistance, you could contact ciroh-it-admin@ua.edu as listed for related support.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- DocuHub\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Account & Access Requests\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I request access to CIROH infrastructure?\n",
      "\n",
      "✅ Final Answer:\n",
      "- For on-prem HPC (Pantarhei/Wukong):\n",
      "  1) PI submits the On-Premises Infrastructure Request Form: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=\n",
      "  2) Each user submits the On-Premise Access Request Form: https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUMzdTOUpKVU5UWFNCU0ZQUlowS0cxV0xFRy4u\n",
      "     - UA users: submit the Access Request only.\n",
      "     - Non-UA users: first complete VPN access in the form, then submit the Access Request.\n",
      "\n",
      "- For public cloud (AWS/Google) and CIROH-2i2c JupyterHub:\n",
      "  - PI submits the Cloud Infrastructure Request Form: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=case_studies_call.md&title=\n",
      "  - JupyterHub individual access: CPU form https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u, GPU form https://forms.office.com/r/mkrVJzyg9u\n",
      "\n",
      "- For JetStream2 via NSF Access:\n",
      "  1) PI submits the On-Premises Infrastructure Request Form (link above).\n",
      "  2) Register NSF Access ID: https://operations.access-ci.org/identity/new-user\n",
      "  3) Submit JetStream2 Access Request: https://forms.office.com/r/ERyKyHbdaC\n",
      "\n",
      "Help: ciroh-it-admin@ua.edu\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Infrastructure Access\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What are the requirements for using CIROH resources?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer with the information given. The provided pages describe CIROH’s services, data/code sharing policy, and external resources, but do not list specific “requirements for using CIROH resources.”\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Services\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > External Resources\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I request a new CIROH account?\n",
      "\n",
      "✅ Final Answer:\n",
      "It depends on what you need:\n",
      "\n",
      "- Pantarhei (on‑prem HPC):\n",
      "  1) Project PI submits the On‑Premises Infrastructure Request (GitHub issue template).\n",
      "  2) Each user submits the On‑Premise Access Request (Microsoft Form).\n",
      "  - UA users: submit only the On‑Premise Access Request.\n",
      "  - Non‑UA users: request VPN access first, then submit the On‑Premise Access Request.\n",
      "  - Help: ciroh-it-admin@ua.edu\n",
      "\n",
      "- Wukong (on‑prem cluster):\n",
      "  1) PI submits the On‑premises Infrastructure Request (GitHub issue) with project details and user roster.\n",
      "  2) Each user submits the CIROH On‑Premise Access Form; non‑UA users complete VPN Access Request first, then the On‑premise Access Request.\n",
      "\n",
      "- CIROH Cloud (AWS or Google Cloud):\n",
      "  - Request subaccounts via the Infrastructure Access workflow referenced on:\n",
      "    • AWS: docs.ciroh.org/docs/services/cloudservices/aws\n",
      "    • Google Cloud: docs.ciroh.org/docs/services/cloudservices/google-cloud\n",
      "  - Default budget: $500/month per project across AWS and Google Cloud; request additional funds via the Exceeding Budget Request GitHub form.\n",
      "\n",
      "If you can’t access forms, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > CIROH Subdomain Request Form\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I submit a request for new software installation?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Fill out the JupyterHub (2i2c) Software Install Form and select “Install Software on CIROH 2i2c JupyterHub” as the reason.\n",
      "- Create and export your conda environment to environment.yml (conda env export -n ENVNAME > environment.yml).\n",
      "- Email the environment.yml to ciroh-it-support@ua.edu after submitting the form.\n",
      "\n",
      "Form: https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > CIROH Subdomain Request Form\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I request access to JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "For CIROH-2i2c JupyterHub:\n",
      "- Go to the Infrastructure Access page linked on the CIROH JupyterHub doc (https://docs.ciroh.org/docs/services/access#accessing-ciroh-2i2c-jupyterhub) and follow the access process. If you’re in a workshop, get login details from the workshop coordinator.\n",
      "\n",
      "For CUAHSI JupyterHub:\n",
      "- Join the CUAHSI JupyterHub Group via HydroShare. After approval, launch from HydroShare Apps, the “Open with” menu on a HydroShare resource, or directly at jupyterhub.cuahsi.org using your HydroShare credentials. For help: help@cuahsi.org.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 6: How do I request GPU access for research?\n",
      "\n",
      "✅ Final Answer:\n",
      "Please clarify the platform:\n",
      "\n",
      "- CIROH-2i2c JupyterHub (GPU): Submit the CIROH-2i2c JupyterHub GPU Access Request Form and include your GitHub username: https://forms.office.com/r/mkrVJzyg9u\n",
      "- On-premises HPC (Pantarhei or Wukong GPUs): \n",
      "  1) Your PI submits the On-Premises Infrastructure Request (GitHub issue): https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=\n",
      "  2) Each user submits the On-Premise Access Request Form: https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUMzdTOUpKVU5UWFNCU0ZQUlowS0cxV0xFRy4u\n",
      "     - Non-UA users: complete VPN Access Request in the same form first.\n",
      "\n",
      "If you can’t access forms, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > Infrastructure Access\n",
      "- Services > External Resources\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 7: How do I request AWS or Google Cloud access through CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Go to the Infrastructure Access page and the “Accessing Public Cloud Services” section: https://docs.ciroh.org/docs/services/access#accessing-public-cloud-services\n",
      "- Submit the request form there to have CIROH create an AWS or Google Cloud subaccount and PI identity (IAM).\n",
      "- For examples of prior submissions: \n",
      "  - AWS: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues?q=is%3Aissue+is%3Aclosed+label%3Aaws\n",
      "  - Google Cloud: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues?q=is:issue+is:closed+label:google\n",
      "- Support: email ciroh-it-admin@ua.edu or post in Slack #ciroh-ua-it-admin.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- Services\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 8: How do I request access to the Pantarhei or Wukong HPC clusters?\n",
      "\n",
      "✅ Final Answer:\n",
      "Pantarhei:\n",
      "- Step 1 (project): PI opens an On‑Premises Infrastructure Request via the GitHub issue template: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=\n",
      "- Step 2 (individual): Each user submits the On‑Premise Access Request Form: https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUMzdTOUpKVU5UWFNCU0ZQUlowS0cxV0xFRy4u\n",
      "- UA users: submit only the On‑Premises Access Request. Non‑UA: request VPN access first, then submit the On‑Premises Access Request. Help: ciroh-it-admin@ua.edu\n",
      "\n",
      "Wukong:\n",
      "- Step 1 (project): PI files the On‑premises Infrastructure Request GitHub issue (same link as above) including PI info and a roster (names, GitHub usernames, emails).\n",
      "- Step 2 (individual): Each user submits the CIROH On‑Premise Access Form: https://forms.office.com/r/hED4zGVACM\n",
      "- Non‑UA users: within that form workflow, request VPN access first, then the On‑Premise Access Request. Each person must submit their own form.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Pantarhei\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 9: How do I request access to the NWM BigQuery API?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Submit the NWM BigQuery API Access Request Form: https://forms.office.com/r/FeNpjZstkr\n",
      "- After approval, use the API at https://nwm-api.ciroh.org (docs: https://nwm-api.ciroh.org/docs).\n",
      "- Insert your API key in example code (e.g., CIROH GitHub notebook).\n",
      "- If needed, contact CIROH IT support: ciroh-it-support@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Data Management and Access Tools > NWM BigQuery API\n",
      "- Blog > Accessing National Water Model (NWM) Data via Google Cloud BigQuery API\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Pantarhei > Obtaining an Account\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Documentation & Training\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: Where can I find NextGen framework documentation?\n",
      "\n",
      "✅ Final Answer:\n",
      "- NextGen Technical Guidance page: https://docs.ciroh.org/docs/policies/NextGen (with full guidance PDF linked there)\n",
      "- NextGen framework overview/presentation: https://docs.ciroh.org/docs/products/nextgen-framework/nextgen/\n",
      "- NGIAB “Intro to NextGen” and getting started materials: https://docs.ciroh.org/docs/products/ngiab/intro/ and https://docs.ciroh.org/docs/products/ngiab/intro/what-is\n",
      "- Run directory structure: https://docs.ciroh.org/docs/products/ngiab/intro/directories/\n",
      "- Forcings tool info: https://docs.ciroh.org/blog/NextGen%20Forcings (with GitHub link)\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > NextGen Framework\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: Where can I find documentation for a specific CIROH tool?\n",
      "\n",
      "✅ Final Answer:\n",
      "Visit the CIROH DocuHub Documentation section at https://docs.ciroh.org/docs/products/intro.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub\n",
      "- Products > CIROH Research Portal\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where can I access tutorials and training materials?\n",
      "\n",
      "✅ Final Answer:\n",
      "- NGIAB 101 training module: https://docs.ciroh.org/training-NGIAB-101/\n",
      "- Key Points: https://docs.ciroh.org/training-NGIAB-101/key-points.html\n",
      "- Glossary: https://docs.ciroh.org/training-NGIAB-101/reference.html\n",
      "- Episodes (Introduction through Advanced Topics): links within NGIAB 101\n",
      "- DevCon 2025 materials:\n",
      "  - Jetstream Instructions: https://docs.ciroh.org/training-NGIAB-101/devcon25js.html\n",
      "  - Troubleshooting: https://docs.ciroh.org/training-NGIAB-101/troubleshooting.html\n",
      "- Intro to NGIAB hub (orientation, install, glossary): https://docs.ciroh.org/docs/products/ngiab/intro/\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NGIAB 101 Learning Module > Learner Profiles\n",
      "- Products > NGIAB 101 Learning Module\n",
      "- Products > NGIAB 101 Learning Module > Instructor - Summary and Schedule > Instructor Notes\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: What educational resources are available for CIROH users?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Educational courses via the CIROH Research Portal (integrated with HydroLearn)\n",
      "- Documentation on CIROH products (e.g., NextGen, Snow model, Tethys, NGIAB) on DocuHub\n",
      "- Training and tutorials contributed by consortium members on DocuHub\n",
      "- Monthly NextGen-focused office hours (mentioned on DocuHub)\n",
      "- CIROH JupyterHub environments supporting learning and experimentation (via Services)\n",
      "- Portal-hosted learning modules and interactive web apps that support education\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > CIROH Research Portal\n",
      "- DocuHub\n",
      "- Services\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Contributing Content\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I contribute to CIROH DocuHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can contribute in several ways:\n",
      "\n",
      "- Minor edits: Go to docs.ciroh.org, open the page you want to change, click “Edit page” at the bottom, make your changes, and submit a pull request. An admin will review and merge.\n",
      "- Major edits: Fork the GitHub repo (CIROH-UA/ciroh-ua_website), make and commit changes, open a pull request. GitHub Actions will build and publish after review/merge. You can test locally with Node.js using: npm install; npm run start; npm run build.\n",
      "- Blog posts: Submit via the DocuHub blog contribution guide; posts should highlight projects using CIROH cyberinfrastructure and are reviewed by CIROH’s tech team.\n",
      "- Product pages: Ensure your product has a public GitHub repo with a clear README, then submit the Product Page Request Form (GitHub issue template).\n",
      "\n",
      "Developer resources and guides:\n",
      "- Working with the repository, technologies used, and adding blog posts: docs.ciroh.org/docs/contribute\n",
      "\n",
      "Support: ciroh-it-admin@ua.edu, Slack, or docs.ciroh.org/contact. Video tutorial: “How to Contribute to CIROH DocuHub?” on YouTube.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub\n",
      "- Release Notes\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I add my project documentation?\n",
      "\n",
      "✅ Final Answer:\n",
      "You have two options:\n",
      "\n",
      "- For simple edits to existing pages: go to docs.ciroh.org, open the page you want to change, click “Edit page,” make changes, and submit a pull request. An admin will review and merge.\n",
      "\n",
      "- To add a new product/project page: ensure your project is public on GitHub with a clear README, then submit the Product Page Request Form: https://github.com/CIROH-UA/ciroh-ua_website/issues/new?assignees=&labels=on-prem&projects=&template=product-request.md\n",
      "\n",
      "For larger additions via code, fork https://github.com/CIROH-UA/ciroh-ua_website, make changes, open a pull request, and it will be reviewed and published by GitHub Actions. For help: ciroh-it-admin@ua.edu or Contact us on docs.ciroh.org/contact.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where do I submit tutorials or training materials?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t find guidance on submitting tutorials or training materials in the provided information. Please contact ciroh-it-admin@ua.edu or use the Contact us page on docs.ciroh.org for assistance.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Products > NGIAB 101 Learning Module > Advanced Topics\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I publish my research findings on DocuHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "You have two options:\n",
      "\n",
      "- Simple updates to existing pages: Go to docs.ciroh.org, open the page to modify, click “Edit page,” make changes, and submit a pull request. An admin will review and merge.\n",
      "\n",
      "- New content (blog post or product page):\n",
      "  - Blog post (highlighting work that uses CIROH cyberinfrastructure): Either add a Markdown post in the /blog folder via a fork and pull request, or submit the Blog Post Request Form (GitHub issue template) for review.\n",
      "  - Product page (for a public GitHub project with a clear README): Submit the Product Page Request Form (GitHub issue template) for review.\n",
      "\n",
      "For larger edits or developer workflow: fork CIROH-UA/ciroh-ua_website, make changes, open a PR; GitHub Actions will build and publish. For help, contact via Slack, email, or the site’s contact page.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub > Working with the DocuHub repository\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- DocuHub\n",
      "- DocuHub > Contributing to CIROH DocuHub > Adding posts to the DocuHub Blog\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: JupyterHub\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I access CIROH JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "Go to the Infrastructure Access page and follow the steps to request/enable access to CIROH’s 2i2c JupyterHub. Once approved, use one of these hubs to log in:\n",
      "- Production: https://ciroh.awi.2i2c.cloud/hub/login\n",
      "- Staging: https://staging.ciroh.awi.2i2c.cloud/hub/login\n",
      "- Workshop (event-limited): https://workshop.ciroh.awi.2i2c.cloud/hub/login\n",
      "\n",
      "If you’re in a workshop, ask your coordinator for login details. For help: ciroh-it-admin@ua.edu or Slack #ciroh-ua-it-admin.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > HydroShare and CIROH JupyterHub Integration\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What’s the difference between Production, Staging, and Workshop JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Production: Main CIROH JupyterHub for day-to-day research use on Google Cloud.\n",
      "- Staging: Test environment for trying changes before they go to production.\n",
      "- Workshop: Temporary hub activated only for specific events; access via event coordinators.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What software is pre-installed on JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "Please check the Dockerfile for the current list of pre-installed software: https://github.com/2i2c-org/awi-ciroh-image/blob/main/Dockerfile\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I stop my JupyterHub server when I’m not using it?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the JupyterHub interface to stop your server when idle. On the CIROH 2i2c hubs, click the control to stop your running server from the hub UI. If you enabled the Jupyter Keepalive extension, first select “Stop keeping server alive” in the JupyterLab Command Palette, then stop the server.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Prevent Server Timeout\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I install custom software on JupyterHub?\n",
      "\n",
      "✅ Final Answer:\n",
      "- First check the current software list in the CIROH JupyterHub Dockerfile: https://github.com/2i2c-org/awi-ciroh-image/blob/main/Dockerfile\n",
      "- If your packages aren’t included, export your conda env to environment.yml:\n",
      "  conda env export -n ENVNAME > environment.yml\n",
      "- Submit the JupyterHub (2i2c) Software Install Form, selecting “Install Software on CIROH 2i2c JupyterHub”:\n",
      "  https://forms.office.com/Pages/ResponsePage.aspx?id=jnIAKtDwtECk6M5DPz-8p4IIpHdEnmhNgjOa9FjrwGtUNUoyV1UxNFIzV1AyTDhTNzdOT1Q5NVlLTC4u\n",
      "- Email the environment.yml to ciroh-it-support@ua.edu after submitting the form.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Push and Pull to GitHub\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: Cloud & HPC\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I request AWS cloud credits through CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Default allocation: $500/month per project across AWS and Google Cloud.\n",
      "- To request additional AWS credits: submit the Exceeding Budget Request Form via CIROH’s GitHub issue template: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=infrastructure&projects=&template=exceeding_budget_request.md&title=\n",
      "- For access/setup help, start at Infrastructure Access: https://docs.ciroh.org/docs/services/access#accessing-public-cloud-services or email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > Public Cloud > CIROH AWS Account > CIROH AWS Office Hours\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What computing resources does CIROH offer?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Multi-cloud and on-prem CIROH CyberInfrastructure\n",
      "- AWS and Google Cloud services (Compute, Storage; BigQuery; Earth Engine)\n",
      "- Managed 2i2c JupyterHub (including optional GPU allocations)\n",
      "- On-premise HPC clusters (Pantarhei and Wukong)\n",
      "- Pre-installed hydrologic software and preconfigured environments\n",
      "- Google BigQuery API for National Water Model data (access via ciroh-it-admin@ua.edu)\n",
      "- Guidance to external NSF ACCESS allocations for additional HPC resources\n",
      "\n",
      "🔗 Sources:\n",
      "- Services\n",
      "- Blog > CIROH Research CyberInfrastructure Update\n",
      "- Services > External Resources\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I use the Pantarhei HPC cluster?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Obtain an account: follow “Obtaining an Account” on the Pantarhei page.\n",
      "- Connect: first join the UA network via VPN (see UA OIT VPN page), then SSH to pantarhei.ua.edu using your Pantarhei username. SSH keys are required; see “Managing SSH Public Keys.” Command: ssh <USERNAME>@pantarhei.ua.edu\n",
      "- Run work via Slurm: do not run intensive tasks on the shared login node. Submit jobs to Slurm queues (CPU, GPU, FPGA) with appropriate walltime and size. Shared-node jobs are supported; long jobs up to 7 days on as many as 6 full nodes are allowed. Standard max job size is 240 cores; larger needs require a support consultation.\n",
      "- For guidance: see “Running Jobs” > “Accessing the Compute Nodes” for queue options and limits.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Pantarhei > Accessing the System\n",
      "- Services > On-Premises > Pantarhei\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Services > On-Premises > Pantarhei > System Architecture\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I access JetStream2?\n",
      "\n",
      "✅ Final Answer:\n",
      "To access JetStream2 through CIROH:\n",
      "\n",
      "1) Your project PI submits the Infrastructure Request Form to request team-wide access: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&labels=on-prem&projects=&template=onprem-request.md&title=\n",
      "2) If you don’t have one, register for an NSF ACCESS account: https://operations.access-ci.org/identity/new-user\n",
      "3) With your NSF ACCESS ID, submit the JetStream2 Access Request Form for an individual account: https://forms.office.com/r/ERyKyHbdaC\n",
      "\n",
      "After approval, follow the JetStream2 login guide: https://docs.jetstream-cloud.org/getting-started/login/\n",
      "\n",
      "If you can’t access the forms, email ciroh-it-admin@ua.edu.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > External Resources > NSF Access > JetStream2\n",
      "- Services > Infrastructure Access\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "- Services > External Resources > NSF Access\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I request compute resources for a workshop or training?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Complete the Workshop IT Request Form (GitHub template) with your technical requirements.\n",
      "  Link: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues/new?assignees=&projects=&template=workshop_IT_request.md\n",
      "- CIROH will provision the needed resources (e.g., CIROH-2i2c JupyterHub, NSF Access/JetStream2 VMs, AWS, Google Cloud) and ensure participant access before the event.\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > DevCon 2025: A DevOps and Cyberinfrastructure Success Story\n",
      "- Services > Infrastructure Access\n",
      "- Services > On-Premises > Wukong > Obtaining an Account\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Services & Infrastructure: Data Access\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I access the AORC dataset?\n",
      "\n",
      "✅ Final Answer:\n",
      "You can access AORC via the user-friendly Jupyter notebooks on HydroShare and run them on the CIROH 2i2c JupyterHub:\n",
      "\n",
      "- Go to the HydroShare resource: https://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f/\n",
      "- Click “Open With” and choose “CIROH 2i2c JupyterHub” to copy and run the notebooks.\n",
      "- Required accounts: a HydroShare account and CIROH-2i2c JupyterHub access via GitHub (request here: https://docs.ciroh.org/docs/services/access/#accessing-ciroh-2i2c-jupyterhub).\n",
      "\n",
      "The notebooks retrieve AORC from AWS in both latitude–longitude and NWM-projected formats for points or polygon shapefiles, with options to spatially subset and temporally aggregate. Development versions: https://github.com/CUAHSI/notebooks (Data Access Examples / AORC).\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: Where can I find forcing data for hydrological models?\n",
      "\n",
      "✅ Final Answer:\n",
      "- CIROH Research DataStream S3 Explorer: https://datastream.ciroh.org/index.html (browse NextGen forcings and metadata)\n",
      "- Supported NWM forcing sources the processor can read:\n",
      "  - Operational v3:\n",
      "    - https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/\n",
      "    - https://storage.googleapis.com/national-water-model/\n",
      "    - https://storage.cloud.google.com/national-water-model/\n",
      "    - gs://national-water-model/\n",
      "    - gcs://national-water-model/\n",
      "    - https://noaa-nwm-pds.s3.amazonaws.com/\n",
      "    - s3://noaa-nwm-pds/\n",
      "  - Retrospective:\n",
      "    - https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/\n",
      "    - s3://noaa-nwm-retrospective-2-1-pds/model_output/\n",
      "    - https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/netcdf/\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream > Datastream Components > Forcing Processor > Forcing Sources\n",
      "- Products > Research Datastream\n",
      "- Products > Research Datastream > Datastream Components > Datastream Configuration\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > Research Datastream > Datastream Components > Forcing Processor\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I retrieve data for a specific region?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the AORC_LL_ZoneRetrieval.ipynb (lat–lon) or AORC_NWMProj_ZoneRetrieval.ipynb (NWM projection) notebooks from the HydroShare resource. Open the HydroShare resource, click “Open With” → “CIROH 2i2c JupyterHub” (requires a HydroShare account and CIROH-2i2c JupyterHub access). In the notebook, provide your polygon shapefile for the region; the code will subset spatially and aggregate temporally as needed to retrieve the data from AWS.\n",
      "\n",
      "🔗 Sources:\n",
      "- Blog > AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "- Blog > Accessing National Water Model (NWM) Data via Google Cloud BigQuery API\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: What data formats are supported by CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer with the information given. The provided pages don’t list specific supported data formats.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Products > CIROH Research Portal\n",
      "- Policies > Data and Code Sharing\n",
      "- Products\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: NextGen Framework\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I get started with the NextGen framework?\n",
      "\n",
      "✅ Final Answer:\n",
      "Start with NGIAB (NextGen In A Box), which packages NextGen and its dependencies into reproducible containers.\n",
      "\n",
      "- Review Intro to NGIAB: docs.ciroh.org/docs/products/ngiab/intro/\n",
      "  - Begin with NGIAB 101 training: https://docs.ciroh.org/training-NGIAB-101/\n",
      "  - See “NWM, NextGen, and NGIAB” overview and “Installing NGIAB Locally”\n",
      "- Install via Docker (or Singularity on HPC), then use guide.sh to configure and run sample jobs.\n",
      "- Use the Data Preprocess tool to subset hydrofabric, generate forcings, and create realization files in a standard ngen-run directory.\n",
      "- Execute runs through guide.sh; optionally run automatically after preprocessing.\n",
      "- Evaluate results with TEEHR (integrated in guide.sh) and visualize via the Tethys-based Data Visualizer.\n",
      "\n",
      "For deeper framework details and integration standards, see the NextGen Technical Guidance PDF linked at: https://docs.ciroh.org/docs/policies/NextGen/\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Intro to NGIAB\n",
      "- Policies > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: What is NextGen In A Box (NGIAB)?\n",
      "\n",
      "✅ Final Answer:\n",
      "NextGen In A Box (NGIAB) is an open-source, containerized distribution of the Next Generation Water Resources Modeling Framework that packages the ngen engine, models, dependencies, and tools into a portable, reproducible environment. It simplifies installation and execution (via Docker/Singularity) to enable end-to-end hydrologic workflows—data preprocessing, model runs, evaluation, visualization, and calibration—across local, cloud, and HPC systems, supporting community contribution and future NWM versions.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NGIAB 101 Learning Module > Introduction\n",
      "- Products > NextGen In A Box > Intro to NGIAB > NWM, NextGen, and NGIAB\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NextGen In A Box\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I run NextGen locally?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Install prerequisites:\n",
      "  - Windows: Install WSL, install and start Docker Desktop, open WSL as Admin, verify with: docker ps -a. Ensure you use a non-Docker-linked WSL distro.\n",
      "  - Mac: Install and start Docker Desktop, verify with: docker ps -a.\n",
      "\n",
      "- Create a project directory and get sample data (optional but recommended to verify):\n",
      "  - mkdir -p NextGen && cd NextGen\n",
      "  - mkdir -p ngen-data && cd ngen-data\n",
      "  - wget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz\n",
      "  - tar -xf AWI_16_10154200_009.tar.gz\n",
      "  - cd ..\n",
      "\n",
      "- Clone NGIAB-CloudInfra and run the interactive guide:\n",
      "  - git clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git\n",
      "  - cd NGIAB-CloudInfra\n",
      "  - On WSL, ensure Git uses LF line endings (see NGIAB 101 if needed).\n",
      "  - ./guide.sh\n",
      "\n",
      "The guide will walk you through selecting input data and running NextGen (including parallel mode), and can optionally run evaluation (TEEHR) and launch the Tethys visualizer.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > DevCon 2025 Jetstream VM Instructions\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I configure NextGen for my basin?\n",
      "\n",
      "✅ Final Answer:\n",
      "You use NGIAB’s Data Preprocess tool to set up your basin, then run NextGen with the generated configuration.\n",
      "\n",
      "Basic steps:\n",
      "- Install and launch NGIAB (Docker/Singularity). Use guide.sh for interactive setup.\n",
      "- Run Data Preprocess to:\n",
      "  - Subset the hydrofabric to your basin,\n",
      "  - Generate meteorological forcings,\n",
      "  - Create the run realization files in a standard ngen-run directory.\n",
      "- Execute NextGen using the produced realization and inputs (via guide.sh or directly).\n",
      "- Optionally, evaluate with TEEHR and visualize with the Data Visualizer (guide.sh or ViewOnTethys.sh).\n",
      "\n",
      "If you need onboarding details, follow the NGIAB 101 module.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "- Products > NGIAB 101 Learning Module > Introduction\n",
      "- Products > NextGen In A Box > Intro to NGIAB > NWM, NextGen, and NGIAB\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I troubleshoot NextGen setup issues?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t fully answer with the information given. The provided context only mentions:\n",
      "- Ensure Docker is running before executing guide.sh.\n",
      "- On Linux, fix permission errors by running Docker with sudo or adding your user to the docker group:\n",
      "  sudo usermod -aG docker ${USER}\n",
      "  newgrp docker\n",
      "- Verify you’re in the correct directory (e.g., ~/NextGen/NGIAB-CloudInfra).\n",
      "- For WSL users, ensure Git uses LF (not CRLF) line endings to avoid shell script failures (see NGIAB 101).\n",
      "- Use the sample datasets and run ./guide.sh to validate the setup.\n",
      "\n",
      "For unresolved issues, use the NGIAB-CloudInfra GitHub issue tracker.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Contact Us\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Installing NGIAB Locally\n",
      "- Products > NGIAB 101 Learning Module > Installation and Setup\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 6: My NextGen model isn’t running—what should I check?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Verify run directory structure matches ngen-run standard:\n",
      "  - Required subfolders exist: config, forcings, outputs (metadata will auto-generate; lakeout/restart optional).\n",
      "- In config:\n",
      "  - realization.json is present and correctly references models, dates, and hydrofabric.\n",
      "  - Exactly one hydrofabric geopackage (e.g., nextgen_09.gpkg) is present and correctly referenced.\n",
      "  - Any required BMI configs exist; per-catchment files are under config/cat-config/<MODEL>/.\n",
      "- Forcings:\n",
      "  - Catchment-level forcing files exist for the selected domain and cover the run date range.\n",
      "- Module ordering:\n",
      "  - In multi-BMI setups, ensure modules are ordered correctly in the realization/run file (execution follows listed order).\n",
      "- Outputs:\n",
      "  - outputs directory exists and is writable.\n",
      "- If using NGIAB:\n",
      "  - Confirm data were prepared with the Data Preprocess tool and the generated ngen-run directory is complete.\n",
      "  - Run via guide.sh to ensure correct environment/container.\n",
      "- If errors persist:\n",
      "  - Inspect autogenerated metadata and logs in outputs/ or container console for missing files, bad paths, or mis-specified model configs.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Run Configuration Directory Structure\n",
      "- Policies > NextGen Framework\n",
      "- Products > NGIAB 101 Learning Module > Key Points\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 7: How do I resolve dependency conflicts in NextGen?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer that with the information given. The provided context describes NextGen’s framework, technical guidance, NGIAB containerization, and data/forcing tools, but does not include dependency conflict troubleshooting steps.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > National Water Model > NextGen Framework\n",
      "- Policies > NextGen Framework\n",
      "- Blog > NextGen Framework Forcings\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Getting Started\n",
      "- Blog > NextGen Monthly News Update - January 2024\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Community & Collaboration\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I connect with other CIROH researchers?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer with the information given. The provided context doesn’t specify channels or steps to connect with other CIROH researchers.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > CIROH Research Portal\n",
      "- Blog > CIROH Science Meeting 2024\n",
      "- Blog > CIROH at AGU 2024\n",
      "- DocuHub\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I join CIROH office hours?\n",
      "\n",
      "✅ Final Answer:\n",
      "Email ciroh-it-admin@ua.edu to receive the Microsoft Teams meeting link and calendar invite, and to subscribe to the monthly office hour sessions.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Cyberinfrastructure and Community NextGen Office Hours\n",
      "- Services > Public Cloud > CIROH AWS Account > CIROH AWS Office Hours\n",
      "- DocuHub\n",
      "- Services\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I provide feedback on CIROH services?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the “Your Feedback Matters” form linked from the DocuHub homepage: https://forms.office.com/r/5ww7qRWwwf. For questions or issues, you can also reach out via the Contact page: https://docs.ciroh.org/contact.\n",
      "\n",
      "🔗 Sources:\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Services\n",
      "- DocuHub\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Technical Support\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: I can’t access my JupyterHub environment—what should I do?\n",
      "\n",
      "✅ Final Answer:\n",
      "It depends which JupyterHub you’re using.\n",
      "\n",
      "If you’re using CUAHSI JupyterHub (jupyterhub.cuahsi.org):\n",
      "- Ensure you’ve joined the CUAHSI JupyterHub Group via HydroShare and your HydroShare profile is complete.\n",
      "- Try launching from HydroShare Apps or “Open with” on a resource, then select a profile environment.\n",
      "- If you still can’t access, email help@cuahsi.org.\n",
      "\n",
      "If you’re using CIROH-2i2c JupyterHub (ciroh.awi.2i2c.cloud and related hubs):\n",
      "- Confirm you have CIROH Infrastructure Access and are using the correct hub link (Production/Staging/Workshop).\n",
      "- If your server was idle, it may have stopped; try starting it again and choose an appropriate server size.\n",
      "- If problems persist, email ciroh-it-admin@ua.edu or post in Slack #ciroh-ua-it-admin.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > Public Cloud > CUAHSI JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: My data processing job failed—how can I troubleshoot?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t diagnose specifics without error details, but here are steps supported by the provided info:\n",
      "\n",
      "- If running on Pantarhei: submit your work as a Slurm job instead of on a login node. Choose an appropriate queue (CPU/GPU/FPGA), and keep within limits (≤240 cores; up to 7 days, ≤6 full nodes), or contact support for larger needs.\n",
      "\n",
      "- If using ngen-datastream/DataStreamCLI:\n",
      "  - Reduce run size or processes, or increase memory. Rules of thumb per process: ~1 GB RAM for days–weeks; ~4 GB for months–year.\n",
      "  - Throttle processes with -n, especially on non-dedicated hosts (default is nprocs_host − 2).\n",
      "  - Shorten simulation duration or decrease catchment count to lower memory footprint (forcingprocessor data cube scales with catchments × time × variables).\n",
      "  - Check that your forcing.tar.gz and ngen-bmi-configs.tar.gz (if supplying custom) are valid; otherwise let the tool auto-generate them.\n",
      "  - Be aware that more complex BMI modules (CFE, PET, SLOTH, NOM, t-route) increase runtime/memory; simplify if needed.\n",
      "\n",
      "If it still fails, share the error message, run size (catchments, time steps), process count, and available RAM for more targeted guidance.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Products > NGIAB 101 Learning Module > Model Execution\n",
      "- Products > Research Datastream\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: How do I optimize my computational resources?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Use Slurm on Pantarhei: submit jobs to queues instead of running heavy tasks on login nodes.\n",
      "- Pick appropriate queues (CPU/GPU/FPGA); shorter walltime/smaller jobs often start faster.\n",
      "- Favor shared-node jobs to improve throughput and cluster utilization.\n",
      "- For long runs, you can request up to 7 days on as many as 6 full nodes; standard max job size is 240 cores (consult support for larger).\n",
      "- For ngen-datastream:\n",
      "  - Control process count with -n; default is (host_procs − 2). On dedicated nodes you can use all cores; on shared/non-dedicated nodes throttle to avoid contention.\n",
      "  - Ensure sufficient RAM per process: ~1 GB for days–weeks runs; ~4 GB for months–year. Adjust memory, domain size, duration, or processes to prevent crashes.\n",
      "  - Expect runtime scaling ~1 minute per 10 hourly steps (varies by config/hardware).\n",
      "- Use Singularity containers on HPC for secure, scheduler-integrated runs; request interactive/compute nodes via Slurm before launching workloads.\n",
      "- If on-prem resources are insufficient, explore NSF ACCESS resources for additional no-cost computing and storage.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Services > External Resources\n",
      "- Products > NGIAB 101 Learning Module > Advanced Topics\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: I need help with processing large datasets—where do I start?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t fully answer without more specifics, but here are starting points from the provided resources:\n",
      "\n",
      "- If you’re running large NextGen hydrologic simulations, use ngen-datastream. Start with the Usage guide to size hardware and processes:\n",
      "  - Default processes: nprocs_host − 2; throttle with -n on shared hosts.\n",
      "  - Memory per process (10,000 catchments): ~1 GB for days–weeks, ~4 GB for months–year.\n",
      "  - Example benchmark: VPU 09 (~11k catchments, 24 hourly steps) ~5 minutes on AWS t4g.2xlarge (8 vCPU, 32 GB, ARM) with CFE, PET, SLOTH, NOM.\n",
      "  - If crashes occur, increase RAM or reduce domain, duration, or process count.\n",
      "\n",
      "- To automate inputs/outputs and run on AWS, use DataStreamCLI:\n",
      "  - Install per INSTALL.md, then follow the interactive datastream_guide.\n",
      "  - Typical run: supply a hydrofabric geopackage, set start/end times, choose forcing set (e.g., NWM v3 Retrospective), and run with -n processes. Outputs go to data/.../ngen-run/outputs.\n",
      "\n",
      "- If you’re on the CIROH NETWA testbed:\n",
      "  - Access via SSH (your_netId@ciroh-testbed.uvm.edu).\n",
      "  - Store/download large shared datasets under /netfiles/ciroh/downloadedData/ to avoid duplication; keep personal data under /netfiles/ciroh/<netID>.\n",
      "  - Initialize mamba/conda from /usr/local/miniforge3; activate shared environments under /data/condaShared/envs.\n",
      "\n",
      "- For bigger workloads or ML workflows, consider HPC (e.g., Wukong) with S3 for storage/versioning and cost optimization.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream > DatastreamCLI > Usage Guide\n",
      "- Products > Research Datastream\n",
      "- Products > Research Datastream > Datastream Components > Datastream Configuration\n",
      "- Blog > Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling\n",
      "- Products > Data Management and Access Tools > NETWA > Getting Started\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 5: How do I report a bug or issue to CIROH IT support?\n",
      "\n",
      "✅ Final Answer:\n",
      "Use the NGIAB GitHub issue tracker for bugs and feature requests: https://github.com/CIROH-UA/NGIAB-CloudInfra/issues. For immediate help, you can also email ciroh-it-admin@ua.edu or post in the CIROH Slack #ciroh-ua-it-support channel (requires workspace access).\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > NextGen In A Box > Distributions > NGIAB-CloudInfra > Contact Us\n",
      "- DocuHub > Contributing to CIROH DocuHub\n",
      "- Services > CIROH Subdomain Request Form\n",
      "- Services > Public Cloud > CIROH JupyterHub > Documentation and Tutorial > Request custom images\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Data & Workflow Best Practices\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What are the best practices for managing CIROH datasets?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer with the information given. The context provides sharing options and policy requirements, but not specific dataset management best practices.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Products > CIROH Research Portal\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I ensure reproducibility in my CIROH research?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Follow CIROH’s guiding principles: make science reproducible through openness.\n",
      "- Share data and code openly with sufficient metadata, using open, machine-readable formats.\n",
      "- Use recommended platforms:\n",
      "  - Data: HydroShare for small, citable, API/spatial datasets; CIROH cloud object storage for large/rapidly changing datasets; include QC description in metadata and DOIs where applicable.\n",
      "  - Code: Host on GitHub (use BSD-3-Clause or MIT); archive releases with Zenodo for DOIs; use CIROH/OWP open-source templates.\n",
      "  - Models/workflows: Share model source on GitHub, instances/notebooks on HydroShare; provide containers (e.g., NextGen In A Box); document workflows (GitHub gists, DocuHub, notebooks).\n",
      "- Use CIROH CyberInfrastructure (JupyterHub, cloud/HPC) to run and share pre-configured, reproducible environments.\n",
      "- Meet NOAA timelines: make data publicly accessible by article publication or within two years, with proper disclaimer and funding acknowledgments.\n",
      "- Include clear citations (DOIs), metadata, QC methods, and access links in publications and final reports.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing\n",
      "- Services\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What tools are available for data visualization in CIROH?\n",
      "\n",
      "✅ Final Answer:\n",
      "- Tethys-CSES (Community Streamflow Evaluation System)\n",
      "- CIROH Research Portal native apps for visualization:\n",
      "  - TethysDash\n",
      "  - Water Data Explorer\n",
      "  - SWEML\n",
      "  - Grace Groundwater Tool\n",
      "  - Snow Inspector\n",
      "  - CSES (for National Water Model evaluation)\n",
      "- Proxy visualization apps accessible via the portal:\n",
      "  - FIM Visualization Deck\n",
      "  - OWP NWM Map Viewer\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Visualization and Analysis Tools\n",
      "- Products > CIROH Research Portal\n",
      "- Services\n",
      "- Community Impact\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I convert between different hydrological data formats?\n",
      "\n",
      "✅ Final Answer:\n",
      "I can’t answer that with the information given. The provided context defines common formats used in NextGen workflows (GeoPackage for hydrofabrics, JSON for realizations, NetCDF-4/Zarr for scientific variables, YAML for settings, tarballs for distribution) and points to tools (Hydrofabric R packages, NGIAB Data Preprocessor, TEEHR), but it does not describe conversion procedures between formats.\n",
      "\n",
      "🔗 Sources:\n",
      "- Products > Research Datastream\n",
      "- Products > NextGen In A Box > Intro to NGIAB > Glossary\n",
      "- Products > Hydrofabric\n",
      "- Products > NextGen In A Box > Components > NGIAB Data Preprocess\n",
      "- Products > Data Management and Access Tools > Data Access\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Policies & Administration\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What are CIROH’s usage policies?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH’s usage policies center on its Data and Code Sharing Policy aligned with NOAA’s Data Sharing Directive (v3.0). Key points:\n",
      "- Open access: Environmental data from CIROH projects must be publicly visible and accessible at no or minimal cost, machine-readable, and sufficiently documented (preferably with open standards/metadata). Include the data’s internet location in final reports.\n",
      "- Quality control: Perform QC and reference methods/results in metadata per NOAA Information Quality Guidelines.\n",
      "- Timeliness: Make data public by the earlier of publication, two years after data verification/collection, or two years after the grant’s original end date, unless NOAA authorizes delay.\n",
      "- Disclaimer: Public data must include “These data and related items of information have not been formally disseminated by NOAA, and do not represent any agency determination, view, or policy.”\n",
      "- Enforcement: Failure or delay to share per the Data Management Plan may trigger enforcement and affect future awards; applies to sub-recipients and contractors.\n",
      "- Funding acknowledgment: Identify federal funding in publications with grant programs and award numbers.\n",
      "- Manuscripts: Submit final accepted manuscripts to the NOAA Institutional Repository (one-year embargo). NOAA advises not sharing preprints prior to peer review.\n",
      "- Platform recommendations: Use HydroShare for small/citable datasets; CIROH Cloud (e.g., AWS) for large/rapid datasets; GitHub with permissive licenses for code; Zenodo DOIs for releases; CIROH/OWP repos and Portal for publications; HydroLearn for educational materials.\n",
      "- Cloud usage: CIROH provides AWS subaccounts; projects manage their environments and user access. Default cloud budget is $500/month across AWS and Google Cloud; request increases if needed.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How much compute time am I allocated?\n",
      "\n",
      "✅ Final Answer:\n",
      "It depends on the resource:\n",
      "\n",
      "- Pantarhei (on-prem): Jobs can run up to 7 days, using up to 6 full nodes; max standard job size is 240 cores. Larger or longer needs a support consultation.\n",
      "- Wukong (on-prem): Single-node system; no specific per-user time allocation stated.\n",
      "- Google Cloud via CIROH: Budget-based, not time-based—$500/month shared across AWS and Google Cloud unless additional funds are approved.\n",
      "- NSF ACCESS: Allocations are free but determined through the ACCESS program process; no fixed time stated here.\n",
      "\n",
      "If you meant a specific system, please specify.\n",
      "\n",
      "🔗 Sources:\n",
      "- Services > On-Premises > Pantarhei > Running Jobs > Accessing the Compute Nodes\n",
      "- Services > On-Premises > Wukong > System Architecture\n",
      "- Services > Public Cloud > CIROH Google Account\n",
      "- Services > On-Premises > Pantarhei > Running Jobs\n",
      "- Services > External Resources > NSF Access\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: What are the CIROH data sharing policies?\n",
      "\n",
      "✅ Final Answer:\n",
      "CIROH follows NOAA’s Data Sharing Directive (v3.0) and requires:\n",
      "\n",
      "- Public access: Environmental data created under CIROH awards must be publicly visible and accessible at no or minimal cost, in machine-readable, open formats with sufficient metadata; include the data’s internet location in final reports.\n",
      "- Quality control: Apply QC and describe QC methods/results in metadata per NOAA Information Quality Guidelines.\n",
      "- Timeliness: Make data accessible no later than the associated peer‑reviewed publication, or within two years of data collection/verification, or two years after the original grant end date—whichever comes first—unless NOAA authorizes a delay.\n",
      "- Disclaimer: Include “These data and related items of information have not been formally disseminated by NOAA, and do not represent any agency determination, view, or policy.”\n",
      "- Compliance: CIROH awardees, sub-recipients, and subcontractors are bound; failure or delay may trigger enforcement and affect future awards.\n",
      "- Funding acknowledgment: Identify federal funding sources and award numbers in publications; use FundRef as applicable.\n",
      "- Publication access: Submit final accepted manuscripts to the NOAA Institutional Repository; a one-year embargo is allowed.\n",
      "- Scope/definitions: Applies to environmental data and peer‑reviewed publications produced with extramural funding; excludes lab notebooks, preliminary analyses, drafts, peer reviews, communications, and physical objects.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > Data and Code Sharing\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Products > CIROH Research Portal\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: Are there restrictions on using CIROH resources for commercial work?\n",
      "\n",
      "✅ Final Answer:\n",
      "I cannot answer with the information given. The provided pages do not state whether CIROH resources may be used for commercial work.\n",
      "\n",
      "🔗 Sources:\n",
      "- Policies > Data and Code Sharing > Policy and Guidance\n",
      "- Policies > Data and Code Sharing > Recommendations\n",
      "- Services > Public Cloud > CIROH AWS Account\n",
      "- Services > External Resources\n",
      "\n",
      "\n",
      "✅ All questions processed. Results saved to rag_results_by_chunk.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_data = []\n",
    "\n",
    "# RAG Pipeline Loop for knowledge searching in TBLURLs and TBLContent with new dictionary structure\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "\n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n❓ PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Get the embedding for the current question\n",
    "        question_embedding = get_embedding(question, dimensions=1792)\n",
    "            \n",
    "        if question_embedding is None:\n",
    "            print(\"Could not generate embedding for the question. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # --- LEVEL 1 SEARCH: Find relevant pages and their summaries ---\n",
    "        retrieved_pages = query_embedding(question_embedding, max_relevant_urls=max_relevant_urls)\n",
    "\n",
    "        if not retrieved_pages:\n",
    "            print(\"No relevant pages found in the database. Skipping.\")\n",
    "            continue\n",
    "                \n",
    "        # Create a map of idurl -> summary from the initial results for efficient access\n",
    "        page_summary_map = {row[0]: row[1] for row in retrieved_pages}\n",
    "        context_ids = list(page_summary_map.keys())\n",
    "        if verbose:\n",
    "            print(f\"📚 Level 1: Found relevant pages with IDs: {context_ids}\")\n",
    "\n",
    "        # --- LEVEL 2 SEARCH: Find relevant chunks within those pages ---\n",
    "        retrieved_chunks = query_chunks(question_embedding, context_ids, max_relevant_chunks=max_relevant_chunks)\n",
    "\n",
    "        final_context_list = []\n",
    "        source_ids = []\n",
    "\n",
    "        if not retrieved_chunks:\n",
    "            # Fallback strategy: If no specific chunks are found, use the page summaries from Level 1\n",
    "            if verbose:\n",
    "                print(\"No specific chunks found, falling back to page summaries for context.\")\n",
    "            final_context_list = list(page_summary_map.values())\n",
    "            source_ids = context_ids\n",
    "        else:\n",
    "            # --- NEW LOGIC: Group chunks by page and build rich, multi-source context ---\n",
    "            if verbose:\n",
    "                print(f\"🎯 Level 2: Found {len(retrieved_chunks)} relevant chunks across pages.\")\n",
    "            \n",
    "            chunks_by_page = {}\n",
    "            for idurl, order, content in retrieved_chunks:\n",
    "                if idurl not in chunks_by_page:\n",
    "                    chunks_by_page[idurl] = []\n",
    "                chunks_by_page[idurl].append(order)\n",
    "\n",
    "            # Iterate through the original pages to maintain relevance order\n",
    "            for page_id in context_ids:\n",
    "                if page_id in chunks_by_page:\n",
    "                    source_ids.append(page_id)\n",
    "                    \n",
    "                    # Get the page summary from the map (no new DB call)\n",
    "                    page_summary = page_summary_map.get(page_id, \"No summary available.\")\n",
    "                    \n",
    "                    # Get the order numbers of relevant chunks for this page\n",
    "                    relevant_orders = chunks_by_page[page_id]\n",
    "                    \n",
    "                    # Expand to include neighbors and remove duplicates\n",
    "                    orders_with_neighbors = set()\n",
    "                    for order_num in relevant_orders:\n",
    "                        orders_with_neighbors.add(order_num - 1)\n",
    "                        orders_with_neighbors.add(order_num)\n",
    "                        orders_with_neighbors.add(order_num + 1)\n",
    "                    \n",
    "                    # Fetch all unique chunks (originals + neighbors) in correct document order\n",
    "                    expanded_chunk_rows = execute_query(conn, \"\"\"\n",
    "                        SELECT Content FROM TBLContent\n",
    "                        WHERE idurl = %s AND \"order\" = ANY(%s)\n",
    "                        ORDER BY \"order\" ASC;\n",
    "                    \"\"\", params=(page_id, list(orders_with_neighbors)), fetch=True)\n",
    "                    \n",
    "                    if expanded_chunk_rows:\n",
    "                        detailed_context = \"\\n\\n\".join([row[0] for row in expanded_chunk_rows])\n",
    "                        \n",
    "                        # Combine summary and detailed context for this page\n",
    "                        page_context = f\"Source Page Summary:\\n{page_summary}\\n\\nDetailed Information from this page:\\n{detailed_context}\"\n",
    "                        final_context_list.append(page_context)\n",
    "\n",
    "        if not final_context_list:\n",
    "            print(\"Could not build any context. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n📝 Final Combined Context for LLM:\")\n",
    "            for idx, ctx in enumerate(final_context_list):\n",
    "                print(f\"--- Context Block {idx+1} ---\\n{ctx[:10000]}...\\n\")\n",
    "\n",
    "        # 3. Build the RAG prompt with the new, multi-source context\n",
    "        rag_prompt = build_rag_prompt(question, final_context_list)\n",
    "            \n",
    "        # 4. Get the final answer from the LLM\n",
    "        final_answer = get_rag_answer(rag_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"✅ Final Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            # Get breadcrumbs for all source pages used in the context\n",
    "            source_breadcrumbs = []\n",
    "            for url_id in source_ids:\n",
    "                breadcrumb = get_breadcrumb(conn, url_id)\n",
    "                if breadcrumb:\n",
    "                    source_breadcrumbs.append(breadcrumb)\n",
    "\n",
    "            sources_list = []\n",
    "            if source_breadcrumbs:\n",
    "                print(\"🔗 Sources:\")\n",
    "                unique_trails = list(dict.fromkeys(source_breadcrumbs))\n",
    "                sources_list = unique_trails\n",
    "                for trail in unique_trails:\n",
    "                    print(f\"- {trail}\")\n",
    "                print()\n",
    "\n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": sources_list\n",
    "            }\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "output_file = \"rag_results_by_chunk.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ All questions processed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3defe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_prompt(question):\n",
    "    \"\"\"\n",
    "    Builds a simple prompt asking the LLM to answer based on its knowledge of a website.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Based on the information available on the website http://docs.ciroh.org, please answer the following question.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_standard_gpt_answer(prompt):\n",
    "    \"\"\"\n",
    "    Calls the LLM with standard parameters to get a general knowledge-based answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            # No 'reasoning_effort' or 'verbosity' parameters are used here\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating standard GPT answer: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325dfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================\n",
      "TOPIC: Getting Started & Navigation\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: What is CIROH DocuHub and how can I use it?\n",
      "\n",
      "✅ Standard GPT-4o Answer:\n",
      "I'm sorry, but I can't access external websites directly, including http://docs.ciroh.org. However, based on typical functions, a \"DocuHub\" might refer to a centralized documentation platform where users can access, manage, and collaborate on various documents, whether for software, research, or other collaborative projects. To use it effectively, you would generally register for an account (if required), navigate through available documentation, and use search features to find specific information. For detailed instructions on usage, I recommend visiting the website directly or looking for tutorials or guides that they provide.\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 2: How do I get started with CIROH services?\n",
      "\n",
      "✅ Standard GPT-4o Answer:\n",
      "I'm sorry, but I cannot access specific content from external websites such as http://docs.ciroh.org directly. However, if you're looking to get started with CIROH services, here are some general steps you might consider:\n",
      "\n",
      "1. **Visit the Official Website**: Start by exploring the homepage and any 'Getting Started', 'About Us', or 'Services' sections on the site.\n",
      "\n",
      "2. **Create an Account**: If there is an option to sign up or create an account, that would typically be one of the first steps to accessing services.\n",
      "\n",
      "3. **Consult Documentation**: Look for any guides, FAQs, or documentation that provide instructions on using their services.\n",
      "\n",
      "4. **Contact Support**: If you're unable to find the information you need, consider reaching out to their customer support or help desk for assistance.\n",
      "\n",
      "5. **Community or Forums**: Check if there are forums, community groups, or discussion boards where you can ask questions and share experiences with other users.\n",
      "\n",
      "For the most accurate guidance, please visit the CIROH website directly and navigate to their getting started section or contact their support team for precise instructions.\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 3: Where can I find the getting started guide?\n",
      "\n",
      "✅ Standard GPT-4o Answer:\n",
      "To find the getting started guide on the website http://docs.ciroh.org, you would typically look for a section specifically labeled \"Getting Started\" on the homepage or within the main navigation menu of the documentation site. This page is commonly designed to help new users quickly understand the basics and begin working with the software or tools provided by Ciroh. If it's not prominently displayed, you might consider using the website's search functionality to locate it or checking under sections like \"Introduction\" or \"Quick Start.\"\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 4: How do I become a CIROH consortium member?\n",
      "\n",
      "✅ Standard GPT-4o Answer:\n",
      "To become a CIROH consortium member, you would typically need to follow the guidelines and procedures outlined on their official website or contact them directly for the most accurate and detailed instructions. Generally, this may involve filling out an application form, meeting specific eligibility criteria, or aligning your research interests with the goals and objectives of the consortium. \n",
      "\n",
      "Since I don’t have direct access to external websites, I recommend visiting http://docs.ciroh.org and looking for sections like \"Membership,\" \"Join Us,\" or \"Consortium\" that might provide specific instructions. Alternatively, you could reach out to them via contact information provided on their website for personalized guidance.\n",
      "\n",
      "\n",
      "=============================================\n",
      "TOPIC: Account & Access Requests\n",
      "=============================================\n",
      "\n",
      "--- \n",
      "\n",
      "❓ PROCESSING QUESTION 1: How do I request access to CIROH infrastructure?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m standard_prompt = build_standard_prompt(question)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 2. Get the answer from the standard LLM call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m final_answer = \u001b[43mget_standard_gpt_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_answer:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Standard GPT-4o Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinal_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mget_standard_gpt_answer\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mCalls the LLM with standard parameters to get a general knowledge-based answer.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# No 'reasoning_effort' or 'verbosity' parameters are used here\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1150\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1147\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1148\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1149\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/CIROH_AIBot/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results_data = []\n",
    "\n",
    "for topic, question_list in questions.items():\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(f\"=============================================\\n\")\n",
    "    \n",
    "    for i, question in enumerate(question_list):\n",
    "        print(f\"--- \\n\\n❓ PROCESSING QUESTION {i+1}: {question}\\n\")\n",
    "            \n",
    "        # 1. Build the standard prompt\n",
    "        standard_prompt = build_standard_prompt(question)\n",
    "            \n",
    "        # 2. Get the answer from the standard LLM call\n",
    "        final_answer = get_standard_gpt_answer(standard_prompt)\n",
    "            \n",
    "        if final_answer:\n",
    "            print(f\"✅ Standard GPT-5 Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "            # Create a dictionary for the current result\n",
    "            result_entry = {\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": []  # Empty list as no sources are retrieved\n",
    "            }\n",
    "            # Add the result to our main list\n",
    "            results_data.append(result_entry)\n",
    "\n",
    "# After the loop is complete, save all results to a new JSON file\n",
    "output_filename = \"rag_results_standard_gpt.json\"\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ All questions processed. Benchmark results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9c6fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIROH_AIBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
